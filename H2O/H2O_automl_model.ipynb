{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML: Automatic Machine Learning\n",
    "\n",
    "AutoML: Automatic Machine Learning  \n",
    "\n",
    "H2Oâ€™s AutoML is used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit. Stacked Ensembles will be automatically trained on collections of individual models to produce highly predictive ensemble models which, in most cases, will be the top performing models in the AutoML Leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorials\n",
    "\n",
    "* Intro to AutoML + Hands-on Lab - Erin LeDell, Machine Learning Scientist... [https://youtu.be/42Oo8TOl85I](https://youtu.be/42Oo8TOl85I)  \n",
    "* Scalable Automatic Machine Learning in H2O [https://youtu.be/j6rqrEYQNdo](https://youtu.be/j6rqrEYQNdo)      \n",
    "\n",
    "\n",
    "![Scalable Automatic Machine Learning in H2O](http://nikbearbrown.com/YouTube/MachineLearning/IMG/Scalable_Automatic_Machine_Learning_in_H2O.png)\n",
    "\n",
    "\n",
    "Scalable Automatic Machine Learning in H2O [https://youtu.be/j6rqrEYQNdo](https://youtu.be/j6rqrEYQNdo)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing H2O and h2o python\n",
    "\n",
    "See [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html)  \n",
    "\n",
    "Click the Download H2O button on the [http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html](http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html) page. This downloads a zip file that contains everything you need to get started.\n",
    "\n",
    "\n",
    "```bash\n",
    "cd ~/Downloads\n",
    "unzip h2o-3.20.0.1.zip\n",
    "cd h2o-3.20.0.1\n",
    "java -jar h2o.jar\n",
    "```\n",
    "\n",
    "Point your browser to http://localhost:54321.\n",
    "\n",
    "**Install in Python**  \n",
    "\n",
    "Install dependencies (prepending with sudo if needed):\n",
    "\n",
    "```bash\n",
    "pip install requests\n",
    "pip install tabulate\n",
    "pip install scikit-learn\n",
    "pip install colorama\n",
    "pip install future\n",
    "```\n",
    "\n",
    "Remove any existing H2O module for Python.\n",
    "\n",
    "```bash\n",
    "pip uninstall h2o\n",
    "```\n",
    "\n",
    "Use pip to install this version of the H2O Python module.  \n",
    "\n",
    "```bash\n",
    "pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o\n",
    "```\n",
    "\n",
    "Note: When installing H2O from pip in OS X El Capitan, users must include the --user flag. For example:\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o --user\n",
    "```\n",
    "\n",
    "Initialize H2O in Python and run a demo to see H2O at work.\n",
    "\n",
    "```python\n",
    "python\n",
    "import h2o\n",
    "h2o.init()\n",
    "h2o.demo(\"glm\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "\n",
    "H2O model file file will be saved in one of two formats.\n",
    "\n",
    "\n",
    "There are two ways to save the leader model -- binary format and MOJO format.  If you're taking your leader model to production, then we'd suggest the MOJO format since it's optimized for production use.\n",
    "\n",
    "See [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/save-and-load-model.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/save-and-load-model.html)    \n",
    "\n",
    "```python\n",
    "\n",
    "# save the model\n",
    "model_path = h2o.save_model(model=model, path=\"/tmp/mymodel\", force=True)\n",
    "\n",
    "# or\n",
    "\n",
    "h2o.save_model(aml.leader, path = \"./models\")\n",
    "\n",
    "# or\n",
    "\n",
    "aml.leader.download_mojo(path = \"./models\")\n",
    "\n",
    "# load the model\n",
    "saved_model = h2o.load_model(model_path)\n",
    "\n",
    "```\n",
    "\n",
    "**Saving data from runs**   \n",
    "\n",
    "\n",
    "Stats about the models can be saved as text or csv or put directly in a database.\n",
    "\n",
    "Much of the data is gathered by converting H2O objects to pandas data frame.  So anything that a pandas data frame can be saved as is supported.\n",
    "[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)    \n",
    "\n",
    "\n",
    "```python\n",
    "data_pd = object.as_data_frame(use_pandas=True)\n",
    "```  \n",
    "\n",
    "Otherwise data is returned as python dictionaries or lists.  \n",
    "\n",
    "```python\n",
    "[('addr_state', 258199.28125, 1.0, 0.19965953057652525), ('int_rate', 203347.0625, 0.7875585924002257, 0.15724357886013807), ('dti', 116477.5703125, 0.45111500600856147, 0.09006941033569575), ('revol_util', 110586.1484375, 0.42829766179877776, 0.08551371010176734), ('annual_inc', 96993.90625, 0.3756552139898724, 0.07500314368384206), ('loan_amnt', 95294.5, 0.36907345186500207, 0.0736890321476241), ('total_acc', 90064.8046875, 0.3488189597255124, 0.06964502975498767), ('longest_credit_length', 84291.921875, 0.3264607146345416, 0.06518099303560954), ('purpose', 77462.203125, 0.30000936776426446, 0.05989972953637317), ('emp_length', 63839.28125, 0.24724809821677224, 0.04936543922589935), ('term', 34895.7265625, 0.1351503629040408, 0.02698405801466782), ('home_ownership', 26499.876953125, 0.10263342649457897, 0.02049174175536795), ('delinq_2yrs', 20556.2578125, 0.0796139234508423, 0.015895678583550586), ('verification_status', 14689.3369140625, 0.056891470971368166, 0.01135892438795138)]\n",
    "```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting H2O server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import h2o package and specific estimator \n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h2o.init with python seems very sensitive the the H2O version.  If the H2O cluster version is 3.20.0.1 and the python h2o library is 3.19.0 it will fail so we set strict_version_check=False\n",
    "\n",
    "If the H2O cluster isn't found h2o.init will start one.\n",
    "\n",
    "Note that the current script starts each H2O instance on a different port.  It's not clear why but should we do this we should choose from only the higher ports.\n",
    "\n",
    "A port number is a 16-bit unsigned integer, thus ranging from 0 to 65535.  There is no reason to choose a port less than 10000.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_121\"; OpenJDK Runtime Environment (Zulu 8.20.0.5-macosx) (build 1.8.0_121-b15); OpenJDK 64-Bit Server VM (Zulu 8.20.0.5-macosx) (build 25.121-b15, mixed mode)\n",
      "  Starting server from /Users/bear/anaconda/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/lh/42j8mfjx069d1bkc2wlf2pw40000gn/T/tmpt3cm7gk3\n",
      "  JVM stdout: /var/folders/lh/42j8mfjx069d1bkc2wlf2pw40000gn/T/tmpt3cm7gk3/h2o_bear_started_from_python.out\n",
      "  JVM stderr: /var/folders/lh/42j8mfjx069d1bkc2wlf2pw40000gn/T/tmpt3cm7gk3/h2o_bear_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>03 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/New_York</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.20.0.1</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>18 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_bear_ebl55g</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.556 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         03 secs\n",
       "H2O cluster timezone:       America/New_York\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.20.0.1\n",
       "H2O cluster version age:    18 days\n",
       "H2O cluster name:           H2O_from_python_bear_ebl55g\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.556 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init(strict_version_check=False) # start h2o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h2o.automl Parameters\n",
    "\n",
    "[http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)  \n",
    "\n",
    "NB:  Eventually one wants to expose all the parameters to the expert user.   \n",
    "\n",
    "**Required Data Parameters**\n",
    "\n",
    "y: This argument is the name (or index) of the response column.  \n",
    "\n",
    "training_frame: Specifies the training set.  \n",
    "\n",
    "The user gives the name of the depenent variable and training file name.   \n",
    "\n",
    "\n",
    "**Required Stopping Parameters**  \n",
    "\n",
    "One of the following stopping strategies (time or number-of-model based) must be specified. When both options are set, then the AutoML run will stop as soon as it hits one of either of these limits.\n",
    "\n",
    "max_runtime_secs: This argument controls how long the AutoML run will execute for. This defaults to 3600 seconds (1 hour).  \n",
    "\n",
    "\n",
    "max_models: Specify the maximum number of models to build in an AutoML run, excluding the Stacked Ensemble models. Defaults to NULL/None.\n",
    "\n",
    "\n",
    "### Optional Parameters\n",
    "\n",
    "**Optional Data Parameters**  \n",
    "\n",
    "x: A list/vector of predictor column names or indexes. This argument only needs to be specified if the user wants to exclude columns from the set of predictors. If all columns (other than the response) should be used in prediction, then this does not need to be set.  \n",
    "\n",
    "validation_frame: This argument is used to specify the validation frame used for early stopping of individual models and early stopping of the grid searches (unless max_models or max_runtime_secs overrides metric-based early stopping).  \n",
    "\n",
    "leaderboard_frame: This argument allows the user to specify a particular data frame use to score & rank models on the leaderboard. This frame will not be used for anything besides leaderboard scoring. If a leaderboard frame is not specified by the user, then the leaderboard will use cross-validation metrics instead (or if cross-validation is turned off by setting nfolds = 0, then a leaderboard frame will be generated automatically from the validation frame (if provided) or the training frame).  \n",
    "\n",
    "fold_column: Specifies a column with cross-validation fold index assignment per observation. This is used to override the default, randomized, 5-fold cross-validation scheme for individual models in the AutoML run.  \n",
    "\n",
    "weights_column: Specifies a column with observation weights. Giving some observation a weight of zero is equivalent to excluding it from the dataset; giving an observation a relative weight of 2 is equivalent to repeating that row twice. Negative weights are not allowed.  \n",
    "\n",
    "ignored_columns: (Optional, Python only) Specify the column or columns (as a list/vector) to be excluded from the model. This is the converse of the x argument.  \n",
    "\n",
    "**Optional Miscellaneous Parameters**  \n",
    "\n",
    "nfolds: Number of folds for k-fold cross-validation of the models in the AutoML run. Defaults to 5. Use 0 to disable cross-validation; this will also disable Stacked Ensembles (thus decreasing the overall best model performance).\n",
    "\n",
    "balance_classes: Specify whether to oversample the minority classes to balance the class distribution. This option is not enabled by default and can increase the data frame size. This option is only applicable for classification. Majority classes can be undersampled to satisfy the max_after_balance_size parameter.\n",
    "\n",
    "class_sampling_factors: Specify the per-class (in lexicographical order) over/under-sampling ratios. By default, these ratios are automatically computed during training to obtain the class balance.\n",
    "\n",
    "max_after_balance_size: Specify the maximum relative size of the training data after balancing class counts (balance_classes must be enabled). Defaults to 5.0. (The value can be less than 1.0).\n",
    "\n",
    "stopping_metric: Specifies the metric to use for early stopping of the grid searches and individual models. Defaults to \"AUTO\". The available options are:\n",
    "\n",
    "AUTO: This defaults to logloss for classification, deviance for regression\n",
    "deviance (mean residual deviance)\n",
    "logloss\n",
    "MSE\n",
    "RMSE\n",
    "MAE\n",
    "RMSLE\n",
    "AUC\n",
    "lift_top_group\n",
    "misclassification\n",
    "mean_per_class_error  \n",
    "\n",
    "stopping_tolerance: This option specifies the relative tolerance for the metric-based stopping criterion to stop a grid search and the training of individual models within the AutoML run. This value defaults to 0.001 if the dataset is at least 1 million rows; otherwise it defaults to a bigger value determined by the size of the dataset and the non-NA-rate. In that case, the value is computed as 1/sqrt(nrows * non-NA-rate).\n",
    "\n",
    "stopping_rounds: This argument is used to stop model training when the stopping metric (e.g. AUC) doesnâ€™t improve for this specified number of training rounds, based on a simple moving average. In the context of AutoML, this controls early stopping both within the random grid searches as well as the individual models. Defaults to 3 and must be an non-negative integer. To disable early stopping altogether, set this to 0.\n",
    "\n",
    "sort_metric: Specifies the metric used to sort the Leaderboard by at the end of an AutoML run. Available options include:\n",
    "\n",
    "AUTO: This defaults to AUC for binary classification, mean_per_class_error for multinomial classification, and deviance for regression.\n",
    "deviance (mean residual deviance)\n",
    "logloss\n",
    "MSE\n",
    "RMSE\n",
    "MAE\n",
    "RMSLE\n",
    "AUC\n",
    "mean_per_class_error  \n",
    "\n",
    "seed: Integer. Set a seed for reproducibility. AutoML can only guarantee reproducibility if max_models is used because max_runtime_secs is resource limited, meaning that if the available compute resources are not the same between runs, AutoML may be able to train more models on one run vs another. Defaults to NULL/None.\n",
    "\n",
    "project_name: Character string to identify an AutoML project. Defaults to NULL/None, which means a project name will be auto-generated based on the training frame ID. More models can be trained and added to an existing AutoML project by specifying the same project name in muliple calls to the AutoML function (as long as the same training frame is used in subsequent runs).\n",
    "\n",
    "exclude_algos: List/vector of character strings naming the algorithms to skip during the model-building phase. An example use is exclude_algos = [\"GLM\", \"DeepLearning\", \"DRF\"] in Python or exclude_algos = c(\"GLM\", \"DeepLearning\", \"DRF\") in R. Defaults to None/NULL, which means that all appropriate H2O algorithms will be used, if the search stopping criteria allow. The algorithm names are:\n",
    "\n",
    "GLM\n",
    "DeepLearning\n",
    "GBM\n",
    "DRF (This includes both the Random Forest and Extremely Randomized Trees (XRT) models. Refer to the Extremely Randomized Trees section in the DRF chapter and the histogram_type parameter description for more information.)\n",
    "StackedEnsemble\n",
    "keep_cross_validation_predictions: Specify whether to keep the predictions of the cross-validation predictions. If set to FALSE, then running the same AutoML object for repeated runs will cause an exception because CV predictions are are required to build additional Stacked Ensemble models in AutoML. This option defaults to TRUE.\n",
    "\n",
    "\n",
    "keep_cross_validation_models: Specify whether to keep the cross-validated models. Deleting cross-validation models will save memory in the H2O cluster. This option defaults to TRUE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assume the following are passed by the user from the web interface\n",
    "\n",
    "'''\n",
    "Need a user id and project id?\n",
    "\n",
    "'''\n",
    "target='bad_loan' \n",
    "data_file='loan.csv'\n",
    "run_time=360\n",
    "run_id='SOME_ID_20180617_221529' # Just some arbitrary ID\n",
    "server_path='/Users/bear/Documents/INFO_7390/H2O'\n",
    "classification=True\n",
    "scale=False\n",
    "max_models=None\n",
    "balance_y=False # balance_classes=balance_y\n",
    "balance_threshold=0.2\n",
    "project =\"automl_test\"  # project_name = project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bear/Documents/INFO_7390/H2O/loan.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use local data file or download from some type of bucket\n",
    "import os\n",
    "\n",
    "data_path=os.path.join(server_path,data_file)\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100%\n"
     ]
    }
   ],
   "source": [
    "# Use local data file or download from some type of bucket\n",
    "if not os.path.isfile(data_path):\n",
    "  data_path = 'https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv'\n",
    "\n",
    "# Load data into H2O\n",
    "df = h2o.import_file(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:163987\n",
      "Cols:15\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>loan_amnt         </th><th>term     </th><th>int_rate          </th><th>emp_length       </th><th>home_ownership  </th><th>annual_inc       </th><th>purpose           </th><th>addr_state  </th><th>dti               </th><th>delinq_2yrs        </th><th>revol_util        </th><th>total_acc         </th><th>bad_loan          </th><th>longest_credit_length  </th><th>verification_status  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>int               </td><td>enum     </td><td>real              </td><td>int              </td><td>enum            </td><td>real             </td><td>enum              </td><td>enum        </td><td>real              </td><td>int                </td><td>real              </td><td>int               </td><td>int               </td><td>int                    </td><td>enum                 </td></tr>\n",
       "<tr><td>mins   </td><td>500.0             </td><td>         </td><td>5.42              </td><td>0.0              </td><td>                </td><td>1896.0           </td><td>                  </td><td>            </td><td>0.0               </td><td>0.0                </td><td>0.0               </td><td>1.0               </td><td>0.0               </td><td>0.0                    </td><td>                     </td></tr>\n",
       "<tr><td>mean   </td><td>13074.169141456332</td><td>         </td><td>13.715904065566189</td><td>5.684352932995338</td><td>                </td><td>71915.67051974905</td><td>                  </td><td>            </td><td>15.881530121290167</td><td>0.22735700606252723</td><td>54.07917280242262 </td><td>24.579733834274574</td><td>0.1830388994249544</td><td>14.854273655448333     </td><td>                     </td></tr>\n",
       "<tr><td>maxs   </td><td>35000.0           </td><td>         </td><td>26.06             </td><td>10.0             </td><td>                </td><td>7141778.0        </td><td>                  </td><td>            </td><td>39.99             </td><td>29.0               </td><td>150.70000000000002</td><td>118.0             </td><td>1.0               </td><td>65.0                   </td><td>                     </td></tr>\n",
       "<tr><td>sigma  </td><td>7993.556188734672 </td><td>         </td><td>4.391939870545808 </td><td>3.610663731100238</td><td>                </td><td>59070.91565491818</td><td>                  </td><td>            </td><td>7.5876682241925355</td><td>0.6941679229284191 </td><td>25.285366766770498</td><td>11.685190365910666</td><td>0.3866995896078875</td><td>6.947732922546689      </td><td>                     </td></tr>\n",
       "<tr><td>zeros  </td><td>0                 </td><td>         </td><td>0                 </td><td>14248            </td><td>                </td><td>0                </td><td>                  </td><td>            </td><td>270               </td><td>139459             </td><td>1562              </td><td>0                 </td><td>133971            </td><td>11                     </td><td>                     </td></tr>\n",
       "<tr><td>missing</td><td>0                 </td><td>0        </td><td>0                 </td><td>5804             </td><td>0               </td><td>4                </td><td>0                 </td><td>0           </td><td>0                 </td><td>29                 </td><td>193               </td><td>29                </td><td>0                 </td><td>29                     </td><td>0                    </td></tr>\n",
       "<tr><td>0      </td><td>5000.0            </td><td>36 months</td><td>10.65             </td><td>10.0             </td><td>RENT            </td><td>24000.0          </td><td>credit_card       </td><td>AZ          </td><td>27.65             </td><td>0.0                </td><td>83.7              </td><td>9.0               </td><td>0.0               </td><td>26.0                   </td><td>verified             </td></tr>\n",
       "<tr><td>1      </td><td>2500.0            </td><td>60 months</td><td>15.27             </td><td>0.0              </td><td>RENT            </td><td>30000.0          </td><td>car               </td><td>GA          </td><td>1.0               </td><td>0.0                </td><td>9.4               </td><td>4.0               </td><td>1.0               </td><td>12.0                   </td><td>verified             </td></tr>\n",
       "<tr><td>2      </td><td>2400.0            </td><td>36 months</td><td>15.96             </td><td>10.0             </td><td>RENT            </td><td>12252.0          </td><td>small_business    </td><td>IL          </td><td>8.72              </td><td>0.0                </td><td>98.5              </td><td>10.0              </td><td>0.0               </td><td>10.0                   </td><td>not verified         </td></tr>\n",
       "<tr><td>3      </td><td>10000.0           </td><td>36 months</td><td>13.49             </td><td>10.0             </td><td>RENT            </td><td>49200.0          </td><td>other             </td><td>CA          </td><td>20.0              </td><td>0.0                </td><td>21.0              </td><td>37.0              </td><td>0.0               </td><td>15.0                   </td><td>verified             </td></tr>\n",
       "<tr><td>4      </td><td>5000.0            </td><td>36 months</td><td>7.9               </td><td>3.0              </td><td>RENT            </td><td>36000.0          </td><td>wedding           </td><td>AZ          </td><td>11.2              </td><td>0.0                </td><td>28.3              </td><td>12.0              </td><td>0.0               </td><td>7.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>5      </td><td>3000.0            </td><td>36 months</td><td>18.64             </td><td>9.0              </td><td>RENT            </td><td>48000.0          </td><td>car               </td><td>CA          </td><td>5.3500000000000005</td><td>0.0                </td><td>87.5              </td><td>4.0               </td><td>0.0               </td><td>4.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>6      </td><td>5600.0            </td><td>60 months</td><td>21.28             </td><td>4.0              </td><td>OWN             </td><td>40000.0          </td><td>small_business    </td><td>CA          </td><td>5.55              </td><td>0.0                </td><td>32.6              </td><td>13.0              </td><td>1.0               </td><td>7.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>7      </td><td>5375.0            </td><td>60 months</td><td>12.69             </td><td>0.0              </td><td>RENT            </td><td>15000.0          </td><td>other             </td><td>TX          </td><td>18.08             </td><td>0.0                </td><td>36.5              </td><td>3.0               </td><td>1.0               </td><td>7.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>8      </td><td>6500.0            </td><td>60 months</td><td>14.65             </td><td>5.0              </td><td>OWN             </td><td>72000.0          </td><td>debt_consolidation</td><td>AZ          </td><td>16.12             </td><td>0.0                </td><td>20.6              </td><td>23.0              </td><td>0.0               </td><td>13.0                   </td><td>not verified         </td></tr>\n",
       "<tr><td>9      </td><td>12000.0           </td><td>36 months</td><td>12.69             </td><td>10.0             </td><td>OWN             </td><td>75000.0          </td><td>debt_consolidation</td><td>CA          </td><td>10.78             </td><td>0.0                </td><td>67.10000000000001 </td><td>34.0              </td><td>0.0               </td><td>22.0                   </td><td>verified             </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad_loan\n",
      "['loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership', 'annual_inc', 'purpose', 'addr_state', 'dti', 'delinq_2yrs', 'revol_util', 'total_acc', 'longest_credit_length', 'verification_status']\n"
     ]
    }
   ],
   "source": [
    "# assign target and inputs for logistic regression\n",
    "y = target\n",
    "X = [name for name in df.columns if name != y]\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loan_amnt', 'emp_length', 'delinq_2yrs', 'total_acc', 'longest_credit_length']\n",
      "['term', 'home_ownership', 'purpose', 'addr_state', 'verification_status']\n",
      "['int_rate', 'annual_inc', 'dti', 'revol_util']\n"
     ]
    }
   ],
   "source": [
    "# determine column types\n",
    "ints, reals, enums = [], [], []\n",
    "for key, val in df.types.items():\n",
    "    if key in X:\n",
    "        if val == 'enum':\n",
    "            enums.append(key)\n",
    "        elif val == 'int':\n",
    "            ints.append(key)            \n",
    "        else: \n",
    "            reals.append(key)\n",
    "\n",
    "print(ints)\n",
    "print(enums)\n",
    "print(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute missing values\n",
    "_ = df[reals].impute(method='mean')\n",
    "_ = df[ints].impute(method='median')\n",
    "\n",
    "if scale:\n",
    "    df[reals] = df[reals].scale()\n",
    "    df[ints] = df[ints].scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set target to factor for classification by default or if user specifies classification\n",
    "if classification:\n",
    "    df[y] = df[y].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[y].levels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### balance_classes check \n",
    "\n",
    "If one class in two class classification is less than 20% of the total then one should set balance_classes=True\n",
    "\n",
    "That is,\n",
    "\n",
    "balance_classes=balance_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if classification:\n",
    "    class_percentage = y_balance=df[y].mean()[0]/(df[y].max()-df[y].min())\n",
    "    if class_percentage < balance_threshold:\n",
    "        balance_y=True\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(run_time)\n",
    "type(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate rather than take a test training split\n",
    "\n",
    "Cross-validation rather than taking a test training split reduces the variance of the estimates of goodness of fit statistics.  In rare cases one should take a test training split but this should be left to the expert users.\n",
    "\n",
    "This also means the pro user can just upload the data and not worry about taking a test training split.  \n",
    "\n",
    "We can pass the original, full dataset, `df` (without passing a `leaderboard_frame`).  This is a more efficient use of our data since we can use 100% of the data for training, rather than 80% or so.  This time our leaderboard will use cross-validated metrics. It also gives better estimates of goodness of fit statistics.\n",
    "\n",
    "*Note: Using an explicit `leaderboard_frame` for scoring may be useful in some cases, which is why the option is available.*  \n",
    "\n",
    "But it's not preferable in most cases.  Leave it as an expert option.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100%\n"
     ]
    }
   ],
   "source": [
    "# automl\n",
    "# runs for run_time seconds then builds a stacked ensemble\n",
    "aml = H2OAutoML(max_runtime_secs=run_time,project_name = project,balance_classes=balance_y) # init automl, run for 300 seconds\n",
    "aml.train(x=X,  \n",
    "           y=y,\n",
    "           training_frame=df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaderboard\n",
    "\n",
    "Next, we will view the AutoML Leaderboard.  Since we did not specify a `leaderboard_frame` in the `H2OAutoML.train()` method for scoring and ranking the models, the AutoML leaderboard uses cross-validation metrics to rank the models.  \n",
    "\n",
    "A default performance metric for each machine learning task (binary classification, multiclass classification, regression) is specified internally and the leaderboard will be sorted by that metric.  In the case of binary classification, the default ranking metric is Area Under the ROC Curve (AUC).  In the future, the user will be able to specify any of the H2O metrics so that different metrics can be used to generate rankings on the leaderboard.\n",
    "\n",
    "The leader model is stored at `aml.leader` and the leaderboard is stored at `aml.leaderboard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                             </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_AllModels_0_AutoML_20180625_175234   </td><td style=\"text-align: right;\">0.70521 </td><td style=\"text-align: right;\"> 0.436573</td><td style=\"text-align: right;\">              0.352879</td><td style=\"text-align: right;\">0.37056 </td><td style=\"text-align: right;\">0.137314</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_0_AutoML_20180625_175234</td><td style=\"text-align: right;\">0.704911</td><td style=\"text-align: right;\"> 0.436754</td><td style=\"text-align: right;\">              0.35223 </td><td style=\"text-align: right;\">0.37064 </td><td style=\"text-align: right;\">0.137374</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180625_175234_model_0            </td><td style=\"text-align: right;\">0.703447</td><td style=\"text-align: right;\"> 0.435224</td><td style=\"text-align: right;\">              0.352229</td><td style=\"text-align: right;\">0.370297</td><td style=\"text-align: right;\">0.13712 </td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180625_175234_model_1            </td><td style=\"text-align: right;\">0.701244</td><td style=\"text-align: right;\"> 0.436209</td><td style=\"text-align: right;\">              0.356297</td><td style=\"text-align: right;\">0.370641</td><td style=\"text-align: right;\">0.137375</td></tr>\n",
       "<tr><td>GLM_grid_0_AutoML_20180625_175234_model_0            </td><td style=\"text-align: right;\">0.697503</td><td style=\"text-align: right;\"> 0.438455</td><td style=\"text-align: right;\">              0.354668</td><td style=\"text-align: right;\">0.371441</td><td style=\"text-align: right;\">0.137968</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180625_175234_model_2            </td><td style=\"text-align: right;\">0.697313</td><td style=\"text-align: right;\"> 0.438347</td><td style=\"text-align: right;\">              0.356853</td><td style=\"text-align: right;\">0.3714  </td><td style=\"text-align: right;\">0.137938</td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20180625_175234_model_3            </td><td style=\"text-align: right;\">0.6889  </td><td style=\"text-align: right;\"> 0.444238</td><td style=\"text-align: right;\">              0.364317</td><td style=\"text-align: right;\">0.373402</td><td style=\"text-align: right;\">0.139429</td></tr>\n",
       "<tr><td>XRT_0_AutoML_20180625_175234                         </td><td style=\"text-align: right;\">0.688545</td><td style=\"text-align: right;\"> 0.47186 </td><td style=\"text-align: right;\">              0.366467</td><td style=\"text-align: right;\">0.382357</td><td style=\"text-align: right;\">0.146197</td></tr>\n",
       "<tr><td>DRF_0_AutoML_20180625_175234                         </td><td style=\"text-align: right;\">0.685141</td><td style=\"text-align: right;\"> 0.479268</td><td style=\"text-align: right;\">              0.364523</td><td style=\"text-align: right;\">0.383454</td><td style=\"text-align: right;\">0.147037</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will view a snapshot of the top models.  Here we should see the two Stacked Ensembles at or near the top of the leaderboard.  Stacked Ensembles can almost always outperform a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_0_AutoML_20180625_175234\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.1167639474829593\n",
      "RMSE: 0.34170740039244\n",
      "LogLoss: 0.3795990920019023\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130948\n",
      "Null deviance: 124374.13927893189\n",
      "Residual deviance: 99420.79818621822\n",
      "AIC: 99434.79818621822\n",
      "AUC: 0.8305862969853443\n",
      "Gini: 0.6611725939706885\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24030127929039052: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>90474.0</td>\n",
       "<td>16607.0</td>\n",
       "<td>0.1551</td>\n",
       "<td> (16607.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9334.0</td>\n",
       "<td>14540.0</td>\n",
       "<td>0.391</td>\n",
       "<td> (9334.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>99808.0</td>\n",
       "<td>31147.0</td>\n",
       "<td>0.1981</td>\n",
       "<td> (25941.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      90474  16607  0.1551   (16607.0/107081.0)\n",
       "1      9334   14540  0.391    (9334.0/23874.0)\n",
       "Total  99808  31147  0.1981   (25941.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2403013</td>\n",
       "<td>0.5285255</td>\n",
       "<td>222.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1529696</td>\n",
       "<td>0.6516791</td>\n",
       "<td>296.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3520442</td>\n",
       "<td>0.5439141</td>\n",
       "<td>151.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4117837</td>\n",
       "<td>0.8448704</td>\n",
       "<td>121.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8488098</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0753915</td>\n",
       "<td>1.0</td>\n",
       "<td>387.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8488098</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2828681</td>\n",
       "<td>0.4124344</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1895643</td>\n",
       "<td>0.7444458</td>\n",
       "<td>262.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1801546</td>\n",
       "<td>0.7459682</td>\n",
       "<td>270.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.240301     0.528525  222\n",
       "max f2                       0.15297      0.651679  296\n",
       "max f0point5                 0.352044     0.543914  151\n",
       "max accuracy                 0.411784     0.84487   121\n",
       "max precision                0.84881      1         0\n",
       "max recall                   0.0753915    1         387\n",
       "max specificity              0.84881      1         0\n",
       "max absolute_mcc             0.282868     0.412434  193\n",
       "max min_per_class_accuracy   0.189564     0.744446  262\n",
       "max mean_per_class_accuracy  0.180155     0.745968  270"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.6271502</td>\n",
       "<td>5.1377168</td>\n",
       "<td>5.1377168</td>\n",
       "<td>0.9366412</td>\n",
       "<td>0.9366412</td>\n",
       "<td>0.0513948</td>\n",
       "<td>0.0513948</td>\n",
       "<td>413.7716811</td>\n",
       "<td>413.7716811</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5662824</td>\n",
       "<td>4.4091408</td>\n",
       "<td>4.7734288</td>\n",
       "<td>0.8038168</td>\n",
       "<td>0.8702290</td>\n",
       "<td>0.0441066</td>\n",
       "<td>0.0955014</td>\n",
       "<td>340.9140833</td>\n",
       "<td>377.3428822</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.5252263</td>\n",
       "<td>4.0437525</td>\n",
       "<td>4.5303272</td>\n",
       "<td>0.7372040</td>\n",
       "<td>0.8259099</td>\n",
       "<td>0.0404205</td>\n",
       "<td>0.1359219</td>\n",
       "<td>304.3752460</td>\n",
       "<td>353.0327178</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4929238</td>\n",
       "<td>3.4921400</td>\n",
       "<td>4.2707309</td>\n",
       "<td>0.6366412</td>\n",
       "<td>0.7785837</td>\n",
       "<td>0.0349334</td>\n",
       "<td>0.1708553</td>\n",
       "<td>249.2140033</td>\n",
       "<td>327.0730851</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4650058</td>\n",
       "<td>3.3774761</td>\n",
       "<td>4.0921618</td>\n",
       "<td>0.6157372</td>\n",
       "<td>0.7460293</td>\n",
       "<td>0.0337606</td>\n",
       "<td>0.2046159</td>\n",
       "<td>237.7476148</td>\n",
       "<td>309.2161760</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3687602</td>\n",
       "<td>2.8247225</td>\n",
       "<td>3.4584421</td>\n",
       "<td>0.5149664</td>\n",
       "<td>0.6304979</td>\n",
       "<td>0.1412415</td>\n",
       "<td>0.3458574</td>\n",
       "<td>182.4722509</td>\n",
       "<td>245.8442134</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.3073983</td>\n",
       "<td>2.2492230</td>\n",
       "<td>3.0553691</td>\n",
       "<td>0.4100489</td>\n",
       "<td>0.5570149</td>\n",
       "<td>0.1124654</td>\n",
       "<td>0.4583229</td>\n",
       "<td>124.9222994</td>\n",
       "<td>205.5369087</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2647046</td>\n",
       "<td>1.8122206</td>\n",
       "<td>2.7446176</td>\n",
       "<td>0.3303803</td>\n",
       "<td>0.5003627</td>\n",
       "<td>0.0906007</td>\n",
       "<td>0.5489235</td>\n",
       "<td>81.2220646</td>\n",
       "<td>174.4617576</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2073810</td>\n",
       "<td>1.4353980</td>\n",
       "<td>2.3081999</td>\n",
       "<td>0.2616830</td>\n",
       "<td>0.4208008</td>\n",
       "<td>0.1435453</td>\n",
       "<td>0.6924688</td>\n",
       "<td>43.5397989</td>\n",
       "<td>130.8199939</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1707956</td>\n",
       "<td>1.0878358</td>\n",
       "<td>2.0031205</td>\n",
       "<td>0.1983200</td>\n",
       "<td>0.3651827</td>\n",
       "<td>0.1087794</td>\n",
       "<td>0.8012482</td>\n",
       "<td>8.7835788</td>\n",
       "<td>100.3120550</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1453638</td>\n",
       "<td>0.7723589</td>\n",
       "<td>1.7569607</td>\n",
       "<td>0.1408064</td>\n",
       "<td>0.3203061</td>\n",
       "<td>0.0772388</td>\n",
       "<td>0.8784871</td>\n",
       "<td>-22.7641117</td>\n",
       "<td>75.6960698</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1262417</td>\n",
       "<td>0.5579504</td>\n",
       "<td>1.5571333</td>\n",
       "<td>0.1017182</td>\n",
       "<td>0.2838761</td>\n",
       "<td>0.0557929</td>\n",
       "<td>0.9342800</td>\n",
       "<td>-44.2049569</td>\n",
       "<td>55.7133283</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1101940</td>\n",
       "<td>0.3535362</td>\n",
       "<td>1.3851965</td>\n",
       "<td>0.0644521</td>\n",
       "<td>0.2525309</td>\n",
       "<td>0.0353523</td>\n",
       "<td>0.9696322</td>\n",
       "<td>-64.6463841</td>\n",
       "<td>38.5196464</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0961032</td>\n",
       "<td>0.2173830</td>\n",
       "<td>1.2392142</td>\n",
       "<td>0.0396304</td>\n",
       "<td>0.2259173</td>\n",
       "<td>0.0217391</td>\n",
       "<td>0.9913714</td>\n",
       "<td>-78.2616996</td>\n",
       "<td>23.9214208</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0822907</td>\n",
       "<td>0.0783309</td>\n",
       "<td>1.1102315</td>\n",
       "<td>0.0142803</td>\n",
       "<td>0.2024029</td>\n",
       "<td>0.0078328</td>\n",
       "<td>0.9992042</td>\n",
       "<td>-92.1669121</td>\n",
       "<td>11.0231549</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0614696</td>\n",
       "<td>0.0079581</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0014508</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0007958</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.2041855</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ----------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.62715            5.13772     5.13772            0.936641         0.936641                    0.0513948       0.0513948                  413.772   413.772\n",
       "    2        0.0200069                   0.566282           4.40914     4.77343            0.803817         0.870229                    0.0441066       0.0955014                  340.914   377.343\n",
       "    3        0.0300027                   0.525226           4.04375     4.53033            0.737204         0.82591                     0.0404205       0.135922                   304.375   353.033\n",
       "    4        0.0400061                   0.492924           3.49214     4.27073            0.636641         0.778584                    0.0349334       0.170855                   249.214   327.073\n",
       "    5        0.0500019                   0.465006           3.37748     4.09216            0.615737         0.746029                    0.0337606       0.204616                   237.748   309.216\n",
       "    6        0.100004                    0.36876            2.82472     3.45844            0.514966         0.630498                    0.141242        0.345857                   182.472   245.844\n",
       "    7        0.150006                    0.307398           2.24922     3.05537            0.410049         0.557015                    0.112465        0.458323                   124.922   205.537\n",
       "    8        0.2                         0.264705           1.81222     2.74462            0.33038          0.500363                    0.0906007       0.548924                   81.2221   174.462\n",
       "    9        0.300004                    0.207381           1.4354      2.3082             0.261683         0.420801                    0.143545        0.692469                   43.5398   130.82\n",
       "    10       0.4                         0.170796           1.08784     2.00312            0.19832          0.365183                    0.108779        0.801248                   8.78358   100.312\n",
       "    11       0.500004                    0.145364           0.772359    1.75696            0.140806         0.320306                    0.0772388       0.878487                   -22.7641  75.6961\n",
       "    12       0.6                         0.126242           0.55795     1.55713            0.101718         0.283876                    0.0557929       0.93428                    -44.205   55.7133\n",
       "    13       0.699996                    0.110194           0.353536    1.3852             0.0644521        0.252531                    0.0353523       0.969632                   -64.6464  38.5196\n",
       "    14       0.8                         0.0961032          0.217383    1.23921            0.0396304        0.225917                    0.0217391       0.991371                   -78.2617  23.9214\n",
       "    15       0.899996                    0.0822907          0.0783309   1.11023            0.0142803        0.202403                    0.00783279      0.999204                   -92.1669  11.0232\n",
       "    16       1                           0.0614696          0.00795814  1                  0.00145082       0.182307                    0.000795845     1                          -99.2042  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.1386513474415332\n",
      "RMSE: 0.37235916457304125\n",
      "LogLoss: 0.4394559813847542\n",
      "Null degrees of freedom: 33031\n",
      "Residual degrees of freedom: 33025\n",
      "Null deviance: 31732.354674481932\n",
      "Residual deviance: 29032.219954202403\n",
      "AIC: 29046.219954202403\n",
      "AUC: 0.7102666513197816\n",
      "Gini: 0.42053330263956323\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.18707726312815404: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>19160.0</td>\n",
       "<td>7730.0</td>\n",
       "<td>0.2875</td>\n",
       "<td> (7730.0/26890.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2529.0</td>\n",
       "<td>3613.0</td>\n",
       "<td>0.4118</td>\n",
       "<td> (2529.0/6142.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>21689.0</td>\n",
       "<td>11343.0</td>\n",
       "<td>0.3106</td>\n",
       "<td> (10259.0/33032.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      19160  7730   0.2875   (7730.0/26890.0)\n",
       "1      2529   3613   0.4118   (2529.0/6142.0)\n",
       "Total  21689  11343  0.3106   (10259.0/33032.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1870773</td>\n",
       "<td>0.4132685</td>\n",
       "<td>255.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1147101</td>\n",
       "<td>0.5726111</td>\n",
       "<td>334.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2910025</td>\n",
       "<td>0.3779960</td>\n",
       "<td>173.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5696954</td>\n",
       "<td>0.8161177</td>\n",
       "<td>37.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8006882</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0668733</td>\n",
       "<td>1.0</td>\n",
       "<td>396.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8006882</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1870773</td>\n",
       "<td>0.2464403</td>\n",
       "<td>255.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1677169</td>\n",
       "<td>0.6499442</td>\n",
       "<td>273.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1499723</td>\n",
       "<td>0.6538983</td>\n",
       "<td>291.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.187077     0.413269  255\n",
       "max f2                       0.11471      0.572611  334\n",
       "max f0point5                 0.291002     0.377996  173\n",
       "max accuracy                 0.569695     0.816118  37\n",
       "max precision                0.800688     1         0\n",
       "max recall                   0.0668733    1         396\n",
       "max specificity              0.800688     1         0\n",
       "max absolute_mcc             0.187077     0.24644   255\n",
       "max min_per_class_accuracy   0.167717     0.649944  273\n",
       "max mean_per_class_accuracy  0.149972     0.653898  291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.59 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100206</td>\n",
       "<td>0.5870321</td>\n",
       "<td>3.2008350</td>\n",
       "<td>3.2008350</td>\n",
       "<td>0.5951662</td>\n",
       "<td>0.5951662</td>\n",
       "<td>0.0320742</td>\n",
       "<td>0.0320742</td>\n",
       "<td>220.0835021</td>\n",
       "<td>220.0835021</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200109</td>\n",
       "<td>0.5271460</td>\n",
       "<td>2.5260551</td>\n",
       "<td>2.8639555</td>\n",
       "<td>0.4696970</td>\n",
       "<td>0.5325265</td>\n",
       "<td>0.0252361</td>\n",
       "<td>0.0573103</td>\n",
       "<td>152.6055080</td>\n",
       "<td>186.3955474</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300012</td>\n",
       "<td>0.4907770</td>\n",
       "<td>2.3467867</td>\n",
       "<td>2.6917398</td>\n",
       "<td>0.4363636</td>\n",
       "<td>0.5005045</td>\n",
       "<td>0.0234451</td>\n",
       "<td>0.0807555</td>\n",
       "<td>134.6786655</td>\n",
       "<td>169.1739823</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400218</td>\n",
       "<td>0.4607515</td>\n",
       "<td>2.4209361</td>\n",
       "<td>2.6239365</td>\n",
       "<td>0.4501511</td>\n",
       "<td>0.4878971</td>\n",
       "<td>0.0242592</td>\n",
       "<td>0.1050147</td>\n",
       "<td>142.0936133</td>\n",
       "<td>162.3936479</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500121</td>\n",
       "<td>0.4368630</td>\n",
       "<td>2.2490039</td>\n",
       "<td>2.5490407</td>\n",
       "<td>0.4181818</td>\n",
       "<td>0.4739709</td>\n",
       "<td>0.0224683</td>\n",
       "<td>0.1274829</td>\n",
       "<td>124.9003878</td>\n",
       "<td>154.9040741</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000242</td>\n",
       "<td>0.3510749</td>\n",
       "<td>2.0086311</td>\n",
       "<td>2.2788359</td>\n",
       "<td>0.3734867</td>\n",
       "<td>0.4237288</td>\n",
       "<td>0.1004559</td>\n",
       "<td>0.2279388</td>\n",
       "<td>100.8631082</td>\n",
       "<td>127.8835912</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500061</td>\n",
       "<td>0.2954589</td>\n",
       "<td>1.8111431</td>\n",
       "<td>2.1230012</td>\n",
       "<td>0.3367656</td>\n",
       "<td>0.3947528</td>\n",
       "<td>0.0905243</td>\n",
       "<td>0.3184630</td>\n",
       "<td>81.1143143</td>\n",
       "<td>112.3001248</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000182</td>\n",
       "<td>0.2562466</td>\n",
       "<td>1.5723968</td>\n",
       "<td>1.9853293</td>\n",
       "<td>0.2923729</td>\n",
       "<td>0.3691539</td>\n",
       "<td>0.0786389</td>\n",
       "<td>0.3971019</td>\n",
       "<td>57.2396779</td>\n",
       "<td>98.5329296</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000121</td>\n",
       "<td>0.2033653</td>\n",
       "<td>1.3644590</td>\n",
       "<td>1.7783934</td>\n",
       "<td>0.2537087</td>\n",
       "<td>0.3306761</td>\n",
       "<td>0.1364376</td>\n",
       "<td>0.5335396</td>\n",
       "<td>36.4459039</td>\n",
       "<td>77.8393428</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000061</td>\n",
       "<td>0.1689245</td>\n",
       "<td>1.1153394</td>\n",
       "<td>1.6126425</td>\n",
       "<td>0.2073872</td>\n",
       "<td>0.2998562</td>\n",
       "<td>0.1115272</td>\n",
       "<td>0.6450668</td>\n",
       "<td>11.5339429</td>\n",
       "<td>61.2642473</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1444336</td>\n",
       "<td>1.0290431</td>\n",
       "<td>1.4959297</td>\n",
       "<td>0.1913412</td>\n",
       "<td>0.2781545</td>\n",
       "<td>0.1028981</td>\n",
       "<td>0.7479648</td>\n",
       "<td>2.9043094</td>\n",
       "<td>49.5929665</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999939</td>\n",
       "<td>0.1259430</td>\n",
       "<td>0.7652694</td>\n",
       "<td>1.3741591</td>\n",
       "<td>0.1422949</td>\n",
       "<td>0.2555124</td>\n",
       "<td>0.0765223</td>\n",
       "<td>0.8244871</td>\n",
       "<td>-23.4730611</td>\n",
       "<td>37.4159097</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999879</td>\n",
       "<td>0.1109070</td>\n",
       "<td>0.6415237</td>\n",
       "<td>1.2695014</td>\n",
       "<td>0.1192855</td>\n",
       "<td>0.2360522</td>\n",
       "<td>0.0641485</td>\n",
       "<td>0.8886356</td>\n",
       "<td>-35.8476299</td>\n",
       "<td>26.9501424</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999818</td>\n",
       "<td>0.0968897</td>\n",
       "<td>0.5259192</td>\n",
       "<td>1.1765572</td>\n",
       "<td>0.0977899</td>\n",
       "<td>0.2187701</td>\n",
       "<td>0.0525887</td>\n",
       "<td>0.9412244</td>\n",
       "<td>-47.4080824</td>\n",
       "<td>17.6557160</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999758</td>\n",
       "<td>0.0829745</td>\n",
       "<td>0.4054300</td>\n",
       "<td>1.0908792</td>\n",
       "<td>0.0753860</td>\n",
       "<td>0.2028391</td>\n",
       "<td>0.0405405</td>\n",
       "<td>0.9817649</td>\n",
       "<td>-59.4570047</td>\n",
       "<td>9.0879242</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0624311</td>\n",
       "<td>0.1823069</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0338983</td>\n",
       "<td>0.1859409</td>\n",
       "<td>0.0182351</td>\n",
       "<td>1.0</td>\n",
       "<td>-81.7693127</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100206                   0.587032           3.20084   3.20084            0.595166         0.595166                    0.0320742       0.0320742                  220.084   220.084\n",
       "    2        0.0200109                   0.527146           2.52606   2.86396            0.469697         0.532526                    0.0252361       0.0573103                  152.606   186.396\n",
       "    3        0.0300012                   0.490777           2.34679   2.69174            0.436364         0.500505                    0.0234451       0.0807555                  134.679   169.174\n",
       "    4        0.0400218                   0.460751           2.42094   2.62394            0.450151         0.487897                    0.0242592       0.105015                   142.094   162.394\n",
       "    5        0.0500121                   0.436863           2.249     2.54904            0.418182         0.473971                    0.0224683       0.127483                   124.9     154.904\n",
       "    6        0.100024                    0.351075           2.00863   2.27884            0.373487         0.423729                    0.100456        0.227939                   100.863   127.884\n",
       "    7        0.150006                    0.295459           1.81114   2.123              0.336766         0.394753                    0.0905243       0.318463                   81.1143   112.3\n",
       "    8        0.200018                    0.256247           1.5724    1.98533            0.292373         0.369154                    0.0786389       0.397102                   57.2397   98.5329\n",
       "    9        0.300012                    0.203365           1.36446   1.77839            0.253709         0.330676                    0.136438        0.53354                    36.4459   77.8393\n",
       "    10       0.400006                    0.168924           1.11534   1.61264            0.207387         0.299856                    0.111527        0.645067                   11.5339   61.2642\n",
       "    11       0.5                         0.144434           1.02904   1.49593            0.191341         0.278155                    0.102898        0.747965                   2.90431   49.593\n",
       "    12       0.599994                    0.125943           0.765269  1.37416            0.142295         0.255512                    0.0765223       0.824487                   -23.4731  37.4159\n",
       "    13       0.699988                    0.110907           0.641524  1.2695             0.119285         0.236052                    0.0641485       0.888636                   -35.8476  26.9501\n",
       "    14       0.799982                    0.0968897          0.525919  1.17656            0.0977899        0.21877                     0.0525887       0.941224                   -47.4081  17.6557\n",
       "    15       0.899976                    0.0829745          0.40543   1.09088            0.075386         0.202839                    0.0405405       0.981765                   -59.457   9.08792\n",
       "    16       1                           0.0624311          0.182307  1                  0.0338983        0.185941                    0.0182351       1                          -81.7693  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.13731436524493457\n",
      "RMSE: 0.37055952996102337\n",
      "LogLoss: 0.4365729016322642\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130947\n",
      "Null deviance: 124375.46514588114\n",
      "Residual deviance: 114342.80866650632\n",
      "AIC: 114358.80866650632\n",
      "AUC: 0.7052102719993631\n",
      "Gini: 0.4104205439987263\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.1834653742477123: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>75544.0</td>\n",
       "<td>31537.0</td>\n",
       "<td>0.2945</td>\n",
       "<td> (31537.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9818.0</td>\n",
       "<td>14056.0</td>\n",
       "<td>0.4112</td>\n",
       "<td> (9818.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>85362.0</td>\n",
       "<td>45593.0</td>\n",
       "<td>0.3158</td>\n",
       "<td> (41355.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      75544  31537  0.2945   (31537.0/107081.0)\n",
       "1      9818   14056  0.4112   (9818.0/23874.0)\n",
       "Total  85362  45593  0.3158   (41355.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1834654</td>\n",
       "<td>0.4046814</td>\n",
       "<td>261.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1142473</td>\n",
       "<td>0.5642710</td>\n",
       "<td>335.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2981927</td>\n",
       "<td>0.3695123</td>\n",
       "<td>172.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6033991</td>\n",
       "<td>0.8184109</td>\n",
       "<td>29.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8158356</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0635534</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8158356</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2024945</td>\n",
       "<td>0.2396925</td>\n",
       "<td>244.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1661343</td>\n",
       "<td>0.6488453</td>\n",
       "<td>278.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1577254</td>\n",
       "<td>0.6492678</td>\n",
       "<td>287.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.183465     0.404681  261\n",
       "max f2                       0.114247     0.564271  335\n",
       "max f0point5                 0.298193     0.369512  172\n",
       "max accuracy                 0.603399     0.818411  29\n",
       "max precision                0.815836     1         0\n",
       "max recall                   0.0635534    1         399\n",
       "max specificity              0.815836     1         0\n",
       "max absolute_mcc             0.202495     0.239692  244\n",
       "max min_per_class_accuracy   0.166134     0.648845  278\n",
       "max mean_per_class_accuracy  0.157725     0.649268  287"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5751406</td>\n",
       "<td>2.9143039</td>\n",
       "<td>2.9143039</td>\n",
       "<td>0.5312977</td>\n",
       "<td>0.5312977</td>\n",
       "<td>0.0291531</td>\n",
       "<td>0.0291531</td>\n",
       "<td>191.4303912</td>\n",
       "<td>191.4303912</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5209152</td>\n",
       "<td>2.6044498</td>\n",
       "<td>2.7593768</td>\n",
       "<td>0.4748092</td>\n",
       "<td>0.5030534</td>\n",
       "<td>0.0260534</td>\n",
       "<td>0.0552065</td>\n",
       "<td>160.4449761</td>\n",
       "<td>175.9376837</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.4825780</td>\n",
       "<td>2.3173006</td>\n",
       "<td>2.6120931</td>\n",
       "<td>0.4224599</td>\n",
       "<td>0.4762026</td>\n",
       "<td>0.0231633</td>\n",
       "<td>0.0783698</td>\n",
       "<td>131.7300632</td>\n",
       "<td>161.2093113</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4532771</td>\n",
       "<td>2.3867144</td>\n",
       "<td>2.5557377</td>\n",
       "<td>0.4351145</td>\n",
       "<td>0.4659286</td>\n",
       "<td>0.0238753</td>\n",
       "<td>0.1022451</td>\n",
       "<td>138.6714411</td>\n",
       "<td>155.5737682</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4296896</td>\n",
       "<td>2.2460635</td>\n",
       "<td>2.4938312</td>\n",
       "<td>0.4094729</td>\n",
       "<td>0.4546426</td>\n",
       "<td>0.0224512</td>\n",
       "<td>0.1246963</td>\n",
       "<td>124.6063542</td>\n",
       "<td>149.3831230</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3444799</td>\n",
       "<td>2.0850339</td>\n",
       "<td>2.2894326</td>\n",
       "<td>0.3801161</td>\n",
       "<td>0.4173794</td>\n",
       "<td>0.1042557</td>\n",
       "<td>0.2289520</td>\n",
       "<td>108.5033904</td>\n",
       "<td>128.9432567</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.2913521</td>\n",
       "<td>1.7440902</td>\n",
       "<td>2.1076518</td>\n",
       "<td>0.3179597</td>\n",
       "<td>0.3842395</td>\n",
       "<td>0.0872078</td>\n",
       "<td>0.3161598</td>\n",
       "<td>74.4090232</td>\n",
       "<td>110.7651789</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2530770</td>\n",
       "<td>1.5650616</td>\n",
       "<td>1.9720198</td>\n",
       "<td>0.2853215</td>\n",
       "<td>0.3595128</td>\n",
       "<td>0.0782441</td>\n",
       "<td>0.3944040</td>\n",
       "<td>56.5061566</td>\n",
       "<td>97.2019770</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2009810</td>\n",
       "<td>1.3796910</td>\n",
       "<td>1.7745718</td>\n",
       "<td>0.2515272</td>\n",
       "<td>0.3235167</td>\n",
       "<td>0.1379744</td>\n",
       "<td>0.5323783</td>\n",
       "<td>37.9690976</td>\n",
       "<td>77.4571813</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1671209</td>\n",
       "<td>1.1100365</td>\n",
       "<td>1.6084443</td>\n",
       "<td>0.2023673</td>\n",
       "<td>0.2932305</td>\n",
       "<td>0.1109994</td>\n",
       "<td>0.6433777</td>\n",
       "<td>11.0036518</td>\n",
       "<td>60.8444333</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1431993</td>\n",
       "<td>0.9369668</td>\n",
       "<td>1.4741447</td>\n",
       "<td>0.1708155</td>\n",
       "<td>0.2687468</td>\n",
       "<td>0.0937003</td>\n",
       "<td>0.7370780</td>\n",
       "<td>-6.3033177</td>\n",
       "<td>47.4144729</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1250298</td>\n",
       "<td>0.8076039</td>\n",
       "<td>1.3630588</td>\n",
       "<td>0.1472318</td>\n",
       "<td>0.2484950</td>\n",
       "<td>0.0807573</td>\n",
       "<td>0.8178353</td>\n",
       "<td>-19.2396073</td>\n",
       "<td>36.3058837</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1098459</td>\n",
       "<td>0.6635086</td>\n",
       "<td>1.2631264</td>\n",
       "<td>0.1209622</td>\n",
       "<td>0.2302767</td>\n",
       "<td>0.0663483</td>\n",
       "<td>0.8841836</td>\n",
       "<td>-33.6491379</td>\n",
       "<td>26.3126362</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0960804</td>\n",
       "<td>0.5298449</td>\n",
       "<td>1.1714627</td>\n",
       "<td>0.0965944</td>\n",
       "<td>0.2135657</td>\n",
       "<td>0.0529865</td>\n",
       "<td>0.9371701</td>\n",
       "<td>-47.0155105</td>\n",
       "<td>17.1462679</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0823591</td>\n",
       "<td>0.4163684</td>\n",
       "<td>1.0875662</td>\n",
       "<td>0.0759068</td>\n",
       "<td>0.1982708</td>\n",
       "<td>0.0416353</td>\n",
       "<td>0.9788054</td>\n",
       "<td>-58.3631585</td>\n",
       "<td>8.7566164</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0614275</td>\n",
       "<td>0.2119380</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0386378</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0211946</td>\n",
       "<td>1.0</td>\n",
       "<td>-78.8062042</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.575141           2.9143    2.9143             0.531298         0.531298                    0.0291531       0.0291531                  191.43    191.43\n",
       "    2        0.0200069                   0.520915           2.60445   2.75938            0.474809         0.503053                    0.0260534       0.0552065                  160.445   175.938\n",
       "    3        0.0300027                   0.482578           2.3173    2.61209            0.42246          0.476203                    0.0231633       0.0783698                  131.73    161.209\n",
       "    4        0.0400061                   0.453277           2.38671   2.55574            0.435115         0.465929                    0.0238753       0.102245                   138.671   155.574\n",
       "    5        0.0500019                   0.42969            2.24606   2.49383            0.409473         0.454643                    0.0224512       0.124696                   124.606   149.383\n",
       "    6        0.100004                    0.34448            2.08503   2.28943            0.380116         0.417379                    0.104256        0.228952                   108.503   128.943\n",
       "    7        0.150006                    0.291352           1.74409   2.10765            0.31796          0.384239                    0.0872078       0.31616                    74.409    110.765\n",
       "    8        0.2                         0.253077           1.56506   1.97202            0.285322         0.359513                    0.0782441       0.394404                   56.5062   97.202\n",
       "    9        0.300004                    0.200981           1.37969   1.77457            0.251527         0.323517                    0.137974        0.532378                   37.9691   77.4572\n",
       "    10       0.4                         0.167121           1.11004   1.60844            0.202367         0.29323                     0.110999        0.643378                   11.0037   60.8444\n",
       "    11       0.500004                    0.143199           0.936967  1.47414            0.170816         0.268747                    0.0937003       0.737078                   -6.30332  47.4145\n",
       "    12       0.6                         0.12503            0.807604  1.36306            0.147232         0.248495                    0.0807573       0.817835                   -19.2396  36.3059\n",
       "    13       0.699996                    0.109846           0.663509  1.26313            0.120962         0.230277                    0.0663483       0.884184                   -33.6491  26.3126\n",
       "    14       0.8                         0.0960804          0.529845  1.17146            0.0965944        0.213566                    0.0529865       0.93717                    -47.0155  17.1463\n",
       "    15       0.899996                    0.0823591          0.416368  1.08757            0.0759068        0.198271                    0.0416353       0.978805                   -58.3632  8.75662\n",
       "    16       1                           0.0614275          0.211938  1                  0.0386378        0.182307                    0.0211946       1                          -78.8062  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stackedensemble'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_check_targets',\n",
       " '_compute_algo',\n",
       " '_estimator_type',\n",
       " '_future',\n",
       " '_get_metrics',\n",
       " '_have_mojo',\n",
       " '_have_pojo',\n",
       " '_id',\n",
       " '_is_xvalidated',\n",
       " '_job',\n",
       " '_keyify_if_h2oframe',\n",
       " '_metrics_class',\n",
       " '_model_json',\n",
       " '_parms',\n",
       " '_plot',\n",
       " '_requires_training_frame',\n",
       " '_resolve_model',\n",
       " '_verify_training_frame_params',\n",
       " '_xval_keys',\n",
       " 'accuracy',\n",
       " 'actual_params',\n",
       " 'aic',\n",
       " 'algo',\n",
       " 'auc',\n",
       " 'base_models',\n",
       " 'biases',\n",
       " 'catoffsets',\n",
       " 'coef',\n",
       " 'coef_norm',\n",
       " 'confusion_matrix',\n",
       " 'cross_validation_fold_assignment',\n",
       " 'cross_validation_holdout_predictions',\n",
       " 'cross_validation_metrics_summary',\n",
       " 'cross_validation_models',\n",
       " 'cross_validation_predictions',\n",
       " 'deepfeatures',\n",
       " 'default_params',\n",
       " 'download_mojo',\n",
       " 'download_pojo',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fit',\n",
       " 'fnr',\n",
       " 'fpr',\n",
       " 'full_parameters',\n",
       " 'gains_lift',\n",
       " 'get_params',\n",
       " 'get_xval_models',\n",
       " 'gini',\n",
       " 'have_mojo',\n",
       " 'have_pojo',\n",
       " 'is_cross_validated',\n",
       " 'join',\n",
       " 'keep_levelone_frame',\n",
       " 'levelone_frame_id',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'max_per_class_error',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metalearner',\n",
       " 'metalearner_algorithm',\n",
       " 'metalearner_fold_assignment',\n",
       " 'metalearner_fold_column',\n",
       " 'metalearner_nfolds',\n",
       " 'metalearner_params',\n",
       " 'metric',\n",
       " 'missrate',\n",
       " 'mixin',\n",
       " 'model_id',\n",
       " 'model_performance',\n",
       " 'mse',\n",
       " 'normmul',\n",
       " 'normsub',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'params',\n",
       " 'parms',\n",
       " 'partial_plot',\n",
       " 'plot',\n",
       " 'pprint_coef',\n",
       " 'precision',\n",
       " 'predict',\n",
       " 'predict_leaf_node_assignment',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'respmul',\n",
       " 'response_column',\n",
       " 'respsub',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'roc',\n",
       " 'rotation',\n",
       " 'save_model_details',\n",
       " 'save_mojo',\n",
       " 'score_history',\n",
       " 'scoring_history',\n",
       " 'seed',\n",
       " 'sensitivity',\n",
       " 'set_params',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'start',\n",
       " 'std_coef_plot',\n",
       " 'summary',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'train',\n",
       " 'training_frame',\n",
       " 'type',\n",
       " 'validation_frame',\n",
       " 'varimp',\n",
       " 'varimp_plot',\n",
       " 'weights',\n",
       " 'xval_keys',\n",
       " 'xvals']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(aml.leader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Exploration\n",
    "\n",
    "To understand how the ensemble works, let's take a peek inside the Stacked Ensemble \"All Models\" model.  The \"All Models\" ensemble is an ensemble of all of the individual models in the AutoML run.  This is often the top performing model on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>auc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>mean_per_class_error</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StackedEnsemble_AllModels_0_AutoML_20180625_17...</td>\n",
       "      <td>0.705210</td>\n",
       "      <td>0.436573</td>\n",
       "      <td>0.352879</td>\n",
       "      <td>0.370560</td>\n",
       "      <td>0.137314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StackedEnsemble_BestOfFamily_0_AutoML_20180625...</td>\n",
       "      <td>0.704911</td>\n",
       "      <td>0.436754</td>\n",
       "      <td>0.352230</td>\n",
       "      <td>0.370640</td>\n",
       "      <td>0.137374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBM_grid_0_AutoML_20180625_175234_model_0</td>\n",
       "      <td>0.703447</td>\n",
       "      <td>0.435224</td>\n",
       "      <td>0.352229</td>\n",
       "      <td>0.370297</td>\n",
       "      <td>0.137120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBM_grid_0_AutoML_20180625_175234_model_1</td>\n",
       "      <td>0.701244</td>\n",
       "      <td>0.436209</td>\n",
       "      <td>0.356297</td>\n",
       "      <td>0.370641</td>\n",
       "      <td>0.137375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GLM_grid_0_AutoML_20180625_175234_model_0</td>\n",
       "      <td>0.697503</td>\n",
       "      <td>0.438455</td>\n",
       "      <td>0.354668</td>\n",
       "      <td>0.371441</td>\n",
       "      <td>0.137968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GBM_grid_0_AutoML_20180625_175234_model_2</td>\n",
       "      <td>0.697313</td>\n",
       "      <td>0.438347</td>\n",
       "      <td>0.356853</td>\n",
       "      <td>0.371400</td>\n",
       "      <td>0.137938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GBM_grid_0_AutoML_20180625_175234_model_3</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>0.444238</td>\n",
       "      <td>0.364317</td>\n",
       "      <td>0.373402</td>\n",
       "      <td>0.139429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XRT_0_AutoML_20180625_175234</td>\n",
       "      <td>0.688545</td>\n",
       "      <td>0.471860</td>\n",
       "      <td>0.366467</td>\n",
       "      <td>0.382357</td>\n",
       "      <td>0.146197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DRF_0_AutoML_20180625_175234</td>\n",
       "      <td>0.685141</td>\n",
       "      <td>0.479268</td>\n",
       "      <td>0.364523</td>\n",
       "      <td>0.383454</td>\n",
       "      <td>0.147037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            model_id       auc   logloss  \\\n",
       "0  StackedEnsemble_AllModels_0_AutoML_20180625_17...  0.705210  0.436573   \n",
       "1  StackedEnsemble_BestOfFamily_0_AutoML_20180625...  0.704911  0.436754   \n",
       "2          GBM_grid_0_AutoML_20180625_175234_model_0  0.703447  0.435224   \n",
       "3          GBM_grid_0_AutoML_20180625_175234_model_1  0.701244  0.436209   \n",
       "4          GLM_grid_0_AutoML_20180625_175234_model_0  0.697503  0.438455   \n",
       "5          GBM_grid_0_AutoML_20180625_175234_model_2  0.697313  0.438347   \n",
       "6          GBM_grid_0_AutoML_20180625_175234_model_3  0.688900  0.444238   \n",
       "7                       XRT_0_AutoML_20180625_175234  0.688545  0.471860   \n",
       "8                       DRF_0_AutoML_20180625_175234  0.685141  0.479268   \n",
       "\n",
       "   mean_per_class_error      rmse       mse  \n",
       "0              0.352879  0.370560  0.137314  \n",
       "1              0.352230  0.370640  0.137374  \n",
       "2              0.352229  0.370297  0.137120  \n",
       "3              0.356297  0.370641  0.137375  \n",
       "4              0.354668  0.371441  0.137968  \n",
       "5              0.356853  0.371400  0.137938  \n",
       "6              0.364317  0.373402  0.139429  \n",
       "7              0.366467  0.382357  0.146197  \n",
       "8              0.364523  0.383454  0.147037  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml_leaderboard_df=aml.leaderboard.as_data_frame()\n",
    "aml_leaderboard_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting models\n",
    "\n",
    "Individul models can ne found through a search of the leader board or directly by the name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get model ids for all models in the AutoML Leaderboard\n",
    "model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n",
    "# Get the \"All Models\" Stacked Ensemble model\n",
    "se = h2o.get_model([mid for mid in model_ids if \"StackedEnsemble_AllModels\" in mid][0])\n",
    "# Get the Stacked Ensemble metalearner model\n",
    "metalearner = h2o.get_model(aml.leader.metalearner()['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DRF_0_AutoML_20180625_175234': 0.0076052552278777965,\n",
       " 'GBM_grid_0_AutoML_20180625_175234_model_0': 0.3256729799242948,\n",
       " 'GBM_grid_0_AutoML_20180625_175234_model_1': 0.16339141880081431,\n",
       " 'GBM_grid_0_AutoML_20180625_175234_model_2': 0.043147358266099715,\n",
       " 'GBM_grid_0_AutoML_20180625_175234_model_3': 0.0,\n",
       " 'GLM_grid_0_AutoML_20180625_175234_model_0': 0.10365901636181934,\n",
       " 'Intercept': -1.6426986148251335,\n",
       " 'XRT_0_AutoML_20180625_175234': 0.0637403077101407}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metalearner.coef_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bear/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB0AAAJTCAYAAABEh264AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "metalearner.std_coef_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting a model directly by name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>auc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>mean_per_class_error</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StackedEnsemble_AllModels_0_AutoML_20180625_17...</td>\n",
       "      <td>0.705210</td>\n",
       "      <td>0.436573</td>\n",
       "      <td>0.352879</td>\n",
       "      <td>0.370560</td>\n",
       "      <td>0.137314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StackedEnsemble_BestOfFamily_0_AutoML_20180625...</td>\n",
       "      <td>0.704911</td>\n",
       "      <td>0.436754</td>\n",
       "      <td>0.352230</td>\n",
       "      <td>0.370640</td>\n",
       "      <td>0.137374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBM_grid_0_AutoML_20180625_175234_model_0</td>\n",
       "      <td>0.703447</td>\n",
       "      <td>0.435224</td>\n",
       "      <td>0.352229</td>\n",
       "      <td>0.370297</td>\n",
       "      <td>0.137120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBM_grid_0_AutoML_20180625_175234_model_1</td>\n",
       "      <td>0.701244</td>\n",
       "      <td>0.436209</td>\n",
       "      <td>0.356297</td>\n",
       "      <td>0.370641</td>\n",
       "      <td>0.137375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GLM_grid_0_AutoML_20180625_175234_model_0</td>\n",
       "      <td>0.697503</td>\n",
       "      <td>0.438455</td>\n",
       "      <td>0.354668</td>\n",
       "      <td>0.371441</td>\n",
       "      <td>0.137968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            model_id       auc   logloss  \\\n",
       "0  StackedEnsemble_AllModels_0_AutoML_20180625_17...  0.705210  0.436573   \n",
       "1  StackedEnsemble_BestOfFamily_0_AutoML_20180625...  0.704911  0.436754   \n",
       "2          GBM_grid_0_AutoML_20180625_175234_model_0  0.703447  0.435224   \n",
       "3          GBM_grid_0_AutoML_20180625_175234_model_1  0.701244  0.436209   \n",
       "4          GLM_grid_0_AutoML_20180625_175234_model_0  0.697503  0.438455   \n",
       "\n",
       "   mean_per_class_error      rmse       mse  \n",
       "0              0.352879  0.370560  0.137314  \n",
       "1              0.352230  0.370640  0.137374  \n",
       "2              0.352229  0.370297  0.137120  \n",
       "3              0.356297  0.370641  0.137375  \n",
       "4              0.354668  0.371441  0.137968  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml_leaderboard_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM_grid_0_AutoML_20180625_175234_model_0\n",
      "GBM_grid_0_AutoML_20180625_175234_model_1\n",
      "GLM_grid_0_AutoML_20180625_175234_model_0\n",
      "GBM_grid_0_AutoML_20180625_175234_model_2\n",
      "GBM_grid_0_AutoML_20180625_175234_model_3\n",
      "XRT_0_AutoML_20180625_175234\n",
      "DRF_0_AutoML_20180625_175234\n",
      "model_id  GBM_grid_0_AutoML_20180625_175234_model_0\n"
     ]
    }
   ],
   "source": [
    "m_id=''\n",
    "for model in aml_leaderboard_df['model_id']:\n",
    "    if 'StackedEnsemble' not in model:\n",
    "      print (model)\n",
    "      if m_id=='':\n",
    "            m_id=model\n",
    "print (\"model_id \", m_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm\n"
     ]
    }
   ],
   "source": [
    "non_stacked= h2o.get_model(m_id)\n",
    "print (non_stacked.algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_check_targets',\n",
       " '_compute_algo',\n",
       " '_estimator_type',\n",
       " '_future',\n",
       " '_get_metrics',\n",
       " '_have_mojo',\n",
       " '_have_pojo',\n",
       " '_id',\n",
       " '_is_xvalidated',\n",
       " '_job',\n",
       " '_keyify_if_h2oframe',\n",
       " '_metrics_class',\n",
       " '_model_json',\n",
       " '_parms',\n",
       " '_plot',\n",
       " '_requires_training_frame',\n",
       " '_resolve_model',\n",
       " '_verify_training_frame_params',\n",
       " '_xval_keys',\n",
       " 'accuracy',\n",
       " 'actual_params',\n",
       " 'aic',\n",
       " 'algo',\n",
       " 'auc',\n",
       " 'balance_classes',\n",
       " 'biases',\n",
       " 'build_tree_one_node',\n",
       " 'calibrate_model',\n",
       " 'calibration_frame',\n",
       " 'categorical_encoding',\n",
       " 'catoffsets',\n",
       " 'checkpoint',\n",
       " 'class_sampling_factors',\n",
       " 'coef',\n",
       " 'coef_norm',\n",
       " 'col_sample_rate',\n",
       " 'col_sample_rate_change_per_level',\n",
       " 'col_sample_rate_per_tree',\n",
       " 'confusion_matrix',\n",
       " 'cross_validation_fold_assignment',\n",
       " 'cross_validation_holdout_predictions',\n",
       " 'cross_validation_metrics_summary',\n",
       " 'cross_validation_models',\n",
       " 'cross_validation_predictions',\n",
       " 'custom_metric_func',\n",
       " 'deepfeatures',\n",
       " 'default_params',\n",
       " 'distribution',\n",
       " 'download_mojo',\n",
       " 'download_pojo',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fit',\n",
       " 'fnr',\n",
       " 'fold_assignment',\n",
       " 'fold_column',\n",
       " 'fpr',\n",
       " 'full_parameters',\n",
       " 'gains_lift',\n",
       " 'get_params',\n",
       " 'get_xval_models',\n",
       " 'gini',\n",
       " 'have_mojo',\n",
       " 'have_pojo',\n",
       " 'histogram_type',\n",
       " 'huber_alpha',\n",
       " 'ignore_const_cols',\n",
       " 'ignored_columns',\n",
       " 'is_cross_validated',\n",
       " 'join',\n",
       " 'keep_cross_validation_fold_assignment',\n",
       " 'keep_cross_validation_predictions',\n",
       " 'learn_rate',\n",
       " 'learn_rate_annealing',\n",
       " 'levelone_frame_id',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'max_abs_leafnode_pred',\n",
       " 'max_after_balance_size',\n",
       " 'max_confusion_matrix_size',\n",
       " 'max_depth',\n",
       " 'max_hit_ratio_k',\n",
       " 'max_per_class_error',\n",
       " 'max_runtime_secs',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metalearner',\n",
       " 'metric',\n",
       " 'min_rows',\n",
       " 'min_split_improvement',\n",
       " 'missrate',\n",
       " 'mixin',\n",
       " 'model_id',\n",
       " 'model_performance',\n",
       " 'mse',\n",
       " 'nbins',\n",
       " 'nbins_cats',\n",
       " 'nbins_top_level',\n",
       " 'nfolds',\n",
       " 'normmul',\n",
       " 'normsub',\n",
       " 'ntrees',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'offset_column',\n",
       " 'params',\n",
       " 'parms',\n",
       " 'partial_plot',\n",
       " 'plot',\n",
       " 'pprint_coef',\n",
       " 'precision',\n",
       " 'pred_noise_bandwidth',\n",
       " 'predict',\n",
       " 'predict_leaf_node_assignment',\n",
       " 'quantile_alpha',\n",
       " 'r2',\n",
       " 'r2_stopping',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'respmul',\n",
       " 'response_column',\n",
       " 'respsub',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'roc',\n",
       " 'rotation',\n",
       " 'sample_rate',\n",
       " 'sample_rate_per_class',\n",
       " 'save_model_details',\n",
       " 'save_mojo',\n",
       " 'score_each_iteration',\n",
       " 'score_history',\n",
       " 'score_tree_interval',\n",
       " 'scoring_history',\n",
       " 'seed',\n",
       " 'sensitivity',\n",
       " 'set_params',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'start',\n",
       " 'std_coef_plot',\n",
       " 'stopping_metric',\n",
       " 'stopping_rounds',\n",
       " 'stopping_tolerance',\n",
       " 'summary',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'train',\n",
       " 'training_frame',\n",
       " 'tweedie_power',\n",
       " 'type',\n",
       " 'validation_frame',\n",
       " 'varimp',\n",
       " 'varimp_plot',\n",
       " 'weights',\n",
       " 'weights_column',\n",
       " 'xval_keys',\n",
       " 'xvals']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(non_stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since this is a pandas dataframe the data can be saved.\n",
    "\n",
    "The type of exploration depends on the learner.  If the learner isn't an ensemble then ensemble exploration doesn't make sense.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the variable importance of the metalearner (combiner) algorithm in the ensemble.  This shows us how much each base learner is contributing to the ensemble. The AutoML Stacked Ensembles use the default metalearner algorithm (GLM with non-negative weights), so the variable importance of the metalearner is actually the standardized coefficient magnitudes of the GLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Leader Model\n",
    "\n",
    "There are two ways to save the leader model -- binary format and MOJO format.  If you're taking your leader model to production, then we'd suggest the MOJO format since it's optimized for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bear/Documents/INFO_7390/H2O/models/StackedEnsemble_AllModels_0_AutoML_20180625_175234'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o.save_model(aml.leader, path = \"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bear/Documents/INFO_7390/H2O/models/StackedEnsemble_AllModels_0_AutoML_20180625_175234.zip'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.download_mojo(path = \"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "If one wants predictions the user will do this on new data.\n",
    "\n",
    "Here we are taking 10% of original file just to show the syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into training and test for showing how to predict\n",
    "train, test = df.split_frame([0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Using Leader Model\n",
    "\n",
    "If you need to generate predictions on a test set, you can make predictions on the `\"H2OAutoML\"` object directly, or on the leader model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">      p0</th><th style=\"text-align: right;\">       p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.628989</td><td style=\"text-align: right;\">0.371011 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.867936</td><td style=\"text-align: right;\">0.132064 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.934951</td><td style=\"text-align: right;\">0.0650486</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.896516</td><td style=\"text-align: right;\">0.103484 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.929352</td><td style=\"text-align: right;\">0.0706475</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.862402</td><td style=\"text-align: right;\">0.137598 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.733236</td><td style=\"text-align: right;\">0.266764 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.848485</td><td style=\"text-align: right;\">0.151515 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.893081</td><td style=\"text-align: right;\">0.106919 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.792241</td><td style=\"text-align: right;\">0.207759 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = aml.predict(test)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_performance()\n",
    "\n",
    "The standard `model_performance()` method can be applied to the AutoML leader model and a test set to generate an H2O model performance object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.1211958392623066\n",
      "RMSE: 0.34813192795592107\n",
      "LogLoss: 0.3916198473210528\n",
      "Null degrees of freedom: 32883\n",
      "Residual degrees of freedom: 32877\n",
      "Null deviance: 31177.51675494144\n",
      "Residual deviance: 25756.054118611002\n",
      "AIC: 25770.054118611002\n",
      "AUC: 0.8048008377699928\n",
      "Gini: 0.6096016755399856\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.2271625431076379: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>22040.0</td>\n",
       "<td>4867.0</td>\n",
       "<td>0.1809</td>\n",
       "<td> (4867.0/26907.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2374.0</td>\n",
       "<td>3603.0</td>\n",
       "<td>0.3972</td>\n",
       "<td> (2374.0/5977.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>24414.0</td>\n",
       "<td>8470.0</td>\n",
       "<td>0.2202</td>\n",
       "<td> (7241.0/32884.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  ----------------\n",
       "0      22040  4867  0.1809   (4867.0/26907.0)\n",
       "1      2374   3603  0.3972   (2374.0/5977.0)\n",
       "Total  24414  8470  0.2202   (7241.0/32884.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2271625</td>\n",
       "<td>0.4987887</td>\n",
       "<td>229.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1427193</td>\n",
       "<td>0.6321656</td>\n",
       "<td>306.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3313928</td>\n",
       "<td>0.5082436</td>\n",
       "<td>158.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4391431</td>\n",
       "<td>0.8366987</td>\n",
       "<td>103.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8544266</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0678547</td>\n",
       "<td>1.0</td>\n",
       "<td>395.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8544266</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2664213</td>\n",
       "<td>0.3750322</td>\n",
       "<td>200.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1840822</td>\n",
       "<td>0.7239752</td>\n",
       "<td>265.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1805421</td>\n",
       "<td>0.7243232</td>\n",
       "<td>269.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.227163     0.498789  229\n",
       "max f2                       0.142719     0.632166  306\n",
       "max f0point5                 0.331393     0.508244  158\n",
       "max accuracy                 0.439143     0.836699  103\n",
       "max precision                0.854427     1         0\n",
       "max recall                   0.0678547    1         395\n",
       "max specificity              0.854427     1         0\n",
       "max absolute_mcc             0.266421     0.375032  200\n",
       "max min_per_class_accuracy   0.184082     0.723975  265\n",
       "max mean_per_class_accuracy  0.180542     0.724323  269"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.18 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100049</td>\n",
       "<td>0.6164729</td>\n",
       "<td>4.7994048</td>\n",
       "<td>4.7994048</td>\n",
       "<td>0.8723404</td>\n",
       "<td>0.8723404</td>\n",
       "<td>0.0480174</td>\n",
       "<td>0.0480174</td>\n",
       "<td>379.9404811</td>\n",
       "<td>379.9404811</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200097</td>\n",
       "<td>0.5539562</td>\n",
       "<td>3.8796582</td>\n",
       "<td>4.3395315</td>\n",
       "<td>0.7051672</td>\n",
       "<td>0.7887538</td>\n",
       "<td>0.0388155</td>\n",
       "<td>0.0868329</td>\n",
       "<td>287.9658244</td>\n",
       "<td>333.9531527</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300146</td>\n",
       "<td>0.5135027</td>\n",
       "<td>3.7625996</td>\n",
       "<td>4.1472209</td>\n",
       "<td>0.6838906</td>\n",
       "<td>0.7537994</td>\n",
       "<td>0.0376443</td>\n",
       "<td>0.1244772</td>\n",
       "<td>276.2599590</td>\n",
       "<td>314.7220882</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400195</td>\n",
       "<td>0.4849859</td>\n",
       "<td>3.1438610</td>\n",
       "<td>3.8963809</td>\n",
       "<td>0.5714286</td>\n",
       "<td>0.7082067</td>\n",
       "<td>0.0314539</td>\n",
       "<td>0.1559311</td>\n",
       "<td>214.3860991</td>\n",
       "<td>289.6380909</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500243</td>\n",
       "<td>0.4589341</td>\n",
       "<td>3.1605837</td>\n",
       "<td>3.7492215</td>\n",
       "<td>0.5744681</td>\n",
       "<td>0.6814590</td>\n",
       "<td>0.0316212</td>\n",
       "<td>0.1875523</td>\n",
       "<td>216.0583656</td>\n",
       "<td>274.9221458</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000182</td>\n",
       "<td>0.3637519</td>\n",
       "<td>2.6471348</td>\n",
       "<td>3.1983457</td>\n",
       "<td>0.4811436</td>\n",
       "<td>0.5813317</td>\n",
       "<td>0.1323406</td>\n",
       "<td>0.3198929</td>\n",
       "<td>164.7134779</td>\n",
       "<td>219.8345660</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500122</td>\n",
       "<td>0.3028416</td>\n",
       "<td>2.2087345</td>\n",
       "<td>2.8685421</td>\n",
       "<td>0.4014599</td>\n",
       "<td>0.5213866</td>\n",
       "<td>0.1104233</td>\n",
       "<td>0.4303162</td>\n",
       "<td>120.8734455</td>\n",
       "<td>186.8542129</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000061</td>\n",
       "<td>0.2611322</td>\n",
       "<td>1.7301753</td>\n",
       "<td>2.5839937</td>\n",
       "<td>0.3144769</td>\n",
       "<td>0.4696670</td>\n",
       "<td>0.0864982</td>\n",
       "<td>0.5168145</td>\n",
       "<td>73.0175323</td>\n",
       "<td>158.3993698</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999939</td>\n",
       "<td>0.2060580</td>\n",
       "<td>1.3854789</td>\n",
       "<td>2.1845293</td>\n",
       "<td>0.2518248</td>\n",
       "<td>0.3970603</td>\n",
       "<td>0.1385310</td>\n",
       "<td>0.6553455</td>\n",
       "<td>38.5478886</td>\n",
       "<td>118.4529258</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000122</td>\n",
       "<td>0.1703300</td>\n",
       "<td>1.1107226</td>\n",
       "<td>1.9160368</td>\n",
       "<td>0.2018851</td>\n",
       "<td>0.3482591</td>\n",
       "<td>0.1110925</td>\n",
       "<td>0.7664380</td>\n",
       "<td>11.0722551</td>\n",
       "<td>91.6036764</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1453568</td>\n",
       "<td>0.8500281</td>\n",
       "<td>1.7028610</td>\n",
       "<td>0.1545012</td>\n",
       "<td>0.3095122</td>\n",
       "<td>0.0849925</td>\n",
       "<td>0.8514305</td>\n",
       "<td>-14.9971891</td>\n",
       "<td>70.2860967</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999878</td>\n",
       "<td>0.1263918</td>\n",
       "<td>0.6074020</td>\n",
       "<td>1.5203030</td>\n",
       "<td>0.1104015</td>\n",
       "<td>0.2763305</td>\n",
       "<td>0.0607328</td>\n",
       "<td>0.9121633</td>\n",
       "<td>-39.2598025</td>\n",
       "<td>52.0302976</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000061</td>\n",
       "<td>0.1103520</td>\n",
       "<td>0.4131754</td>\n",
       "<td>1.3621144</td>\n",
       "<td>0.0750988</td>\n",
       "<td>0.2475781</td>\n",
       "<td>0.0413251</td>\n",
       "<td>0.9534884</td>\n",
       "<td>-58.6824593</td>\n",
       "<td>36.2114411</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999939</td>\n",
       "<td>0.0962377</td>\n",
       "<td>0.2827849</td>\n",
       "<td>1.2272136</td>\n",
       "<td>0.0513990</td>\n",
       "<td>0.2230585</td>\n",
       "<td>0.0282751</td>\n",
       "<td>0.9817634</td>\n",
       "<td>-71.7215058</td>\n",
       "<td>22.7213613</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999818</td>\n",
       "<td>0.0822933</td>\n",
       "<td>0.1439024</td>\n",
       "<td>1.1068579</td>\n",
       "<td>0.0261557</td>\n",
       "<td>0.2011826</td>\n",
       "<td>0.0143885</td>\n",
       "<td>0.9961519</td>\n",
       "<td>-85.6097604</td>\n",
       "<td>10.6857902</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0615548</td>\n",
       "<td>0.0384738</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0069930</td>\n",
       "<td>0.1817601</td>\n",
       "<td>0.0038481</td>\n",
       "<td>1.0</td>\n",
       "<td>-96.1526177</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100049                   0.616473           4.7994     4.7994             0.87234          0.87234                     0.0480174       0.0480174                  379.94    379.94\n",
       "    2        0.0200097                   0.553956           3.87966    4.33953            0.705167         0.788754                    0.0388155       0.0868329                  287.966   333.953\n",
       "    3        0.0300146                   0.513503           3.7626     4.14722            0.683891         0.753799                    0.0376443       0.124477                   276.26    314.722\n",
       "    4        0.0400195                   0.484986           3.14386    3.89638            0.571429         0.708207                    0.0314539       0.155931                   214.386   289.638\n",
       "    5        0.0500243                   0.458934           3.16058    3.74922            0.574468         0.681459                    0.0316212       0.187552                   216.058   274.922\n",
       "    6        0.100018                    0.363752           2.64713    3.19835            0.481144         0.581332                    0.132341        0.319893                   164.713   219.835\n",
       "    7        0.150012                    0.302842           2.20873    2.86854            0.40146          0.521387                    0.110423        0.430316                   120.873   186.854\n",
       "    8        0.200006                    0.261132           1.73018    2.58399            0.314477         0.469667                    0.0864982       0.516814                   73.0175   158.399\n",
       "    9        0.299994                    0.206058           1.38548    2.18453            0.251825         0.39706                     0.138531        0.655345                   38.5479   118.453\n",
       "    10       0.400012                    0.17033            1.11072    1.91604            0.201885         0.348259                    0.111093        0.766438                   11.0723   91.6037\n",
       "    11       0.5                         0.145357           0.850028   1.70286            0.154501         0.309512                    0.0849925       0.85143                    -14.9972  70.2861\n",
       "    12       0.599988                    0.126392           0.607402   1.5203             0.110401         0.27633                     0.0607328       0.912163                   -39.2598  52.0303\n",
       "    13       0.700006                    0.110352           0.413175   1.36211            0.0750988        0.247578                    0.0413251       0.953488                   -58.6825  36.2114\n",
       "    14       0.799994                    0.0962377          0.282785   1.22721            0.051399         0.223059                    0.0282751       0.981763                   -71.7215  22.7214\n",
       "    15       0.899982                    0.0822933          0.143902   1.10686            0.0261557        0.201183                    0.0143885       0.996152                   -85.6098  10.6858\n",
       "    16       1                           0.0615548          0.0384738  1                  0.00699301       0.18176                     0.00384808      1                          -96.1526  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf = aml.leader.model_performance(test)\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.2271625431076379: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>22040.0</td>\n",
       "<td>4867.0</td>\n",
       "<td>0.1809</td>\n",
       "<td> (4867.0/26907.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2374.0</td>\n",
       "<td>3603.0</td>\n",
       "<td>0.3972</td>\n",
       "<td> (2374.0/5977.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>24414.0</td>\n",
       "<td>8470.0</td>\n",
       "<td>0.2202</td>\n",
       "<td> (7241.0/32884.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  ----------------\n",
       "0      22040  4867  0.1809   (4867.0/26907.0)\n",
       "1      2374   3603  0.3972   (2374.0/5977.0)\n",
       "Total  24414  8470  0.2202   (7241.0/32884.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=perf.confusion_matrix()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_algo',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_has',\n",
       " '_metric_json',\n",
       " '_on_train',\n",
       " '_on_valid',\n",
       " '_on_xval',\n",
       " 'accuracy',\n",
       " 'aic',\n",
       " 'auc',\n",
       " 'confusion_matrix',\n",
       " 'custom_metric_name',\n",
       " 'custom_metric_value',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fnr',\n",
       " 'fpr',\n",
       " 'fprs',\n",
       " 'gains_lift',\n",
       " 'gini',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'make',\n",
       " 'max_per_class_error',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metric',\n",
       " 'missrate',\n",
       " 'mse',\n",
       " 'nobs',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'plot',\n",
       " 'precision',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'sensitivity',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'tprs']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In R we get plots like:\n",
    "    \n",
    "    #compute performance\n",
    "perf <- h2o.performance(automl_leader,conv_data.hex)\n",
    "h2o.confusionMatrix(perf)\n",
    "h2o.accuracy(perf)\n",
    "h2o.tpr(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stackedensemble'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_check_targets',\n",
       " '_compute_algo',\n",
       " '_estimator_type',\n",
       " '_future',\n",
       " '_get_metrics',\n",
       " '_have_mojo',\n",
       " '_have_pojo',\n",
       " '_id',\n",
       " '_is_xvalidated',\n",
       " '_job',\n",
       " '_keyify_if_h2oframe',\n",
       " '_metrics_class',\n",
       " '_model_json',\n",
       " '_parms',\n",
       " '_plot',\n",
       " '_requires_training_frame',\n",
       " '_resolve_model',\n",
       " '_verify_training_frame_params',\n",
       " '_xval_keys',\n",
       " 'accuracy',\n",
       " 'actual_params',\n",
       " 'aic',\n",
       " 'algo',\n",
       " 'auc',\n",
       " 'base_models',\n",
       " 'biases',\n",
       " 'catoffsets',\n",
       " 'coef',\n",
       " 'coef_norm',\n",
       " 'confusion_matrix',\n",
       " 'cross_validation_fold_assignment',\n",
       " 'cross_validation_holdout_predictions',\n",
       " 'cross_validation_metrics_summary',\n",
       " 'cross_validation_models',\n",
       " 'cross_validation_predictions',\n",
       " 'deepfeatures',\n",
       " 'default_params',\n",
       " 'download_mojo',\n",
       " 'download_pojo',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fit',\n",
       " 'fnr',\n",
       " 'fpr',\n",
       " 'full_parameters',\n",
       " 'gains_lift',\n",
       " 'get_params',\n",
       " 'get_xval_models',\n",
       " 'gini',\n",
       " 'have_mojo',\n",
       " 'have_pojo',\n",
       " 'is_cross_validated',\n",
       " 'join',\n",
       " 'keep_levelone_frame',\n",
       " 'levelone_frame_id',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'max_per_class_error',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metalearner',\n",
       " 'metalearner_algorithm',\n",
       " 'metalearner_fold_assignment',\n",
       " 'metalearner_fold_column',\n",
       " 'metalearner_nfolds',\n",
       " 'metalearner_params',\n",
       " 'metric',\n",
       " 'missrate',\n",
       " 'mixin',\n",
       " 'model_id',\n",
       " 'model_performance',\n",
       " 'mse',\n",
       " 'normmul',\n",
       " 'normsub',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'params',\n",
       " 'parms',\n",
       " 'partial_plot',\n",
       " 'plot',\n",
       " 'pprint_coef',\n",
       " 'precision',\n",
       " 'predict',\n",
       " 'predict_leaf_node_assignment',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'respmul',\n",
       " 'response_column',\n",
       " 'respsub',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'roc',\n",
       " 'rotation',\n",
       " 'save_model_details',\n",
       " 'save_mojo',\n",
       " 'score_history',\n",
       " 'scoring_history',\n",
       " 'seed',\n",
       " 'sensitivity',\n",
       " 'set_params',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'start',\n",
       " 'std_coef_plot',\n",
       " 'summary',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'train',\n",
       " 'training_frame',\n",
       " 'type',\n",
       " 'validation_frame',\n",
       " 'varimp',\n",
       " 'varimp_plot',\n",
       " 'weights',\n",
       " 'xval_keys',\n",
       " 'xvals']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(aml.leader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8048008377699928"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.model_performance(test).auc() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.1167639474829593\n",
      "RMSE: 0.34170740039244\n",
      "LogLoss: 0.3795990920019023\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130948\n",
      "Null deviance: 124374.13927893189\n",
      "Residual deviance: 99420.79818621822\n",
      "AIC: 99434.79818621822\n",
      "AUC: 0.8305862969853443\n",
      "Gini: 0.6611725939706885\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24030127929039052: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>90474.0</td>\n",
       "<td>16607.0</td>\n",
       "<td>0.1551</td>\n",
       "<td> (16607.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9334.0</td>\n",
       "<td>14540.0</td>\n",
       "<td>0.391</td>\n",
       "<td> (9334.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>99808.0</td>\n",
       "<td>31147.0</td>\n",
       "<td>0.1981</td>\n",
       "<td> (25941.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      90474  16607  0.1551   (16607.0/107081.0)\n",
       "1      9334   14540  0.391    (9334.0/23874.0)\n",
       "Total  99808  31147  0.1981   (25941.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2403013</td>\n",
       "<td>0.5285255</td>\n",
       "<td>222.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1529696</td>\n",
       "<td>0.6516791</td>\n",
       "<td>296.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3520442</td>\n",
       "<td>0.5439141</td>\n",
       "<td>151.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4117837</td>\n",
       "<td>0.8448704</td>\n",
       "<td>121.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8488098</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0753915</td>\n",
       "<td>1.0</td>\n",
       "<td>387.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8488098</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2828681</td>\n",
       "<td>0.4124344</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1895643</td>\n",
       "<td>0.7444458</td>\n",
       "<td>262.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1801546</td>\n",
       "<td>0.7459682</td>\n",
       "<td>270.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.240301     0.528525  222\n",
       "max f2                       0.15297      0.651679  296\n",
       "max f0point5                 0.352044     0.543914  151\n",
       "max accuracy                 0.411784     0.84487   121\n",
       "max precision                0.84881      1         0\n",
       "max recall                   0.0753915    1         387\n",
       "max specificity              0.84881      1         0\n",
       "max absolute_mcc             0.282868     0.412434  193\n",
       "max min_per_class_accuracy   0.189564     0.744446  262\n",
       "max mean_per_class_accuracy  0.180155     0.745968  270"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.6271502</td>\n",
       "<td>5.1377168</td>\n",
       "<td>5.1377168</td>\n",
       "<td>0.9366412</td>\n",
       "<td>0.9366412</td>\n",
       "<td>0.0513948</td>\n",
       "<td>0.0513948</td>\n",
       "<td>413.7716811</td>\n",
       "<td>413.7716811</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5662824</td>\n",
       "<td>4.4091408</td>\n",
       "<td>4.7734288</td>\n",
       "<td>0.8038168</td>\n",
       "<td>0.8702290</td>\n",
       "<td>0.0441066</td>\n",
       "<td>0.0955014</td>\n",
       "<td>340.9140833</td>\n",
       "<td>377.3428822</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.5252263</td>\n",
       "<td>4.0437525</td>\n",
       "<td>4.5303272</td>\n",
       "<td>0.7372040</td>\n",
       "<td>0.8259099</td>\n",
       "<td>0.0404205</td>\n",
       "<td>0.1359219</td>\n",
       "<td>304.3752460</td>\n",
       "<td>353.0327178</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4929238</td>\n",
       "<td>3.4921400</td>\n",
       "<td>4.2707309</td>\n",
       "<td>0.6366412</td>\n",
       "<td>0.7785837</td>\n",
       "<td>0.0349334</td>\n",
       "<td>0.1708553</td>\n",
       "<td>249.2140033</td>\n",
       "<td>327.0730851</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4650058</td>\n",
       "<td>3.3774761</td>\n",
       "<td>4.0921618</td>\n",
       "<td>0.6157372</td>\n",
       "<td>0.7460293</td>\n",
       "<td>0.0337606</td>\n",
       "<td>0.2046159</td>\n",
       "<td>237.7476148</td>\n",
       "<td>309.2161760</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3687602</td>\n",
       "<td>2.8247225</td>\n",
       "<td>3.4584421</td>\n",
       "<td>0.5149664</td>\n",
       "<td>0.6304979</td>\n",
       "<td>0.1412415</td>\n",
       "<td>0.3458574</td>\n",
       "<td>182.4722509</td>\n",
       "<td>245.8442134</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.3073983</td>\n",
       "<td>2.2492230</td>\n",
       "<td>3.0553691</td>\n",
       "<td>0.4100489</td>\n",
       "<td>0.5570149</td>\n",
       "<td>0.1124654</td>\n",
       "<td>0.4583229</td>\n",
       "<td>124.9222994</td>\n",
       "<td>205.5369087</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2647046</td>\n",
       "<td>1.8122206</td>\n",
       "<td>2.7446176</td>\n",
       "<td>0.3303803</td>\n",
       "<td>0.5003627</td>\n",
       "<td>0.0906007</td>\n",
       "<td>0.5489235</td>\n",
       "<td>81.2220646</td>\n",
       "<td>174.4617576</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2073810</td>\n",
       "<td>1.4353980</td>\n",
       "<td>2.3081999</td>\n",
       "<td>0.2616830</td>\n",
       "<td>0.4208008</td>\n",
       "<td>0.1435453</td>\n",
       "<td>0.6924688</td>\n",
       "<td>43.5397989</td>\n",
       "<td>130.8199939</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1707956</td>\n",
       "<td>1.0878358</td>\n",
       "<td>2.0031205</td>\n",
       "<td>0.1983200</td>\n",
       "<td>0.3651827</td>\n",
       "<td>0.1087794</td>\n",
       "<td>0.8012482</td>\n",
       "<td>8.7835788</td>\n",
       "<td>100.3120550</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1453638</td>\n",
       "<td>0.7723589</td>\n",
       "<td>1.7569607</td>\n",
       "<td>0.1408064</td>\n",
       "<td>0.3203061</td>\n",
       "<td>0.0772388</td>\n",
       "<td>0.8784871</td>\n",
       "<td>-22.7641117</td>\n",
       "<td>75.6960698</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1262417</td>\n",
       "<td>0.5579504</td>\n",
       "<td>1.5571333</td>\n",
       "<td>0.1017182</td>\n",
       "<td>0.2838761</td>\n",
       "<td>0.0557929</td>\n",
       "<td>0.9342800</td>\n",
       "<td>-44.2049569</td>\n",
       "<td>55.7133283</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1101940</td>\n",
       "<td>0.3535362</td>\n",
       "<td>1.3851965</td>\n",
       "<td>0.0644521</td>\n",
       "<td>0.2525309</td>\n",
       "<td>0.0353523</td>\n",
       "<td>0.9696322</td>\n",
       "<td>-64.6463841</td>\n",
       "<td>38.5196464</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0961032</td>\n",
       "<td>0.2173830</td>\n",
       "<td>1.2392142</td>\n",
       "<td>0.0396304</td>\n",
       "<td>0.2259173</td>\n",
       "<td>0.0217391</td>\n",
       "<td>0.9913714</td>\n",
       "<td>-78.2616996</td>\n",
       "<td>23.9214208</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0822907</td>\n",
       "<td>0.0783309</td>\n",
       "<td>1.1102315</td>\n",
       "<td>0.0142803</td>\n",
       "<td>0.2024029</td>\n",
       "<td>0.0078328</td>\n",
       "<td>0.9992042</td>\n",
       "<td>-92.1669121</td>\n",
       "<td>11.0231549</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0614696</td>\n",
       "<td>0.0079581</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0014508</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0007958</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.2041855</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ----------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.62715            5.13772     5.13772            0.936641         0.936641                    0.0513948       0.0513948                  413.772   413.772\n",
       "    2        0.0200069                   0.566282           4.40914     4.77343            0.803817         0.870229                    0.0441066       0.0955014                  340.914   377.343\n",
       "    3        0.0300027                   0.525226           4.04375     4.53033            0.737204         0.82591                     0.0404205       0.135922                   304.375   353.033\n",
       "    4        0.0400061                   0.492924           3.49214     4.27073            0.636641         0.778584                    0.0349334       0.170855                   249.214   327.073\n",
       "    5        0.0500019                   0.465006           3.37748     4.09216            0.615737         0.746029                    0.0337606       0.204616                   237.748   309.216\n",
       "    6        0.100004                    0.36876            2.82472     3.45844            0.514966         0.630498                    0.141242        0.345857                   182.472   245.844\n",
       "    7        0.150006                    0.307398           2.24922     3.05537            0.410049         0.557015                    0.112465        0.458323                   124.922   205.537\n",
       "    8        0.2                         0.264705           1.81222     2.74462            0.33038          0.500363                    0.0906007       0.548924                   81.2221   174.462\n",
       "    9        0.300004                    0.207381           1.4354      2.3082             0.261683         0.420801                    0.143545        0.692469                   43.5398   130.82\n",
       "    10       0.4                         0.170796           1.08784     2.00312            0.19832          0.365183                    0.108779        0.801248                   8.78358   100.312\n",
       "    11       0.500004                    0.145364           0.772359    1.75696            0.140806         0.320306                    0.0772388       0.878487                   -22.7641  75.6961\n",
       "    12       0.6                         0.126242           0.55795     1.55713            0.101718         0.283876                    0.0557929       0.93428                    -44.205   55.7133\n",
       "    13       0.699996                    0.110194           0.353536    1.3852             0.0644521        0.252531                    0.0353523       0.969632                   -64.6464  38.5196\n",
       "    14       0.8                         0.0961032          0.217383    1.23921            0.0396304        0.225917                    0.0217391       0.991371                   -78.2617  23.9214\n",
       "    15       0.899996                    0.0822907          0.0783309   1.11023            0.0142803        0.202403                    0.00783279      0.999204                   -92.1669  11.0232\n",
       "    16       1                           0.0614696          0.00795814  1                  0.00145082       0.182307                    0.000795845     1                          -99.2042  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_perf = aml.leader.model_performance()\n",
    "best_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_perf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24030127929039052: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>90474.0</td>\n",
       "<td>16607.0</td>\n",
       "<td>0.1551</td>\n",
       "<td> (16607.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9334.0</td>\n",
       "<td>14540.0</td>\n",
       "<td>0.391</td>\n",
       "<td> (9334.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>99808.0</td>\n",
       "<td>31147.0</td>\n",
       "<td>0.1981</td>\n",
       "<td> (25941.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      90474  16607  0.1551   (16607.0/107081.0)\n",
       "1      9334   14540  0.391    (9334.0/23874.0)\n",
       "Total  99808  31147  0.1981   (25941.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.confusion_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  9.33872489050345e-06,\n",
       "  9.33872489050345e-06,\n",
       "  9.33872489050345e-06,\n",
       "  9.33872489050345e-06,\n",
       "  9.33872489050345e-06,\n",
       "  9.33872489050345e-06,\n",
       "  9.33872489050345e-06,\n",
       "  1.86774497810069e-05,\n",
       "  2.8016174671510353e-05,\n",
       "  3.73548995620138e-05,\n",
       "  4.669362445251725e-05,\n",
       "  7.47097991240276e-05,\n",
       "  9.33872489050345e-05,\n",
       "  0.00010272597379553796,\n",
       "  0.00012140342357654485,\n",
       "  0.0001307421484670483,\n",
       "  0.0001494195982480552,\n",
       "  0.00016809704802906212,\n",
       "  0.00019611322270057246,\n",
       "  0.00025214557204359316,\n",
       "  0.0002988391964961104,\n",
       "  0.0003081779213866139,\n",
       "  0.0003642102707296346,\n",
       "  0.0004015651702916484,\n",
       "  0.00042024262007265526,\n",
       "  0.00044825879474416565,\n",
       "  0.000476274969415676,\n",
       "  0.0005416460436492002,\n",
       "  0.0005696622183207104,\n",
       "  0.0006256945676637312,\n",
       "  0.0007284205414592692,\n",
       "  0.0008031303405832968,\n",
       "  0.0008871788645978278,\n",
       "  0.0009432112139408485,\n",
       "  0.0009992435632838691,\n",
       "  0.0011113082619699107,\n",
       "  0.0011766793362034348,\n",
       "  0.00132609893445149,\n",
       "  0.0014475023580280348,\n",
       "  0.0015502283318235728,\n",
       "  0.0016342768558381038,\n",
       "  0.0017370028296336418,\n",
       "  0.0018957611527722004,\n",
       "  0.002054519475910759,\n",
       "  0.002175922899487304,\n",
       "  0.002325342497735359,\n",
       "  0.0023907135719688833,\n",
       "  0.0025961655195599594,\n",
       "  0.002736246392917511,\n",
       "  0.002866988541384559,\n",
       "  0.003063101764085132,\n",
       "  0.0032592149867857042,\n",
       "  0.0034459894845957732,\n",
       "  0.003604747807734332,\n",
       "  0.003754167405982387,\n",
       "  0.0039409419037924566,\n",
       "  0.004174410026055043,\n",
       "  0.004417216873208132,\n",
       "  0.0046973786199232355,\n",
       "  0.004902830567514312,\n",
       "  0.005154976139557905,\n",
       "  0.005425799161382505,\n",
       "  0.0057713319823311324,\n",
       "  0.005827364331674153,\n",
       "  0.006042155004155733,\n",
       "  0.006350332925542346,\n",
       "  0.006723881921162484,\n",
       "  0.00711610836656363,\n",
       "  0.007405608838169236,\n",
       "  0.007704448034665347,\n",
       "  0.00801262595605196,\n",
       "  0.008367497501891091,\n",
       "  0.00882509502152576,\n",
       "  0.009133272942912374,\n",
       "  0.009544176838094526,\n",
       "  0.010057806707072216,\n",
       "  0.01058077530094041,\n",
       "  0.011047711545465582,\n",
       "  0.011439937990866726,\n",
       "  0.0119068742353919,\n",
       "  0.012411165379479085,\n",
       "  0.012710004575975197,\n",
       "  0.013120908471157348,\n",
       "  0.013494457466777486,\n",
       "  0.013839990287726114,\n",
       "  0.014278910357579775,\n",
       "  0.014820556401228976,\n",
       "  0.015212782846630121,\n",
       "  0.015436912244002204,\n",
       "  0.015847816139184356,\n",
       "  0.016202687685023486,\n",
       "  0.01669764010422017,\n",
       "  0.017388705746117426,\n",
       "  0.01796770668932864,\n",
       "  0.018210513536481728,\n",
       "  0.018686788505897403,\n",
       "  0.01907901495129855,\n",
       "  0.019686032069181274,\n",
       "  0.020265033012392488,\n",
       "  0.020694614357355645,\n",
       "  0.02097477610407075,\n",
       "  0.02151642214771995,\n",
       "  0.02200203584202613,\n",
       "  0.022581036785237343,\n",
       "  0.023262763702244095,\n",
       "  0.023720361221878764,\n",
       "  0.02414060384195142,\n",
       "  0.024803653309177165,\n",
       "  0.025625461099541468,\n",
       "  0.02614842969340966,\n",
       "  0.02676478553618289,\n",
       "  0.027455851178080144,\n",
       "  0.027913448697714813,\n",
       "  0.028623191789393078,\n",
       "  0.02928624125661882,\n",
       "  0.030108049046983124,\n",
       "  0.03061234019107031,\n",
       "  0.031331422007639075,\n",
       "  0.03205050382420784,\n",
       "  0.03279760181544812,\n",
       "  0.033376602758659335,\n",
       "  0.034245104173476155,\n",
       "  0.03497352471493542,\n",
       "  0.035393767335008076,\n",
       "  0.0362622687498249,\n",
       "  0.0370840765401892,\n",
       "  0.037961916679896526,\n",
       "  0.03894248279339939,\n",
       "  0.039736274409092186,\n",
       "  0.04051138857500397,\n",
       "  0.041249147841353744,\n",
       "  0.042042939457046534,\n",
       "  0.04284606979762983,\n",
       "  0.04366787758799413,\n",
       "  0.044555056452591967,\n",
       "  0.04548892894164231,\n",
       "  0.04632007545689711,\n",
       "  0.04728196412061897,\n",
       "  0.04830922385857435,\n",
       "  0.049355161046310735,\n",
       "  0.05024233991090857,\n",
       "  0.05113885750039689,\n",
       "  0.052110084889009256,\n",
       "  0.05257702113353443,\n",
       "  0.053454861273241755,\n",
       "  0.05463154060944519,\n",
       "  0.05572417142163409,\n",
       "  0.056191107666159264,\n",
       "  0.05715299632988112,\n",
       "  0.058189594792727,\n",
       "  0.05892735405907677,\n",
       "  0.059982629971703665,\n",
       "  0.060925841185644515,\n",
       "  0.0617102940764468,\n",
       "  0.0627749087139642,\n",
       "  0.06395158805016762,\n",
       "  0.06508157376191855,\n",
       "  0.066314285447465,\n",
       "  0.06741625498454441,\n",
       "  0.06896648331636798,\n",
       "  0.07028324352592898,\n",
       "  0.07122645473986981,\n",
       "  0.07247784387519728,\n",
       "  0.07373857173541525,\n",
       "  0.07508334811964774,\n",
       "  0.07583044611088802,\n",
       "  0.07726860974402555,\n",
       "  0.07851999887935301,\n",
       "  0.079836759088914,\n",
       "  0.08087335755175988,\n",
       "  0.08211540796219684,\n",
       "  0.08344150689664834,\n",
       "  0.08475826710620932,\n",
       "  0.08587891409306973,\n",
       "  0.08734509390087877,\n",
       "  0.0885124345121917,\n",
       "  0.08999729176978176,\n",
       "  0.09127669707978073,\n",
       "  0.09292965138539984,\n",
       "  0.09346195870415853,\n",
       "  0.09476004146393852,\n",
       "  0.09617018892240453,\n",
       "  0.09748694913196553,\n",
       "  0.09888775786554103,\n",
       "  0.10017650190043051,\n",
       "  0.10166135915802056,\n",
       "  0.10308084534137708,\n",
       "  0.10504197756838281,\n",
       "  0.10667625442422092,\n",
       "  0.10861870920144563,\n",
       "  0.11036505075596978,\n",
       "  0.11184990801355983,\n",
       "  0.11345616869472643,\n",
       "  0.11419392796107619,\n",
       "  0.11606167293917688,\n",
       "  0.11757454637143844,\n",
       "  0.11954501732333467,\n",
       "  0.12159953679924543,\n",
       "  0.12332720090398856,\n",
       "  0.12508288118340322,\n",
       "  0.12708136830997097,\n",
       "  0.12898646818763365,\n",
       "  0.13091024551507738,\n",
       "  0.13326360418748423,\n",
       "  0.13540217218740952,\n",
       "  0.1370084328685761,\n",
       "  0.1388294842222243,\n",
       "  0.14074392282477752,\n",
       "  0.1426210065277687,\n",
       "  0.14446073533119788,\n",
       "  0.1461603832612695,\n",
       "  0.14812151548827523,\n",
       "  0.1499612442917044,\n",
       "  0.15195973141827215,\n",
       "  0.15275352303396494,\n",
       "  0.1550882042565908,\n",
       "  0.1572454497062971,\n",
       "  0.15926261428264585,\n",
       "  0.16141052100746164,\n",
       "  0.1632409110860003,\n",
       "  0.16545418888504965,\n",
       "  0.1666962392954866,\n",
       "  0.1688441460203024,\n",
       "  0.17084263314687012,\n",
       "  0.1729345075223429,\n",
       "  0.17504505934759668,\n",
       "  0.17739841802000356,\n",
       "  0.17977979286708193,\n",
       "  0.1810311820024094,\n",
       "  0.18370205732109338,\n",
       "  0.18596202874459522,\n",
       "  0.1883714197663451,\n",
       "  0.19127576320729167,\n",
       "  0.19412407429889522,\n",
       "  0.1965147878708641,\n",
       "  0.19880277546903746,\n",
       "  0.20204331300604214,\n",
       "  0.20463013980071162,\n",
       "  0.20703953082246151,\n",
       "  0.20990651936384605,\n",
       "  0.21233458783537695,\n",
       "  0.21475331758201735,\n",
       "  0.2173308056517963,\n",
       "  0.21994564862113727,\n",
       "  0.22314883125857995,\n",
       "  0.22595978745062148,\n",
       "  0.22931238968631223,\n",
       "  0.2319645875552152,\n",
       "  0.23462612414900869,\n",
       "  0.23727832201791169,\n",
       "  0.24064960170338343,\n",
       "  0.2432551059478339,\n",
       "  0.24606606213987542,\n",
       "  0.24947469672490918,\n",
       "  0.25263118573789933,\n",
       "  0.25555420662862693,\n",
       "  0.2596165519559959,\n",
       "  0.262623621370738,\n",
       "  0.26588283635752374,\n",
       "  0.2687685023486893,\n",
       "  0.2719810237110225,\n",
       "  0.2752215612480272,\n",
       "  0.27826598556233134,\n",
       "  0.2820855240425472,\n",
       "  0.28536341647911395,\n",
       "  0.2884825505925421,\n",
       "  0.29018219852261373,\n",
       "  0.29343207478450894,\n",
       "  0.29672864467085663,\n",
       "  0.3010151193955977,\n",
       "  0.3044237539806315,\n",
       "  0.3080004856136943,\n",
       "  0.3127165416833986,\n",
       "  0.3163866605653664,\n",
       "  0.3199073598490862,\n",
       "  0.3234747527572585,\n",
       "  0.32728495251258394,\n",
       "  0.3311044909927998,\n",
       "  0.33555906276557,\n",
       "  0.34005098943790213,\n",
       "  0.34361838234607445,\n",
       "  0.34898814915811394,\n",
       "  0.3529477685116874,\n",
       "  0.3567019359176698,\n",
       "  0.3603533773498567,\n",
       "  0.36634883872955987,\n",
       "  0.37028978063335233,\n",
       "  0.37409998038867776,\n",
       "  0.3780035673929082,\n",
       "  0.3819818641962626,\n",
       "  0.38633370999523725,\n",
       "  0.39126455673742305,\n",
       "  0.3953269020647921,\n",
       "  0.3993985861170516,\n",
       "  0.4048804176277771,\n",
       "  0.4094190379245618,\n",
       "  0.41375220627375536,\n",
       "  0.41793595502470093,\n",
       "  0.42205433270141296,\n",
       "  0.42614469420345347,\n",
       "  0.43044050765308506,\n",
       "  0.4346616113035926,\n",
       "  0.4401341040894276,\n",
       "  0.4448688376089129,\n",
       "  0.4508269440890541,\n",
       "  0.4554402741849628,\n",
       "  0.45992286213240446,\n",
       "  0.4671603739225446,\n",
       "  0.4736321102716635,\n",
       "  0.4788804736601264,\n",
       "  0.4839420625507793,\n",
       "  0.4886487798955931,\n",
       "  0.49373838496091743,\n",
       "  0.4996124429170441,\n",
       "  0.5050102259037551,\n",
       "  0.5100251211699555,\n",
       "  0.5149746453619223,\n",
       "  0.5211008488900926,\n",
       "  0.5283383606802328,\n",
       "  0.5334746593700096,\n",
       "  0.5389564908807352,\n",
       "  0.5453628561556205,\n",
       "  0.5514423660593383,\n",
       "  0.5567280843473632,\n",
       "  0.5617616570633446,\n",
       "  0.5671034077007125,\n",
       "  0.5738926606961086,\n",
       "  0.5776655055518719,\n",
       "  0.584361371298363,\n",
       "  0.5895443636125923,\n",
       "  0.5966698107040465,\n",
       "  0.6037952577955006,\n",
       "  0.6093517991053502,\n",
       "  0.6164118751225708,\n",
       "  0.6219404002577488,\n",
       "  0.6287483307029258,\n",
       "  0.6358831165192704,\n",
       "  0.644166565497147,\n",
       "  0.6518056424575789,\n",
       "  0.6579972170599826,\n",
       "  0.6666355375836983,\n",
       "  0.6723321597669054,\n",
       "  0.6782996049719371,\n",
       "  0.6842670501769689,\n",
       "  0.6939886627879829,\n",
       "  0.7000681726917006,\n",
       "  0.7031966455300194,\n",
       "  0.7113119974598668,\n",
       "  0.7175969593111756,\n",
       "  0.7239379535118275,\n",
       "  0.7296532531448157,\n",
       "  0.7356954081489714,\n",
       "  0.7445485193451686,\n",
       "  0.7505719968995433,\n",
       "  0.7592103174232591,\n",
       "  0.7657847797461734,\n",
       "  0.7738721155013495,\n",
       "  0.782062177230321,\n",
       "  0.7881043322344767,\n",
       "  0.7946601171076101,\n",
       "  0.8011972245309625,\n",
       "  0.8097514965306637,\n",
       "  0.8163166201286877,\n",
       "  0.8225268721808724,\n",
       "  0.8308009824338585,\n",
       "  0.8368057825384522,\n",
       "  0.8442580850010739,\n",
       "  0.8541011010356646,\n",
       "  0.860750273157703,\n",
       "  0.8671379609828074,\n",
       "  0.8749451349912682,\n",
       "  0.8778774946068864,\n",
       "  0.8859741690869529,\n",
       "  0.8920536789906706,\n",
       "  0.8968070899599369,\n",
       "  0.9030920518112457,\n",
       "  0.9088260288940148,\n",
       "  0.9164837833042276,\n",
       "  0.9238520372428348,\n",
       "  0.929455272177137,\n",
       "  0.9365620418188101,\n",
       "  0.9442197962290229,\n",
       "  0.949991128211354,\n",
       "  0.9557531214687947,\n",
       "  0.9629999719838254,\n",
       "  0.9712927596865923,\n",
       "  0.9758127025335961,\n",
       "  0.9809396624984824,\n",
       "  0.9859265415900113,\n",
       "  0.9912215986029268,\n",
       "  0.9959843482970835,\n",
       "  0.9985618363668625,\n",
       "  1.0],\n",
       " [0.00016754628466113765,\n",
       "  0.0004607522828181285,\n",
       "  0.000921504565636257,\n",
       "  0.0014660299907849543,\n",
       "  0.0022199882717600736,\n",
       "  0.002638853983412918,\n",
       "  0.0030158331239004774,\n",
       "  0.0034346988355533218,\n",
       "  0.004062997403032588,\n",
       "  0.004900728826338276,\n",
       "  0.005487140822652258,\n",
       "  0.0063248722459579455,\n",
       "  0.007707129094412331,\n",
       "  0.00883806651587501,\n",
       "  0.009550138225684845,\n",
       "  0.010681075647147525,\n",
       "  0.01214710563793248,\n",
       "  0.013948228198039708,\n",
       "  0.015456144759989947,\n",
       "  0.017131607606601325,\n",
       "  0.01851386445505571,\n",
       "  0.02023121387283237,\n",
       "  0.022116109575270167,\n",
       "  0.02358213956605512,\n",
       "  0.02546703526849292,\n",
       "  0.026765518974616737,\n",
       "  0.02906928038870738,\n",
       "  0.030744743235318756,\n",
       "  0.03133115523163274,\n",
       "  0.033299824076401106,\n",
       "  0.03484962720951663,\n",
       "  0.03686018262545028,\n",
       "  0.03807489318924353,\n",
       "  0.0402948814610036,\n",
       "  0.04293373544441652,\n",
       "  0.04439976543520147,\n",
       "  0.046410320851135126,\n",
       "  0.048965401692217476,\n",
       "  0.051604255675630394,\n",
       "  0.053824243947390465,\n",
       "  0.055876685934489405,\n",
       "  0.05872497277372874,\n",
       "  0.06069364161849711,\n",
       "  0.06278797017676133,\n",
       "  0.06588757644299238,\n",
       "  0.0682751109994136,\n",
       "  0.06995057384602496,\n",
       "  0.07246376811594203,\n",
       "  0.07380413839323113,\n",
       "  0.07635921923431348,\n",
       "  0.077909022367429,\n",
       "  0.08046410320851136,\n",
       "  0.08280975119376728,\n",
       "  0.08394068861522996,\n",
       "  0.08620256345815532,\n",
       "  0.08817123230292369,\n",
       "  0.09068442657284075,\n",
       "  0.09294630141576611,\n",
       "  0.09621345396665829,\n",
       "  0.09943871994638519,\n",
       "  0.10191002764513697,\n",
       "  0.10387869648990533,\n",
       "  0.10672698332914468,\n",
       "  0.10995224930887157,\n",
       "  0.11233978386529278,\n",
       "  0.11514618413336684,\n",
       "  0.11841333668425903,\n",
       "  0.12147105637932479,\n",
       "  0.12473820893021698,\n",
       "  0.12775404205411744,\n",
       "  0.12989025718354696,\n",
       "  0.13282231716511686,\n",
       "  0.1356706040043562,\n",
       "  0.138435117701265,\n",
       "  0.14153472396749603,\n",
       "  0.14450867052023122,\n",
       "  0.14626790650917315,\n",
       "  0.1488229873502555,\n",
       "  0.15221579961464354,\n",
       "  0.15573427159252742,\n",
       "  0.15845689871827093,\n",
       "  0.1620172572673201,\n",
       "  0.16608025467035267,\n",
       "  0.16867722208260033,\n",
       "  0.17156739549300495,\n",
       "  0.17487643461506241,\n",
       "  0.17805981402362403,\n",
       "  0.1807824411493675,\n",
       "  0.18316997570578872,\n",
       "  0.1867303342548379,\n",
       "  0.18987182709223424,\n",
       "  0.19255256764681244,\n",
       "  0.19594537991120048,\n",
       "  0.1994219653179191,\n",
       "  0.2021027058724973,\n",
       "  0.20361062243444752,\n",
       "  0.20691966155650499,\n",
       "  0.20888833040127336,\n",
       "  0.21244868895032254,\n",
       "  0.21550640864538828,\n",
       "  0.21889922090977632,\n",
       "  0.22078411661221412,\n",
       "  0.22413504230543688,\n",
       "  0.22631314400603167,\n",
       "  0.22911954427410572,\n",
       "  0.23280556253665075,\n",
       "  0.2359470553740471,\n",
       "  0.23766440479182374,\n",
       "  0.2405126916310631,\n",
       "  0.2439892770377817,\n",
       "  0.24671190416352518,\n",
       "  0.24947641786043395,\n",
       "  0.25178017927452456,\n",
       "  0.2534137555499707,\n",
       "  0.2567646812431934,\n",
       "  0.26099522493088717,\n",
       "  0.2648487894780933,\n",
       "  0.26832537488481195,\n",
       "  0.2710898885817207,\n",
       "  0.27385440227862945,\n",
       "  0.27741476082767863,\n",
       "  0.2804305939515791,\n",
       "  0.2834883136466449,\n",
       "  0.28570830191840496,\n",
       "  0.289101114182793,\n",
       "  0.29211694730669346,\n",
       "  0.2952584401440898,\n",
       "  0.297939180698668,\n",
       "  0.3012063332495602,\n",
       "  0.3051855575102622,\n",
       "  0.30686102035687357,\n",
       "  0.31021194605009633,\n",
       "  0.3132277791739968,\n",
       "  0.3170394571500377,\n",
       "  0.3199715171316076,\n",
       "  0.3234899891094915,\n",
       "  0.3263801625198961,\n",
       "  0.32977297478428413,\n",
       "  0.33304012733517635,\n",
       "  0.33593030074558095,\n",
       "  0.33894613386948147,\n",
       "  0.34355365669766275,\n",
       "  0.34673703610622436,\n",
       "  0.35029739465527354,\n",
       "  0.3532294546368434,\n",
       "  0.35695735947055374,\n",
       "  0.36097847030242103,\n",
       "  0.3642875094244785,\n",
       "  0.3678059814023624,\n",
       "  0.37178520566306444,\n",
       "  0.3740470805059898,\n",
       "  0.37727234648571667,\n",
       "  0.38070704532127,\n",
       "  0.38380665158750105,\n",
       "  0.3856077741476083,\n",
       "  0.38887492669850043,\n",
       "  0.3922258523917232,\n",
       "  0.3955348915137807,\n",
       "  0.399262796347491,\n",
       "  0.4030744743235319,\n",
       "  0.40562955516461424,\n",
       "  0.4088967077155064,\n",
       "  0.4121219736952333,\n",
       "  0.41522157996146436,\n",
       "  0.41874005193934827,\n",
       "  0.4228868224847114,\n",
       "  0.4277875513110497,\n",
       "  0.43113847700427244,\n",
       "  0.43319091899137135,\n",
       "  0.4365837312557594,\n",
       "  0.43943201809499877,\n",
       "  0.4430761497863785,\n",
       "  0.4460082097679484,\n",
       "  0.44910781603417943,\n",
       "  0.4520817625869146,\n",
       "  0.4545111837145011,\n",
       "  0.4572338108402446,\n",
       "  0.4603334171064757,\n",
       "  0.463768115942029,\n",
       "  0.46757979391806986,\n",
       "  0.46975789561866466,\n",
       "  0.4721035436039206,\n",
       "  0.47541258272597803,\n",
       "  0.4788472815615314,\n",
       "  0.4820725475412583,\n",
       "  0.48617743151545617,\n",
       "  0.4876434615062411,\n",
       "  0.4912038200552903,\n",
       "  0.49476417860433947,\n",
       "  0.49832453715338865,\n",
       "  0.5010890508502974,\n",
       "  0.5042305436876937,\n",
       "  0.5080003350925694,\n",
       "  0.5112256010722962,\n",
       "  0.5141995476250314,\n",
       "  0.5177599061740806,\n",
       "  0.5206919661556505,\n",
       "  0.5241266649912039,\n",
       "  0.5277707966825835,\n",
       "  0.5304096506659964,\n",
       "  0.5317919075144508,\n",
       "  0.5366926363407891,\n",
       "  0.5400854486051772,\n",
       "  0.5443997654352015,\n",
       "  0.5481276702689117,\n",
       "  0.5513529362486387,\n",
       "  0.554201223087878,\n",
       "  0.557426489067605,\n",
       "  0.5606098684761666,\n",
       "  0.5642958867387116,\n",
       "  0.5685264304264053,\n",
       "  0.5713747172656446,\n",
       "  0.5747256429588674,\n",
       "  0.5783278880790819,\n",
       "  0.5820557929127922,\n",
       "  0.5856999246041719,\n",
       "  0.5889251905838988,\n",
       "  0.5922342297059563,\n",
       "  0.5960040211108318,\n",
       "  0.5995224930887157,\n",
       "  0.6028315322107732,\n",
       "  0.6046326547708805,\n",
       "  0.6090307447432353,\n",
       "  0.6118790315824747,\n",
       "  0.6147692049928792,\n",
       "  0.6185808829689201,\n",
       "  0.6220574683756388,\n",
       "  0.6248638686437128,\n",
       "  0.6261623523498366,\n",
       "  0.6293876183295636,\n",
       "  0.6328642037362822,\n",
       "  0.636089469716009,\n",
       "  0.6396917148362236,\n",
       "  0.643293959956438,\n",
       "  0.6468543185054871,\n",
       "  0.6485716679232638,\n",
       "  0.6517131607606601,\n",
       "  0.6545195610287342,\n",
       "  0.6579123732931222,\n",
       "  0.6608863198458574,\n",
       "  0.665535729245204,\n",
       "  0.6692636340789143,\n",
       "  0.6730334254837899,\n",
       "  0.6766775571751696,\n",
       "  0.6802798022953841,\n",
       "  0.684091480271425,\n",
       "  0.6881125911032923,\n",
       "  0.6920918153639943,\n",
       "  0.6958616067688699,\n",
       "  0.6995057384602497,\n",
       "  0.7033174164362905,\n",
       "  0.7073385272681578,\n",
       "  0.7107732261037112,\n",
       "  0.7145849040797521,\n",
       "  0.7178101700594789,\n",
       "  0.7211192091815364,\n",
       "  0.7243444751612633,\n",
       "  0.7284493591354612,\n",
       "  0.7323029236826674,\n",
       "  0.7351512105219067,\n",
       "  0.7394236407807657,\n",
       "  0.7424394739046661,\n",
       "  0.7456228533132278,\n",
       "  0.7499371701432521,\n",
       "  0.7532043226941443,\n",
       "  0.7564714752450364,\n",
       "  0.759487308368937,\n",
       "  0.7629638937756555,\n",
       "  0.7658540671860602,\n",
       "  0.7697076317332663,\n",
       "  0.7740219485632907,\n",
       "  0.7768283488313646,\n",
       "  0.7794672028147775,\n",
       "  0.781142665661389,\n",
       "  0.784535477925777,\n",
       "  0.7880958364748262,\n",
       "  0.7924939264471811,\n",
       "  0.7963474909943872,\n",
       "  0.798818798693139,\n",
       "  0.803091228951998,\n",
       "  0.805897629220072,\n",
       "  0.8090391220574684,\n",
       "  0.8118874088967077,\n",
       "  0.8146938091647817,\n",
       "  0.8176677557175169,\n",
       "  0.8218564128340454,\n",
       "  0.8250816788137723,\n",
       "  0.8281393985088381,\n",
       "  0.8325793750523582,\n",
       "  0.8350087961799447,\n",
       "  0.8380665158750105,\n",
       "  0.8405797101449275,\n",
       "  0.8446008209767948,\n",
       "  0.84757476752953,\n",
       "  0.8505906006534305,\n",
       "  0.8534807740638352,\n",
       "  0.8562452877607439,\n",
       "  0.8588003686018263,\n",
       "  0.8621094077238837,\n",
       "  0.8648320348496272,\n",
       "  0.8676803216888666,\n",
       "  0.8709055876685935,\n",
       "  0.8740051939348245,\n",
       "  0.8770629136298903,\n",
       "  0.8797855407556338,\n",
       "  0.8826338275948731,\n",
       "  0.8858172070034347,\n",
       "  0.8884141744156824,\n",
       "  0.890717935829773,\n",
       "  0.8936918823825082,\n",
       "  0.8962469632235905,\n",
       "  0.8988858172070034,\n",
       "  0.9012733517634246,\n",
       "  0.90353522660635,\n",
       "  0.9067186060149116,\n",
       "  0.9096925525676468,\n",
       "  0.9125408394068861,\n",
       "  0.9154310128172908,\n",
       "  0.9179023205160426,\n",
       "  0.919745329647315,\n",
       "  0.9231800284828684,\n",
       "  0.9255256764681243,\n",
       "  0.9275781184552233,\n",
       "  0.9296724470134875,\n",
       "  0.9321856412834045,\n",
       "  0.9353690206919661,\n",
       "  0.9371282566809082,\n",
       "  0.939180698668007,\n",
       "  0.9407305018011226,\n",
       "  0.9427829437882215,\n",
       "  0.9445421797771635,\n",
       "  0.9461338694814443,\n",
       "  0.9480606517550473,\n",
       "  0.9502387534556421,\n",
       "  0.9512859177347742,\n",
       "  0.9539247717181871,\n",
       "  0.9556840077071291,\n",
       "  0.9575689034095669,\n",
       "  0.9595375722543352,\n",
       "  0.9615062410991037,\n",
       "  0.9634330233727068,\n",
       "  0.9651503727904833,\n",
       "  0.96686772220826,\n",
       "  0.9687107313395326,\n",
       "  0.9710563793247885,\n",
       "  0.972606182457904,\n",
       "  0.9739465527351931,\n",
       "  0.9759152215799615,\n",
       "  0.9769623858590936,\n",
       "  0.9782189829940521,\n",
       "  0.9797268995560023,\n",
       "  0.9809834966909609,\n",
       "  0.9822400938259194,\n",
       "  0.9828265058222334,\n",
       "  0.9841249895283573,\n",
       "  0.9856747926614727,\n",
       "  0.9867638435117702,\n",
       "  0.9876853480774064,\n",
       "  0.9887743989277038,\n",
       "  0.9898634497780012,\n",
       "  0.9907430677724721,\n",
       "  0.9915807991957778,\n",
       "  0.9923347574767529,\n",
       "  0.9931724889000586,\n",
       "  0.994261539750356,\n",
       "  0.9948898383178353,\n",
       "  0.9955600234564799,\n",
       "  0.9961045488816286,\n",
       "  0.9966909608779425,\n",
       "  0.9969841668760995,\n",
       "  0.9972773728742566,\n",
       "  0.9975286923012482,\n",
       "  0.9978637848705705,\n",
       "  0.9983664237245539,\n",
       "  0.9986596297227109,\n",
       "  0.998827176007372,\n",
       "  0.9989528357208679,\n",
       "  0.9991622685766943,\n",
       "  0.9992460417190249,\n",
       "  0.9994973611460166,\n",
       "  0.9996230208595125,\n",
       "  0.9996649074306777,\n",
       "  0.9997486805730083,\n",
       "  0.9998324537153389,\n",
       "  0.9998743402865041,\n",
       "  0.9999162268576695,\n",
       "  0.9999162268576695,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc=aml.leader.roc()\n",
    "roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_0_AutoML_20180625_175234\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.1167639474829593\n",
      "RMSE: 0.34170740039244\n",
      "LogLoss: 0.3795990920019023\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130948\n",
      "Null deviance: 124374.13927893189\n",
      "Residual deviance: 99420.79818621822\n",
      "AIC: 99434.79818621822\n",
      "AUC: 0.8305862969853443\n",
      "Gini: 0.6611725939706885\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24030127929039052: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>90474.0</td>\n",
       "<td>16607.0</td>\n",
       "<td>0.1551</td>\n",
       "<td> (16607.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9334.0</td>\n",
       "<td>14540.0</td>\n",
       "<td>0.391</td>\n",
       "<td> (9334.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>99808.0</td>\n",
       "<td>31147.0</td>\n",
       "<td>0.1981</td>\n",
       "<td> (25941.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      90474  16607  0.1551   (16607.0/107081.0)\n",
       "1      9334   14540  0.391    (9334.0/23874.0)\n",
       "Total  99808  31147  0.1981   (25941.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2403013</td>\n",
       "<td>0.5285255</td>\n",
       "<td>222.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1529696</td>\n",
       "<td>0.6516791</td>\n",
       "<td>296.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3520442</td>\n",
       "<td>0.5439141</td>\n",
       "<td>151.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4117837</td>\n",
       "<td>0.8448704</td>\n",
       "<td>121.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8488098</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0753915</td>\n",
       "<td>1.0</td>\n",
       "<td>387.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8488098</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2828681</td>\n",
       "<td>0.4124344</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1895643</td>\n",
       "<td>0.7444458</td>\n",
       "<td>262.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1801546</td>\n",
       "<td>0.7459682</td>\n",
       "<td>270.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.240301     0.528525  222\n",
       "max f2                       0.15297      0.651679  296\n",
       "max f0point5                 0.352044     0.543914  151\n",
       "max accuracy                 0.411784     0.84487   121\n",
       "max precision                0.84881      1         0\n",
       "max recall                   0.0753915    1         387\n",
       "max specificity              0.84881      1         0\n",
       "max absolute_mcc             0.282868     0.412434  193\n",
       "max min_per_class_accuracy   0.189564     0.744446  262\n",
       "max mean_per_class_accuracy  0.180155     0.745968  270"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.6271502</td>\n",
       "<td>5.1377168</td>\n",
       "<td>5.1377168</td>\n",
       "<td>0.9366412</td>\n",
       "<td>0.9366412</td>\n",
       "<td>0.0513948</td>\n",
       "<td>0.0513948</td>\n",
       "<td>413.7716811</td>\n",
       "<td>413.7716811</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5662824</td>\n",
       "<td>4.4091408</td>\n",
       "<td>4.7734288</td>\n",
       "<td>0.8038168</td>\n",
       "<td>0.8702290</td>\n",
       "<td>0.0441066</td>\n",
       "<td>0.0955014</td>\n",
       "<td>340.9140833</td>\n",
       "<td>377.3428822</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.5252263</td>\n",
       "<td>4.0437525</td>\n",
       "<td>4.5303272</td>\n",
       "<td>0.7372040</td>\n",
       "<td>0.8259099</td>\n",
       "<td>0.0404205</td>\n",
       "<td>0.1359219</td>\n",
       "<td>304.3752460</td>\n",
       "<td>353.0327178</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4929238</td>\n",
       "<td>3.4921400</td>\n",
       "<td>4.2707309</td>\n",
       "<td>0.6366412</td>\n",
       "<td>0.7785837</td>\n",
       "<td>0.0349334</td>\n",
       "<td>0.1708553</td>\n",
       "<td>249.2140033</td>\n",
       "<td>327.0730851</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4650058</td>\n",
       "<td>3.3774761</td>\n",
       "<td>4.0921618</td>\n",
       "<td>0.6157372</td>\n",
       "<td>0.7460293</td>\n",
       "<td>0.0337606</td>\n",
       "<td>0.2046159</td>\n",
       "<td>237.7476148</td>\n",
       "<td>309.2161760</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3687602</td>\n",
       "<td>2.8247225</td>\n",
       "<td>3.4584421</td>\n",
       "<td>0.5149664</td>\n",
       "<td>0.6304979</td>\n",
       "<td>0.1412415</td>\n",
       "<td>0.3458574</td>\n",
       "<td>182.4722509</td>\n",
       "<td>245.8442134</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.3073983</td>\n",
       "<td>2.2492230</td>\n",
       "<td>3.0553691</td>\n",
       "<td>0.4100489</td>\n",
       "<td>0.5570149</td>\n",
       "<td>0.1124654</td>\n",
       "<td>0.4583229</td>\n",
       "<td>124.9222994</td>\n",
       "<td>205.5369087</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2647046</td>\n",
       "<td>1.8122206</td>\n",
       "<td>2.7446176</td>\n",
       "<td>0.3303803</td>\n",
       "<td>0.5003627</td>\n",
       "<td>0.0906007</td>\n",
       "<td>0.5489235</td>\n",
       "<td>81.2220646</td>\n",
       "<td>174.4617576</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2073810</td>\n",
       "<td>1.4353980</td>\n",
       "<td>2.3081999</td>\n",
       "<td>0.2616830</td>\n",
       "<td>0.4208008</td>\n",
       "<td>0.1435453</td>\n",
       "<td>0.6924688</td>\n",
       "<td>43.5397989</td>\n",
       "<td>130.8199939</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1707956</td>\n",
       "<td>1.0878358</td>\n",
       "<td>2.0031205</td>\n",
       "<td>0.1983200</td>\n",
       "<td>0.3651827</td>\n",
       "<td>0.1087794</td>\n",
       "<td>0.8012482</td>\n",
       "<td>8.7835788</td>\n",
       "<td>100.3120550</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1453638</td>\n",
       "<td>0.7723589</td>\n",
       "<td>1.7569607</td>\n",
       "<td>0.1408064</td>\n",
       "<td>0.3203061</td>\n",
       "<td>0.0772388</td>\n",
       "<td>0.8784871</td>\n",
       "<td>-22.7641117</td>\n",
       "<td>75.6960698</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1262417</td>\n",
       "<td>0.5579504</td>\n",
       "<td>1.5571333</td>\n",
       "<td>0.1017182</td>\n",
       "<td>0.2838761</td>\n",
       "<td>0.0557929</td>\n",
       "<td>0.9342800</td>\n",
       "<td>-44.2049569</td>\n",
       "<td>55.7133283</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1101940</td>\n",
       "<td>0.3535362</td>\n",
       "<td>1.3851965</td>\n",
       "<td>0.0644521</td>\n",
       "<td>0.2525309</td>\n",
       "<td>0.0353523</td>\n",
       "<td>0.9696322</td>\n",
       "<td>-64.6463841</td>\n",
       "<td>38.5196464</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0961032</td>\n",
       "<td>0.2173830</td>\n",
       "<td>1.2392142</td>\n",
       "<td>0.0396304</td>\n",
       "<td>0.2259173</td>\n",
       "<td>0.0217391</td>\n",
       "<td>0.9913714</td>\n",
       "<td>-78.2616996</td>\n",
       "<td>23.9214208</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0822907</td>\n",
       "<td>0.0783309</td>\n",
       "<td>1.1102315</td>\n",
       "<td>0.0142803</td>\n",
       "<td>0.2024029</td>\n",
       "<td>0.0078328</td>\n",
       "<td>0.9992042</td>\n",
       "<td>-92.1669121</td>\n",
       "<td>11.0231549</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0614696</td>\n",
       "<td>0.0079581</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0014508</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0007958</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.2041855</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ----------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.62715            5.13772     5.13772            0.936641         0.936641                    0.0513948       0.0513948                  413.772   413.772\n",
       "    2        0.0200069                   0.566282           4.40914     4.77343            0.803817         0.870229                    0.0441066       0.0955014                  340.914   377.343\n",
       "    3        0.0300027                   0.525226           4.04375     4.53033            0.737204         0.82591                     0.0404205       0.135922                   304.375   353.033\n",
       "    4        0.0400061                   0.492924           3.49214     4.27073            0.636641         0.778584                    0.0349334       0.170855                   249.214   327.073\n",
       "    5        0.0500019                   0.465006           3.37748     4.09216            0.615737         0.746029                    0.0337606       0.204616                   237.748   309.216\n",
       "    6        0.100004                    0.36876            2.82472     3.45844            0.514966         0.630498                    0.141242        0.345857                   182.472   245.844\n",
       "    7        0.150006                    0.307398           2.24922     3.05537            0.410049         0.557015                    0.112465        0.458323                   124.922   205.537\n",
       "    8        0.2                         0.264705           1.81222     2.74462            0.33038          0.500363                    0.0906007       0.548924                   81.2221   174.462\n",
       "    9        0.300004                    0.207381           1.4354      2.3082             0.261683         0.420801                    0.143545        0.692469                   43.5398   130.82\n",
       "    10       0.4                         0.170796           1.08784     2.00312            0.19832          0.365183                    0.108779        0.801248                   8.78358   100.312\n",
       "    11       0.500004                    0.145364           0.772359    1.75696            0.140806         0.320306                    0.0772388       0.878487                   -22.7641  75.6961\n",
       "    12       0.6                         0.126242           0.55795     1.55713            0.101718         0.283876                    0.0557929       0.93428                    -44.205   55.7133\n",
       "    13       0.699996                    0.110194           0.353536    1.3852             0.0644521        0.252531                    0.0353523       0.969632                   -64.6464  38.5196\n",
       "    14       0.8                         0.0961032          0.217383    1.23921            0.0396304        0.225917                    0.0217391       0.991371                   -78.2617  23.9214\n",
       "    15       0.899996                    0.0822907          0.0783309   1.11023            0.0142803        0.202403                    0.00783279      0.999204                   -92.1669  11.0232\n",
       "    16       1                           0.0614696          0.00795814  1                  0.00145082       0.182307                    0.000795845     1                          -99.2042  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.1386513474415332\n",
      "RMSE: 0.37235916457304125\n",
      "LogLoss: 0.4394559813847542\n",
      "Null degrees of freedom: 33031\n",
      "Residual degrees of freedom: 33025\n",
      "Null deviance: 31732.354674481932\n",
      "Residual deviance: 29032.219954202403\n",
      "AIC: 29046.219954202403\n",
      "AUC: 0.7102666513197816\n",
      "Gini: 0.42053330263956323\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.18707726312815404: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>19160.0</td>\n",
       "<td>7730.0</td>\n",
       "<td>0.2875</td>\n",
       "<td> (7730.0/26890.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2529.0</td>\n",
       "<td>3613.0</td>\n",
       "<td>0.4118</td>\n",
       "<td> (2529.0/6142.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>21689.0</td>\n",
       "<td>11343.0</td>\n",
       "<td>0.3106</td>\n",
       "<td> (10259.0/33032.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      19160  7730   0.2875   (7730.0/26890.0)\n",
       "1      2529   3613   0.4118   (2529.0/6142.0)\n",
       "Total  21689  11343  0.3106   (10259.0/33032.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1870773</td>\n",
       "<td>0.4132685</td>\n",
       "<td>255.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1147101</td>\n",
       "<td>0.5726111</td>\n",
       "<td>334.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2910025</td>\n",
       "<td>0.3779960</td>\n",
       "<td>173.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5696954</td>\n",
       "<td>0.8161177</td>\n",
       "<td>37.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8006882</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0668733</td>\n",
       "<td>1.0</td>\n",
       "<td>396.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8006882</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1870773</td>\n",
       "<td>0.2464403</td>\n",
       "<td>255.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1677169</td>\n",
       "<td>0.6499442</td>\n",
       "<td>273.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1499723</td>\n",
       "<td>0.6538983</td>\n",
       "<td>291.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.187077     0.413269  255\n",
       "max f2                       0.11471      0.572611  334\n",
       "max f0point5                 0.291002     0.377996  173\n",
       "max accuracy                 0.569695     0.816118  37\n",
       "max precision                0.800688     1         0\n",
       "max recall                   0.0668733    1         396\n",
       "max specificity              0.800688     1         0\n",
       "max absolute_mcc             0.187077     0.24644   255\n",
       "max min_per_class_accuracy   0.167717     0.649944  273\n",
       "max mean_per_class_accuracy  0.149972     0.653898  291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.59 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100206</td>\n",
       "<td>0.5870321</td>\n",
       "<td>3.2008350</td>\n",
       "<td>3.2008350</td>\n",
       "<td>0.5951662</td>\n",
       "<td>0.5951662</td>\n",
       "<td>0.0320742</td>\n",
       "<td>0.0320742</td>\n",
       "<td>220.0835021</td>\n",
       "<td>220.0835021</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200109</td>\n",
       "<td>0.5271460</td>\n",
       "<td>2.5260551</td>\n",
       "<td>2.8639555</td>\n",
       "<td>0.4696970</td>\n",
       "<td>0.5325265</td>\n",
       "<td>0.0252361</td>\n",
       "<td>0.0573103</td>\n",
       "<td>152.6055080</td>\n",
       "<td>186.3955474</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300012</td>\n",
       "<td>0.4907770</td>\n",
       "<td>2.3467867</td>\n",
       "<td>2.6917398</td>\n",
       "<td>0.4363636</td>\n",
       "<td>0.5005045</td>\n",
       "<td>0.0234451</td>\n",
       "<td>0.0807555</td>\n",
       "<td>134.6786655</td>\n",
       "<td>169.1739823</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400218</td>\n",
       "<td>0.4607515</td>\n",
       "<td>2.4209361</td>\n",
       "<td>2.6239365</td>\n",
       "<td>0.4501511</td>\n",
       "<td>0.4878971</td>\n",
       "<td>0.0242592</td>\n",
       "<td>0.1050147</td>\n",
       "<td>142.0936133</td>\n",
       "<td>162.3936479</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500121</td>\n",
       "<td>0.4368630</td>\n",
       "<td>2.2490039</td>\n",
       "<td>2.5490407</td>\n",
       "<td>0.4181818</td>\n",
       "<td>0.4739709</td>\n",
       "<td>0.0224683</td>\n",
       "<td>0.1274829</td>\n",
       "<td>124.9003878</td>\n",
       "<td>154.9040741</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000242</td>\n",
       "<td>0.3510749</td>\n",
       "<td>2.0086311</td>\n",
       "<td>2.2788359</td>\n",
       "<td>0.3734867</td>\n",
       "<td>0.4237288</td>\n",
       "<td>0.1004559</td>\n",
       "<td>0.2279388</td>\n",
       "<td>100.8631082</td>\n",
       "<td>127.8835912</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500061</td>\n",
       "<td>0.2954589</td>\n",
       "<td>1.8111431</td>\n",
       "<td>2.1230012</td>\n",
       "<td>0.3367656</td>\n",
       "<td>0.3947528</td>\n",
       "<td>0.0905243</td>\n",
       "<td>0.3184630</td>\n",
       "<td>81.1143143</td>\n",
       "<td>112.3001248</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000182</td>\n",
       "<td>0.2562466</td>\n",
       "<td>1.5723968</td>\n",
       "<td>1.9853293</td>\n",
       "<td>0.2923729</td>\n",
       "<td>0.3691539</td>\n",
       "<td>0.0786389</td>\n",
       "<td>0.3971019</td>\n",
       "<td>57.2396779</td>\n",
       "<td>98.5329296</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000121</td>\n",
       "<td>0.2033653</td>\n",
       "<td>1.3644590</td>\n",
       "<td>1.7783934</td>\n",
       "<td>0.2537087</td>\n",
       "<td>0.3306761</td>\n",
       "<td>0.1364376</td>\n",
       "<td>0.5335396</td>\n",
       "<td>36.4459039</td>\n",
       "<td>77.8393428</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000061</td>\n",
       "<td>0.1689245</td>\n",
       "<td>1.1153394</td>\n",
       "<td>1.6126425</td>\n",
       "<td>0.2073872</td>\n",
       "<td>0.2998562</td>\n",
       "<td>0.1115272</td>\n",
       "<td>0.6450668</td>\n",
       "<td>11.5339429</td>\n",
       "<td>61.2642473</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1444336</td>\n",
       "<td>1.0290431</td>\n",
       "<td>1.4959297</td>\n",
       "<td>0.1913412</td>\n",
       "<td>0.2781545</td>\n",
       "<td>0.1028981</td>\n",
       "<td>0.7479648</td>\n",
       "<td>2.9043094</td>\n",
       "<td>49.5929665</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999939</td>\n",
       "<td>0.1259430</td>\n",
       "<td>0.7652694</td>\n",
       "<td>1.3741591</td>\n",
       "<td>0.1422949</td>\n",
       "<td>0.2555124</td>\n",
       "<td>0.0765223</td>\n",
       "<td>0.8244871</td>\n",
       "<td>-23.4730611</td>\n",
       "<td>37.4159097</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999879</td>\n",
       "<td>0.1109070</td>\n",
       "<td>0.6415237</td>\n",
       "<td>1.2695014</td>\n",
       "<td>0.1192855</td>\n",
       "<td>0.2360522</td>\n",
       "<td>0.0641485</td>\n",
       "<td>0.8886356</td>\n",
       "<td>-35.8476299</td>\n",
       "<td>26.9501424</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999818</td>\n",
       "<td>0.0968897</td>\n",
       "<td>0.5259192</td>\n",
       "<td>1.1765572</td>\n",
       "<td>0.0977899</td>\n",
       "<td>0.2187701</td>\n",
       "<td>0.0525887</td>\n",
       "<td>0.9412244</td>\n",
       "<td>-47.4080824</td>\n",
       "<td>17.6557160</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999758</td>\n",
       "<td>0.0829745</td>\n",
       "<td>0.4054300</td>\n",
       "<td>1.0908792</td>\n",
       "<td>0.0753860</td>\n",
       "<td>0.2028391</td>\n",
       "<td>0.0405405</td>\n",
       "<td>0.9817649</td>\n",
       "<td>-59.4570047</td>\n",
       "<td>9.0879242</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0624311</td>\n",
       "<td>0.1823069</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0338983</td>\n",
       "<td>0.1859409</td>\n",
       "<td>0.0182351</td>\n",
       "<td>1.0</td>\n",
       "<td>-81.7693127</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100206                   0.587032           3.20084   3.20084            0.595166         0.595166                    0.0320742       0.0320742                  220.084   220.084\n",
       "    2        0.0200109                   0.527146           2.52606   2.86396            0.469697         0.532526                    0.0252361       0.0573103                  152.606   186.396\n",
       "    3        0.0300012                   0.490777           2.34679   2.69174            0.436364         0.500505                    0.0234451       0.0807555                  134.679   169.174\n",
       "    4        0.0400218                   0.460751           2.42094   2.62394            0.450151         0.487897                    0.0242592       0.105015                   142.094   162.394\n",
       "    5        0.0500121                   0.436863           2.249     2.54904            0.418182         0.473971                    0.0224683       0.127483                   124.9     154.904\n",
       "    6        0.100024                    0.351075           2.00863   2.27884            0.373487         0.423729                    0.100456        0.227939                   100.863   127.884\n",
       "    7        0.150006                    0.295459           1.81114   2.123              0.336766         0.394753                    0.0905243       0.318463                   81.1143   112.3\n",
       "    8        0.200018                    0.256247           1.5724    1.98533            0.292373         0.369154                    0.0786389       0.397102                   57.2397   98.5329\n",
       "    9        0.300012                    0.203365           1.36446   1.77839            0.253709         0.330676                    0.136438        0.53354                    36.4459   77.8393\n",
       "    10       0.400006                    0.168924           1.11534   1.61264            0.207387         0.299856                    0.111527        0.645067                   11.5339   61.2642\n",
       "    11       0.5                         0.144434           1.02904   1.49593            0.191341         0.278155                    0.102898        0.747965                   2.90431   49.593\n",
       "    12       0.599994                    0.125943           0.765269  1.37416            0.142295         0.255512                    0.0765223       0.824487                   -23.4731  37.4159\n",
       "    13       0.699988                    0.110907           0.641524  1.2695             0.119285         0.236052                    0.0641485       0.888636                   -35.8476  26.9501\n",
       "    14       0.799982                    0.0968897          0.525919  1.17656            0.0977899        0.21877                     0.0525887       0.941224                   -47.4081  17.6557\n",
       "    15       0.899976                    0.0829745          0.40543   1.09088            0.075386         0.202839                    0.0405405       0.981765                   -59.457   9.08792\n",
       "    16       1                           0.0624311          0.182307  1                  0.0338983        0.185941                    0.0182351       1                          -81.7693  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.13731436524493457\n",
      "RMSE: 0.37055952996102337\n",
      "LogLoss: 0.4365729016322642\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130947\n",
      "Null deviance: 124375.46514588114\n",
      "Residual deviance: 114342.80866650632\n",
      "AIC: 114358.80866650632\n",
      "AUC: 0.7052102719993631\n",
      "Gini: 0.4104205439987263\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.1834653742477123: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>75544.0</td>\n",
       "<td>31537.0</td>\n",
       "<td>0.2945</td>\n",
       "<td> (31537.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9818.0</td>\n",
       "<td>14056.0</td>\n",
       "<td>0.4112</td>\n",
       "<td> (9818.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>85362.0</td>\n",
       "<td>45593.0</td>\n",
       "<td>0.3158</td>\n",
       "<td> (41355.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      75544  31537  0.2945   (31537.0/107081.0)\n",
       "1      9818   14056  0.4112   (9818.0/23874.0)\n",
       "Total  85362  45593  0.3158   (41355.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1834654</td>\n",
       "<td>0.4046814</td>\n",
       "<td>261.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1142473</td>\n",
       "<td>0.5642710</td>\n",
       "<td>335.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2981927</td>\n",
       "<td>0.3695123</td>\n",
       "<td>172.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6033991</td>\n",
       "<td>0.8184109</td>\n",
       "<td>29.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8158356</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0635534</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8158356</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2024945</td>\n",
       "<td>0.2396925</td>\n",
       "<td>244.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1661343</td>\n",
       "<td>0.6488453</td>\n",
       "<td>278.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1577254</td>\n",
       "<td>0.6492678</td>\n",
       "<td>287.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.183465     0.404681  261\n",
       "max f2                       0.114247     0.564271  335\n",
       "max f0point5                 0.298193     0.369512  172\n",
       "max accuracy                 0.603399     0.818411  29\n",
       "max precision                0.815836     1         0\n",
       "max recall                   0.0635534    1         399\n",
       "max specificity              0.815836     1         0\n",
       "max absolute_mcc             0.202495     0.239692  244\n",
       "max min_per_class_accuracy   0.166134     0.648845  278\n",
       "max mean_per_class_accuracy  0.157725     0.649268  287"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5751406</td>\n",
       "<td>2.9143039</td>\n",
       "<td>2.9143039</td>\n",
       "<td>0.5312977</td>\n",
       "<td>0.5312977</td>\n",
       "<td>0.0291531</td>\n",
       "<td>0.0291531</td>\n",
       "<td>191.4303912</td>\n",
       "<td>191.4303912</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5209152</td>\n",
       "<td>2.6044498</td>\n",
       "<td>2.7593768</td>\n",
       "<td>0.4748092</td>\n",
       "<td>0.5030534</td>\n",
       "<td>0.0260534</td>\n",
       "<td>0.0552065</td>\n",
       "<td>160.4449761</td>\n",
       "<td>175.9376837</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.4825780</td>\n",
       "<td>2.3173006</td>\n",
       "<td>2.6120931</td>\n",
       "<td>0.4224599</td>\n",
       "<td>0.4762026</td>\n",
       "<td>0.0231633</td>\n",
       "<td>0.0783698</td>\n",
       "<td>131.7300632</td>\n",
       "<td>161.2093113</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4532771</td>\n",
       "<td>2.3867144</td>\n",
       "<td>2.5557377</td>\n",
       "<td>0.4351145</td>\n",
       "<td>0.4659286</td>\n",
       "<td>0.0238753</td>\n",
       "<td>0.1022451</td>\n",
       "<td>138.6714411</td>\n",
       "<td>155.5737682</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4296896</td>\n",
       "<td>2.2460635</td>\n",
       "<td>2.4938312</td>\n",
       "<td>0.4094729</td>\n",
       "<td>0.4546426</td>\n",
       "<td>0.0224512</td>\n",
       "<td>0.1246963</td>\n",
       "<td>124.6063542</td>\n",
       "<td>149.3831230</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3444799</td>\n",
       "<td>2.0850339</td>\n",
       "<td>2.2894326</td>\n",
       "<td>0.3801161</td>\n",
       "<td>0.4173794</td>\n",
       "<td>0.1042557</td>\n",
       "<td>0.2289520</td>\n",
       "<td>108.5033904</td>\n",
       "<td>128.9432567</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.2913521</td>\n",
       "<td>1.7440902</td>\n",
       "<td>2.1076518</td>\n",
       "<td>0.3179597</td>\n",
       "<td>0.3842395</td>\n",
       "<td>0.0872078</td>\n",
       "<td>0.3161598</td>\n",
       "<td>74.4090232</td>\n",
       "<td>110.7651789</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2530770</td>\n",
       "<td>1.5650616</td>\n",
       "<td>1.9720198</td>\n",
       "<td>0.2853215</td>\n",
       "<td>0.3595128</td>\n",
       "<td>0.0782441</td>\n",
       "<td>0.3944040</td>\n",
       "<td>56.5061566</td>\n",
       "<td>97.2019770</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2009810</td>\n",
       "<td>1.3796910</td>\n",
       "<td>1.7745718</td>\n",
       "<td>0.2515272</td>\n",
       "<td>0.3235167</td>\n",
       "<td>0.1379744</td>\n",
       "<td>0.5323783</td>\n",
       "<td>37.9690976</td>\n",
       "<td>77.4571813</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1671209</td>\n",
       "<td>1.1100365</td>\n",
       "<td>1.6084443</td>\n",
       "<td>0.2023673</td>\n",
       "<td>0.2932305</td>\n",
       "<td>0.1109994</td>\n",
       "<td>0.6433777</td>\n",
       "<td>11.0036518</td>\n",
       "<td>60.8444333</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1431993</td>\n",
       "<td>0.9369668</td>\n",
       "<td>1.4741447</td>\n",
       "<td>0.1708155</td>\n",
       "<td>0.2687468</td>\n",
       "<td>0.0937003</td>\n",
       "<td>0.7370780</td>\n",
       "<td>-6.3033177</td>\n",
       "<td>47.4144729</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1250298</td>\n",
       "<td>0.8076039</td>\n",
       "<td>1.3630588</td>\n",
       "<td>0.1472318</td>\n",
       "<td>0.2484950</td>\n",
       "<td>0.0807573</td>\n",
       "<td>0.8178353</td>\n",
       "<td>-19.2396073</td>\n",
       "<td>36.3058837</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1098459</td>\n",
       "<td>0.6635086</td>\n",
       "<td>1.2631264</td>\n",
       "<td>0.1209622</td>\n",
       "<td>0.2302767</td>\n",
       "<td>0.0663483</td>\n",
       "<td>0.8841836</td>\n",
       "<td>-33.6491379</td>\n",
       "<td>26.3126362</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0960804</td>\n",
       "<td>0.5298449</td>\n",
       "<td>1.1714627</td>\n",
       "<td>0.0965944</td>\n",
       "<td>0.2135657</td>\n",
       "<td>0.0529865</td>\n",
       "<td>0.9371701</td>\n",
       "<td>-47.0155105</td>\n",
       "<td>17.1462679</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0823591</td>\n",
       "<td>0.4163684</td>\n",
       "<td>1.0875662</td>\n",
       "<td>0.0759068</td>\n",
       "<td>0.1982708</td>\n",
       "<td>0.0416353</td>\n",
       "<td>0.9788054</td>\n",
       "<td>-58.3631585</td>\n",
       "<td>8.7566164</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0614275</td>\n",
       "<td>0.2119380</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0386378</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0211946</td>\n",
       "<td>1.0</td>\n",
       "<td>-78.8062042</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.575141           2.9143    2.9143             0.531298         0.531298                    0.0291531       0.0291531                  191.43    191.43\n",
       "    2        0.0200069                   0.520915           2.60445   2.75938            0.474809         0.503053                    0.0260534       0.0552065                  160.445   175.938\n",
       "    3        0.0300027                   0.482578           2.3173    2.61209            0.42246          0.476203                    0.0231633       0.0783698                  131.73    161.209\n",
       "    4        0.0400061                   0.453277           2.38671   2.55574            0.435115         0.465929                    0.0238753       0.102245                   138.671   155.574\n",
       "    5        0.0500019                   0.42969            2.24606   2.49383            0.409473         0.454643                    0.0224512       0.124696                   124.606   149.383\n",
       "    6        0.100004                    0.34448            2.08503   2.28943            0.380116         0.417379                    0.104256        0.228952                   108.503   128.943\n",
       "    7        0.150006                    0.291352           1.74409   2.10765            0.31796          0.384239                    0.0872078       0.31616                    74.409    110.765\n",
       "    8        0.2                         0.253077           1.56506   1.97202            0.285322         0.359513                    0.0782441       0.394404                   56.5062   97.202\n",
       "    9        0.300004                    0.200981           1.37969   1.77457            0.251527         0.323517                    0.137974        0.532378                   37.9691   77.4572\n",
       "    10       0.4                         0.167121           1.11004   1.60844            0.202367         0.29323                     0.110999        0.643378                   11.0037   60.8444\n",
       "    11       0.500004                    0.143199           0.936967  1.47414            0.170816         0.268747                    0.0937003       0.737078                   -6.30332  47.4145\n",
       "    12       0.6                         0.12503            0.807604  1.36306            0.147232         0.248495                    0.0807573       0.817835                   -19.2396  36.3059\n",
       "    13       0.699996                    0.109846           0.663509  1.26313            0.120962         0.230277                    0.0663483       0.884184                   -33.6491  26.3126\n",
       "    14       0.8                         0.0960804          0.529845  1.17146            0.0965944        0.213566                    0.0529865       0.93717                    -47.0155  17.1463\n",
       "    15       0.899996                    0.0823591          0.416368  1.08757            0.0759068        0.198271                    0.0416353       0.978805                   -58.3632  8.75662\n",
       "    16       1                           0.0614275          0.211938  1                  0.0386378        0.182307                    0.0211946       1                          -78.8062  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method H2OBinomialModel.tnr of >"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_0_AutoML_20180625_175234\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.1167639474829593\n",
      "RMSE: 0.34170740039244\n",
      "LogLoss: 0.3795990920019023\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130948\n",
      "Null deviance: 124374.13927893189\n",
      "Residual deviance: 99420.79818621822\n",
      "AIC: 99434.79818621822\n",
      "AUC: 0.8305862969853443\n",
      "Gini: 0.6611725939706885\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24030127929039052: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>90474.0</td>\n",
       "<td>16607.0</td>\n",
       "<td>0.1551</td>\n",
       "<td> (16607.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9334.0</td>\n",
       "<td>14540.0</td>\n",
       "<td>0.391</td>\n",
       "<td> (9334.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>99808.0</td>\n",
       "<td>31147.0</td>\n",
       "<td>0.1981</td>\n",
       "<td> (25941.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      90474  16607  0.1551   (16607.0/107081.0)\n",
       "1      9334   14540  0.391    (9334.0/23874.0)\n",
       "Total  99808  31147  0.1981   (25941.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2403013</td>\n",
       "<td>0.5285255</td>\n",
       "<td>222.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1529696</td>\n",
       "<td>0.6516791</td>\n",
       "<td>296.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3520442</td>\n",
       "<td>0.5439141</td>\n",
       "<td>151.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4117837</td>\n",
       "<td>0.8448704</td>\n",
       "<td>121.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8488098</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0753915</td>\n",
       "<td>1.0</td>\n",
       "<td>387.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8488098</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2828681</td>\n",
       "<td>0.4124344</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1895643</td>\n",
       "<td>0.7444458</td>\n",
       "<td>262.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1801546</td>\n",
       "<td>0.7459682</td>\n",
       "<td>270.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.240301     0.528525  222\n",
       "max f2                       0.15297      0.651679  296\n",
       "max f0point5                 0.352044     0.543914  151\n",
       "max accuracy                 0.411784     0.84487   121\n",
       "max precision                0.84881      1         0\n",
       "max recall                   0.0753915    1         387\n",
       "max specificity              0.84881      1         0\n",
       "max absolute_mcc             0.282868     0.412434  193\n",
       "max min_per_class_accuracy   0.189564     0.744446  262\n",
       "max mean_per_class_accuracy  0.180155     0.745968  270"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.6271502</td>\n",
       "<td>5.1377168</td>\n",
       "<td>5.1377168</td>\n",
       "<td>0.9366412</td>\n",
       "<td>0.9366412</td>\n",
       "<td>0.0513948</td>\n",
       "<td>0.0513948</td>\n",
       "<td>413.7716811</td>\n",
       "<td>413.7716811</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5662824</td>\n",
       "<td>4.4091408</td>\n",
       "<td>4.7734288</td>\n",
       "<td>0.8038168</td>\n",
       "<td>0.8702290</td>\n",
       "<td>0.0441066</td>\n",
       "<td>0.0955014</td>\n",
       "<td>340.9140833</td>\n",
       "<td>377.3428822</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.5252263</td>\n",
       "<td>4.0437525</td>\n",
       "<td>4.5303272</td>\n",
       "<td>0.7372040</td>\n",
       "<td>0.8259099</td>\n",
       "<td>0.0404205</td>\n",
       "<td>0.1359219</td>\n",
       "<td>304.3752460</td>\n",
       "<td>353.0327178</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4929238</td>\n",
       "<td>3.4921400</td>\n",
       "<td>4.2707309</td>\n",
       "<td>0.6366412</td>\n",
       "<td>0.7785837</td>\n",
       "<td>0.0349334</td>\n",
       "<td>0.1708553</td>\n",
       "<td>249.2140033</td>\n",
       "<td>327.0730851</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4650058</td>\n",
       "<td>3.3774761</td>\n",
       "<td>4.0921618</td>\n",
       "<td>0.6157372</td>\n",
       "<td>0.7460293</td>\n",
       "<td>0.0337606</td>\n",
       "<td>0.2046159</td>\n",
       "<td>237.7476148</td>\n",
       "<td>309.2161760</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3687602</td>\n",
       "<td>2.8247225</td>\n",
       "<td>3.4584421</td>\n",
       "<td>0.5149664</td>\n",
       "<td>0.6304979</td>\n",
       "<td>0.1412415</td>\n",
       "<td>0.3458574</td>\n",
       "<td>182.4722509</td>\n",
       "<td>245.8442134</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.3073983</td>\n",
       "<td>2.2492230</td>\n",
       "<td>3.0553691</td>\n",
       "<td>0.4100489</td>\n",
       "<td>0.5570149</td>\n",
       "<td>0.1124654</td>\n",
       "<td>0.4583229</td>\n",
       "<td>124.9222994</td>\n",
       "<td>205.5369087</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2647046</td>\n",
       "<td>1.8122206</td>\n",
       "<td>2.7446176</td>\n",
       "<td>0.3303803</td>\n",
       "<td>0.5003627</td>\n",
       "<td>0.0906007</td>\n",
       "<td>0.5489235</td>\n",
       "<td>81.2220646</td>\n",
       "<td>174.4617576</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2073810</td>\n",
       "<td>1.4353980</td>\n",
       "<td>2.3081999</td>\n",
       "<td>0.2616830</td>\n",
       "<td>0.4208008</td>\n",
       "<td>0.1435453</td>\n",
       "<td>0.6924688</td>\n",
       "<td>43.5397989</td>\n",
       "<td>130.8199939</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1707956</td>\n",
       "<td>1.0878358</td>\n",
       "<td>2.0031205</td>\n",
       "<td>0.1983200</td>\n",
       "<td>0.3651827</td>\n",
       "<td>0.1087794</td>\n",
       "<td>0.8012482</td>\n",
       "<td>8.7835788</td>\n",
       "<td>100.3120550</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1453638</td>\n",
       "<td>0.7723589</td>\n",
       "<td>1.7569607</td>\n",
       "<td>0.1408064</td>\n",
       "<td>0.3203061</td>\n",
       "<td>0.0772388</td>\n",
       "<td>0.8784871</td>\n",
       "<td>-22.7641117</td>\n",
       "<td>75.6960698</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1262417</td>\n",
       "<td>0.5579504</td>\n",
       "<td>1.5571333</td>\n",
       "<td>0.1017182</td>\n",
       "<td>0.2838761</td>\n",
       "<td>0.0557929</td>\n",
       "<td>0.9342800</td>\n",
       "<td>-44.2049569</td>\n",
       "<td>55.7133283</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1101940</td>\n",
       "<td>0.3535362</td>\n",
       "<td>1.3851965</td>\n",
       "<td>0.0644521</td>\n",
       "<td>0.2525309</td>\n",
       "<td>0.0353523</td>\n",
       "<td>0.9696322</td>\n",
       "<td>-64.6463841</td>\n",
       "<td>38.5196464</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0961032</td>\n",
       "<td>0.2173830</td>\n",
       "<td>1.2392142</td>\n",
       "<td>0.0396304</td>\n",
       "<td>0.2259173</td>\n",
       "<td>0.0217391</td>\n",
       "<td>0.9913714</td>\n",
       "<td>-78.2616996</td>\n",
       "<td>23.9214208</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0822907</td>\n",
       "<td>0.0783309</td>\n",
       "<td>1.1102315</td>\n",
       "<td>0.0142803</td>\n",
       "<td>0.2024029</td>\n",
       "<td>0.0078328</td>\n",
       "<td>0.9992042</td>\n",
       "<td>-92.1669121</td>\n",
       "<td>11.0231549</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0614696</td>\n",
       "<td>0.0079581</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0014508</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0007958</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.2041855</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ----------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.62715            5.13772     5.13772            0.936641         0.936641                    0.0513948       0.0513948                  413.772   413.772\n",
       "    2        0.0200069                   0.566282           4.40914     4.77343            0.803817         0.870229                    0.0441066       0.0955014                  340.914   377.343\n",
       "    3        0.0300027                   0.525226           4.04375     4.53033            0.737204         0.82591                     0.0404205       0.135922                   304.375   353.033\n",
       "    4        0.0400061                   0.492924           3.49214     4.27073            0.636641         0.778584                    0.0349334       0.170855                   249.214   327.073\n",
       "    5        0.0500019                   0.465006           3.37748     4.09216            0.615737         0.746029                    0.0337606       0.204616                   237.748   309.216\n",
       "    6        0.100004                    0.36876            2.82472     3.45844            0.514966         0.630498                    0.141242        0.345857                   182.472   245.844\n",
       "    7        0.150006                    0.307398           2.24922     3.05537            0.410049         0.557015                    0.112465        0.458323                   124.922   205.537\n",
       "    8        0.2                         0.264705           1.81222     2.74462            0.33038          0.500363                    0.0906007       0.548924                   81.2221   174.462\n",
       "    9        0.300004                    0.207381           1.4354      2.3082             0.261683         0.420801                    0.143545        0.692469                   43.5398   130.82\n",
       "    10       0.4                         0.170796           1.08784     2.00312            0.19832          0.365183                    0.108779        0.801248                   8.78358   100.312\n",
       "    11       0.500004                    0.145364           0.772359    1.75696            0.140806         0.320306                    0.0772388       0.878487                   -22.7641  75.6961\n",
       "    12       0.6                         0.126242           0.55795     1.55713            0.101718         0.283876                    0.0557929       0.93428                    -44.205   55.7133\n",
       "    13       0.699996                    0.110194           0.353536    1.3852             0.0644521        0.252531                    0.0353523       0.969632                   -64.6464  38.5196\n",
       "    14       0.8                         0.0961032          0.217383    1.23921            0.0396304        0.225917                    0.0217391       0.991371                   -78.2617  23.9214\n",
       "    15       0.899996                    0.0822907          0.0783309   1.11023            0.0142803        0.202403                    0.00783279      0.999204                   -92.1669  11.0232\n",
       "    16       1                           0.0614696          0.00795814  1                  0.00145082       0.182307                    0.000795845     1                          -99.2042  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.1386513474415332\n",
      "RMSE: 0.37235916457304125\n",
      "LogLoss: 0.4394559813847542\n",
      "Null degrees of freedom: 33031\n",
      "Residual degrees of freedom: 33025\n",
      "Null deviance: 31732.354674481932\n",
      "Residual deviance: 29032.219954202403\n",
      "AIC: 29046.219954202403\n",
      "AUC: 0.7102666513197816\n",
      "Gini: 0.42053330263956323\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.18707726312815404: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>19160.0</td>\n",
       "<td>7730.0</td>\n",
       "<td>0.2875</td>\n",
       "<td> (7730.0/26890.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2529.0</td>\n",
       "<td>3613.0</td>\n",
       "<td>0.4118</td>\n",
       "<td> (2529.0/6142.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>21689.0</td>\n",
       "<td>11343.0</td>\n",
       "<td>0.3106</td>\n",
       "<td> (10259.0/33032.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      19160  7730   0.2875   (7730.0/26890.0)\n",
       "1      2529   3613   0.4118   (2529.0/6142.0)\n",
       "Total  21689  11343  0.3106   (10259.0/33032.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1870773</td>\n",
       "<td>0.4132685</td>\n",
       "<td>255.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1147101</td>\n",
       "<td>0.5726111</td>\n",
       "<td>334.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2910025</td>\n",
       "<td>0.3779960</td>\n",
       "<td>173.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5696954</td>\n",
       "<td>0.8161177</td>\n",
       "<td>37.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8006882</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0668733</td>\n",
       "<td>1.0</td>\n",
       "<td>396.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8006882</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1870773</td>\n",
       "<td>0.2464403</td>\n",
       "<td>255.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1677169</td>\n",
       "<td>0.6499442</td>\n",
       "<td>273.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1499723</td>\n",
       "<td>0.6538983</td>\n",
       "<td>291.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.187077     0.413269  255\n",
       "max f2                       0.11471      0.572611  334\n",
       "max f0point5                 0.291002     0.377996  173\n",
       "max accuracy                 0.569695     0.816118  37\n",
       "max precision                0.800688     1         0\n",
       "max recall                   0.0668733    1         396\n",
       "max specificity              0.800688     1         0\n",
       "max absolute_mcc             0.187077     0.24644   255\n",
       "max min_per_class_accuracy   0.167717     0.649944  273\n",
       "max mean_per_class_accuracy  0.149972     0.653898  291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.59 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100206</td>\n",
       "<td>0.5870321</td>\n",
       "<td>3.2008350</td>\n",
       "<td>3.2008350</td>\n",
       "<td>0.5951662</td>\n",
       "<td>0.5951662</td>\n",
       "<td>0.0320742</td>\n",
       "<td>0.0320742</td>\n",
       "<td>220.0835021</td>\n",
       "<td>220.0835021</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200109</td>\n",
       "<td>0.5271460</td>\n",
       "<td>2.5260551</td>\n",
       "<td>2.8639555</td>\n",
       "<td>0.4696970</td>\n",
       "<td>0.5325265</td>\n",
       "<td>0.0252361</td>\n",
       "<td>0.0573103</td>\n",
       "<td>152.6055080</td>\n",
       "<td>186.3955474</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300012</td>\n",
       "<td>0.4907770</td>\n",
       "<td>2.3467867</td>\n",
       "<td>2.6917398</td>\n",
       "<td>0.4363636</td>\n",
       "<td>0.5005045</td>\n",
       "<td>0.0234451</td>\n",
       "<td>0.0807555</td>\n",
       "<td>134.6786655</td>\n",
       "<td>169.1739823</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400218</td>\n",
       "<td>0.4607515</td>\n",
       "<td>2.4209361</td>\n",
       "<td>2.6239365</td>\n",
       "<td>0.4501511</td>\n",
       "<td>0.4878971</td>\n",
       "<td>0.0242592</td>\n",
       "<td>0.1050147</td>\n",
       "<td>142.0936133</td>\n",
       "<td>162.3936479</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500121</td>\n",
       "<td>0.4368630</td>\n",
       "<td>2.2490039</td>\n",
       "<td>2.5490407</td>\n",
       "<td>0.4181818</td>\n",
       "<td>0.4739709</td>\n",
       "<td>0.0224683</td>\n",
       "<td>0.1274829</td>\n",
       "<td>124.9003878</td>\n",
       "<td>154.9040741</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000242</td>\n",
       "<td>0.3510749</td>\n",
       "<td>2.0086311</td>\n",
       "<td>2.2788359</td>\n",
       "<td>0.3734867</td>\n",
       "<td>0.4237288</td>\n",
       "<td>0.1004559</td>\n",
       "<td>0.2279388</td>\n",
       "<td>100.8631082</td>\n",
       "<td>127.8835912</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500061</td>\n",
       "<td>0.2954589</td>\n",
       "<td>1.8111431</td>\n",
       "<td>2.1230012</td>\n",
       "<td>0.3367656</td>\n",
       "<td>0.3947528</td>\n",
       "<td>0.0905243</td>\n",
       "<td>0.3184630</td>\n",
       "<td>81.1143143</td>\n",
       "<td>112.3001248</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000182</td>\n",
       "<td>0.2562466</td>\n",
       "<td>1.5723968</td>\n",
       "<td>1.9853293</td>\n",
       "<td>0.2923729</td>\n",
       "<td>0.3691539</td>\n",
       "<td>0.0786389</td>\n",
       "<td>0.3971019</td>\n",
       "<td>57.2396779</td>\n",
       "<td>98.5329296</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000121</td>\n",
       "<td>0.2033653</td>\n",
       "<td>1.3644590</td>\n",
       "<td>1.7783934</td>\n",
       "<td>0.2537087</td>\n",
       "<td>0.3306761</td>\n",
       "<td>0.1364376</td>\n",
       "<td>0.5335396</td>\n",
       "<td>36.4459039</td>\n",
       "<td>77.8393428</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000061</td>\n",
       "<td>0.1689245</td>\n",
       "<td>1.1153394</td>\n",
       "<td>1.6126425</td>\n",
       "<td>0.2073872</td>\n",
       "<td>0.2998562</td>\n",
       "<td>0.1115272</td>\n",
       "<td>0.6450668</td>\n",
       "<td>11.5339429</td>\n",
       "<td>61.2642473</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1444336</td>\n",
       "<td>1.0290431</td>\n",
       "<td>1.4959297</td>\n",
       "<td>0.1913412</td>\n",
       "<td>0.2781545</td>\n",
       "<td>0.1028981</td>\n",
       "<td>0.7479648</td>\n",
       "<td>2.9043094</td>\n",
       "<td>49.5929665</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999939</td>\n",
       "<td>0.1259430</td>\n",
       "<td>0.7652694</td>\n",
       "<td>1.3741591</td>\n",
       "<td>0.1422949</td>\n",
       "<td>0.2555124</td>\n",
       "<td>0.0765223</td>\n",
       "<td>0.8244871</td>\n",
       "<td>-23.4730611</td>\n",
       "<td>37.4159097</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999879</td>\n",
       "<td>0.1109070</td>\n",
       "<td>0.6415237</td>\n",
       "<td>1.2695014</td>\n",
       "<td>0.1192855</td>\n",
       "<td>0.2360522</td>\n",
       "<td>0.0641485</td>\n",
       "<td>0.8886356</td>\n",
       "<td>-35.8476299</td>\n",
       "<td>26.9501424</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999818</td>\n",
       "<td>0.0968897</td>\n",
       "<td>0.5259192</td>\n",
       "<td>1.1765572</td>\n",
       "<td>0.0977899</td>\n",
       "<td>0.2187701</td>\n",
       "<td>0.0525887</td>\n",
       "<td>0.9412244</td>\n",
       "<td>-47.4080824</td>\n",
       "<td>17.6557160</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999758</td>\n",
       "<td>0.0829745</td>\n",
       "<td>0.4054300</td>\n",
       "<td>1.0908792</td>\n",
       "<td>0.0753860</td>\n",
       "<td>0.2028391</td>\n",
       "<td>0.0405405</td>\n",
       "<td>0.9817649</td>\n",
       "<td>-59.4570047</td>\n",
       "<td>9.0879242</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0624311</td>\n",
       "<td>0.1823069</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0338983</td>\n",
       "<td>0.1859409</td>\n",
       "<td>0.0182351</td>\n",
       "<td>1.0</td>\n",
       "<td>-81.7693127</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100206                   0.587032           3.20084   3.20084            0.595166         0.595166                    0.0320742       0.0320742                  220.084   220.084\n",
       "    2        0.0200109                   0.527146           2.52606   2.86396            0.469697         0.532526                    0.0252361       0.0573103                  152.606   186.396\n",
       "    3        0.0300012                   0.490777           2.34679   2.69174            0.436364         0.500505                    0.0234451       0.0807555                  134.679   169.174\n",
       "    4        0.0400218                   0.460751           2.42094   2.62394            0.450151         0.487897                    0.0242592       0.105015                   142.094   162.394\n",
       "    5        0.0500121                   0.436863           2.249     2.54904            0.418182         0.473971                    0.0224683       0.127483                   124.9     154.904\n",
       "    6        0.100024                    0.351075           2.00863   2.27884            0.373487         0.423729                    0.100456        0.227939                   100.863   127.884\n",
       "    7        0.150006                    0.295459           1.81114   2.123              0.336766         0.394753                    0.0905243       0.318463                   81.1143   112.3\n",
       "    8        0.200018                    0.256247           1.5724    1.98533            0.292373         0.369154                    0.0786389       0.397102                   57.2397   98.5329\n",
       "    9        0.300012                    0.203365           1.36446   1.77839            0.253709         0.330676                    0.136438        0.53354                    36.4459   77.8393\n",
       "    10       0.400006                    0.168924           1.11534   1.61264            0.207387         0.299856                    0.111527        0.645067                   11.5339   61.2642\n",
       "    11       0.5                         0.144434           1.02904   1.49593            0.191341         0.278155                    0.102898        0.747965                   2.90431   49.593\n",
       "    12       0.599994                    0.125943           0.765269  1.37416            0.142295         0.255512                    0.0765223       0.824487                   -23.4731  37.4159\n",
       "    13       0.699988                    0.110907           0.641524  1.2695             0.119285         0.236052                    0.0641485       0.888636                   -35.8476  26.9501\n",
       "    14       0.799982                    0.0968897          0.525919  1.17656            0.0977899        0.21877                     0.0525887       0.941224                   -47.4081  17.6557\n",
       "    15       0.899976                    0.0829745          0.40543   1.09088            0.075386         0.202839                    0.0405405       0.981765                   -59.457   9.08792\n",
       "    16       1                           0.0624311          0.182307  1                  0.0338983        0.185941                    0.0182351       1                          -81.7693  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.13731436524493457\n",
      "RMSE: 0.37055952996102337\n",
      "LogLoss: 0.4365729016322642\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130947\n",
      "Null deviance: 124375.46514588114\n",
      "Residual deviance: 114342.80866650632\n",
      "AIC: 114358.80866650632\n",
      "AUC: 0.7052102719993631\n",
      "Gini: 0.4104205439987263\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.1834653742477123: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>75544.0</td>\n",
       "<td>31537.0</td>\n",
       "<td>0.2945</td>\n",
       "<td> (31537.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9818.0</td>\n",
       "<td>14056.0</td>\n",
       "<td>0.4112</td>\n",
       "<td> (9818.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>85362.0</td>\n",
       "<td>45593.0</td>\n",
       "<td>0.3158</td>\n",
       "<td> (41355.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      75544  31537  0.2945   (31537.0/107081.0)\n",
       "1      9818   14056  0.4112   (9818.0/23874.0)\n",
       "Total  85362  45593  0.3158   (41355.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1834654</td>\n",
       "<td>0.4046814</td>\n",
       "<td>261.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1142473</td>\n",
       "<td>0.5642710</td>\n",
       "<td>335.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2981927</td>\n",
       "<td>0.3695123</td>\n",
       "<td>172.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6033991</td>\n",
       "<td>0.8184109</td>\n",
       "<td>29.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8158356</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0635534</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8158356</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2024945</td>\n",
       "<td>0.2396925</td>\n",
       "<td>244.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1661343</td>\n",
       "<td>0.6488453</td>\n",
       "<td>278.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1577254</td>\n",
       "<td>0.6492678</td>\n",
       "<td>287.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.183465     0.404681  261\n",
       "max f2                       0.114247     0.564271  335\n",
       "max f0point5                 0.298193     0.369512  172\n",
       "max accuracy                 0.603399     0.818411  29\n",
       "max precision                0.815836     1         0\n",
       "max recall                   0.0635534    1         399\n",
       "max specificity              0.815836     1         0\n",
       "max absolute_mcc             0.202495     0.239692  244\n",
       "max min_per_class_accuracy   0.166134     0.648845  278\n",
       "max mean_per_class_accuracy  0.157725     0.649268  287"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5751406</td>\n",
       "<td>2.9143039</td>\n",
       "<td>2.9143039</td>\n",
       "<td>0.5312977</td>\n",
       "<td>0.5312977</td>\n",
       "<td>0.0291531</td>\n",
       "<td>0.0291531</td>\n",
       "<td>191.4303912</td>\n",
       "<td>191.4303912</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5209152</td>\n",
       "<td>2.6044498</td>\n",
       "<td>2.7593768</td>\n",
       "<td>0.4748092</td>\n",
       "<td>0.5030534</td>\n",
       "<td>0.0260534</td>\n",
       "<td>0.0552065</td>\n",
       "<td>160.4449761</td>\n",
       "<td>175.9376837</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.4825780</td>\n",
       "<td>2.3173006</td>\n",
       "<td>2.6120931</td>\n",
       "<td>0.4224599</td>\n",
       "<td>0.4762026</td>\n",
       "<td>0.0231633</td>\n",
       "<td>0.0783698</td>\n",
       "<td>131.7300632</td>\n",
       "<td>161.2093113</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4532771</td>\n",
       "<td>2.3867144</td>\n",
       "<td>2.5557377</td>\n",
       "<td>0.4351145</td>\n",
       "<td>0.4659286</td>\n",
       "<td>0.0238753</td>\n",
       "<td>0.1022451</td>\n",
       "<td>138.6714411</td>\n",
       "<td>155.5737682</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4296896</td>\n",
       "<td>2.2460635</td>\n",
       "<td>2.4938312</td>\n",
       "<td>0.4094729</td>\n",
       "<td>0.4546426</td>\n",
       "<td>0.0224512</td>\n",
       "<td>0.1246963</td>\n",
       "<td>124.6063542</td>\n",
       "<td>149.3831230</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3444799</td>\n",
       "<td>2.0850339</td>\n",
       "<td>2.2894326</td>\n",
       "<td>0.3801161</td>\n",
       "<td>0.4173794</td>\n",
       "<td>0.1042557</td>\n",
       "<td>0.2289520</td>\n",
       "<td>108.5033904</td>\n",
       "<td>128.9432567</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.2913521</td>\n",
       "<td>1.7440902</td>\n",
       "<td>2.1076518</td>\n",
       "<td>0.3179597</td>\n",
       "<td>0.3842395</td>\n",
       "<td>0.0872078</td>\n",
       "<td>0.3161598</td>\n",
       "<td>74.4090232</td>\n",
       "<td>110.7651789</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2530770</td>\n",
       "<td>1.5650616</td>\n",
       "<td>1.9720198</td>\n",
       "<td>0.2853215</td>\n",
       "<td>0.3595128</td>\n",
       "<td>0.0782441</td>\n",
       "<td>0.3944040</td>\n",
       "<td>56.5061566</td>\n",
       "<td>97.2019770</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2009810</td>\n",
       "<td>1.3796910</td>\n",
       "<td>1.7745718</td>\n",
       "<td>0.2515272</td>\n",
       "<td>0.3235167</td>\n",
       "<td>0.1379744</td>\n",
       "<td>0.5323783</td>\n",
       "<td>37.9690976</td>\n",
       "<td>77.4571813</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1671209</td>\n",
       "<td>1.1100365</td>\n",
       "<td>1.6084443</td>\n",
       "<td>0.2023673</td>\n",
       "<td>0.2932305</td>\n",
       "<td>0.1109994</td>\n",
       "<td>0.6433777</td>\n",
       "<td>11.0036518</td>\n",
       "<td>60.8444333</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1431993</td>\n",
       "<td>0.9369668</td>\n",
       "<td>1.4741447</td>\n",
       "<td>0.1708155</td>\n",
       "<td>0.2687468</td>\n",
       "<td>0.0937003</td>\n",
       "<td>0.7370780</td>\n",
       "<td>-6.3033177</td>\n",
       "<td>47.4144729</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1250298</td>\n",
       "<td>0.8076039</td>\n",
       "<td>1.3630588</td>\n",
       "<td>0.1472318</td>\n",
       "<td>0.2484950</td>\n",
       "<td>0.0807573</td>\n",
       "<td>0.8178353</td>\n",
       "<td>-19.2396073</td>\n",
       "<td>36.3058837</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1098459</td>\n",
       "<td>0.6635086</td>\n",
       "<td>1.2631264</td>\n",
       "<td>0.1209622</td>\n",
       "<td>0.2302767</td>\n",
       "<td>0.0663483</td>\n",
       "<td>0.8841836</td>\n",
       "<td>-33.6491379</td>\n",
       "<td>26.3126362</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0960804</td>\n",
       "<td>0.5298449</td>\n",
       "<td>1.1714627</td>\n",
       "<td>0.0965944</td>\n",
       "<td>0.2135657</td>\n",
       "<td>0.0529865</td>\n",
       "<td>0.9371701</td>\n",
       "<td>-47.0155105</td>\n",
       "<td>17.1462679</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0823591</td>\n",
       "<td>0.4163684</td>\n",
       "<td>1.0875662</td>\n",
       "<td>0.0759068</td>\n",
       "<td>0.1982708</td>\n",
       "<td>0.0416353</td>\n",
       "<td>0.9788054</td>\n",
       "<td>-58.3631585</td>\n",
       "<td>8.7566164</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0614275</td>\n",
       "<td>0.2119380</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0386378</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0211946</td>\n",
       "<td>1.0</td>\n",
       "<td>-78.8062042</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.575141           2.9143    2.9143             0.531298         0.531298                    0.0291531       0.0291531                  191.43    191.43\n",
       "    2        0.0200069                   0.520915           2.60445   2.75938            0.474809         0.503053                    0.0260534       0.0552065                  160.445   175.938\n",
       "    3        0.0300027                   0.482578           2.3173    2.61209            0.42246          0.476203                    0.0231633       0.0783698                  131.73    161.209\n",
       "    4        0.0400061                   0.453277           2.38671   2.55574            0.435115         0.465929                    0.0238753       0.102245                   138.671   155.574\n",
       "    5        0.0500019                   0.42969            2.24606   2.49383            0.409473         0.454643                    0.0224512       0.124696                   124.606   149.383\n",
       "    6        0.100004                    0.34448            2.08503   2.28943            0.380116         0.417379                    0.104256        0.228952                   108.503   128.943\n",
       "    7        0.150006                    0.291352           1.74409   2.10765            0.31796          0.384239                    0.0872078       0.31616                    74.409    110.765\n",
       "    8        0.2                         0.253077           1.56506   1.97202            0.285322         0.359513                    0.0782441       0.394404                   56.5062   97.202\n",
       "    9        0.300004                    0.200981           1.37969   1.77457            0.251527         0.323517                    0.137974        0.532378                   37.9691   77.4572\n",
       "    10       0.4                         0.167121           1.11004   1.60844            0.202367         0.29323                     0.110999        0.643378                   11.0037   60.8444\n",
       "    11       0.500004                    0.143199           0.936967  1.47414            0.170816         0.268747                    0.0937003       0.737078                   -6.30332  47.4145\n",
       "    12       0.6                         0.12503            0.807604  1.36306            0.147232         0.248495                    0.0807573       0.817835                   -19.2396  36.3059\n",
       "    13       0.699996                    0.109846           0.663509  1.26313            0.120962         0.230277                    0.0663483       0.884184                   -33.6491  26.3126\n",
       "    14       0.8                         0.0960804          0.529845  1.17146            0.0965944        0.213566                    0.0529865       0.93717                    -47.0155  17.1463\n",
       "    15       0.899996                    0.0823591          0.416368  1.08757            0.0759068        0.198271                    0.0416353       0.978805                   -58.3632  8.75662\n",
       "    16       1                           0.0614275          0.211938  1                  0.0386378        0.182307                    0.0211946       1                          -78.8062  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method H2OBinomialModel.tpr of >"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_0_AutoML_20180625_175234\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.1167639474829593\n",
      "RMSE: 0.34170740039244\n",
      "LogLoss: 0.3795990920019023\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130948\n",
      "Null deviance: 124374.13927893189\n",
      "Residual deviance: 99420.79818621822\n",
      "AIC: 99434.79818621822\n",
      "AUC: 0.8305862969853443\n",
      "Gini: 0.6611725939706885\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.24030127929039052: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>90474.0</td>\n",
       "<td>16607.0</td>\n",
       "<td>0.1551</td>\n",
       "<td> (16607.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9334.0</td>\n",
       "<td>14540.0</td>\n",
       "<td>0.391</td>\n",
       "<td> (9334.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>99808.0</td>\n",
       "<td>31147.0</td>\n",
       "<td>0.1981</td>\n",
       "<td> (25941.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      90474  16607  0.1551   (16607.0/107081.0)\n",
       "1      9334   14540  0.391    (9334.0/23874.0)\n",
       "Total  99808  31147  0.1981   (25941.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2403013</td>\n",
       "<td>0.5285255</td>\n",
       "<td>222.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1529696</td>\n",
       "<td>0.6516791</td>\n",
       "<td>296.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3520442</td>\n",
       "<td>0.5439141</td>\n",
       "<td>151.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4117837</td>\n",
       "<td>0.8448704</td>\n",
       "<td>121.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8488098</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0753915</td>\n",
       "<td>1.0</td>\n",
       "<td>387.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8488098</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2828681</td>\n",
       "<td>0.4124344</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1895643</td>\n",
       "<td>0.7444458</td>\n",
       "<td>262.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1801546</td>\n",
       "<td>0.7459682</td>\n",
       "<td>270.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.240301     0.528525  222\n",
       "max f2                       0.15297      0.651679  296\n",
       "max f0point5                 0.352044     0.543914  151\n",
       "max accuracy                 0.411784     0.84487   121\n",
       "max precision                0.84881      1         0\n",
       "max recall                   0.0753915    1         387\n",
       "max specificity              0.84881      1         0\n",
       "max absolute_mcc             0.282868     0.412434  193\n",
       "max min_per_class_accuracy   0.189564     0.744446  262\n",
       "max mean_per_class_accuracy  0.180155     0.745968  270"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.6271502</td>\n",
       "<td>5.1377168</td>\n",
       "<td>5.1377168</td>\n",
       "<td>0.9366412</td>\n",
       "<td>0.9366412</td>\n",
       "<td>0.0513948</td>\n",
       "<td>0.0513948</td>\n",
       "<td>413.7716811</td>\n",
       "<td>413.7716811</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5662824</td>\n",
       "<td>4.4091408</td>\n",
       "<td>4.7734288</td>\n",
       "<td>0.8038168</td>\n",
       "<td>0.8702290</td>\n",
       "<td>0.0441066</td>\n",
       "<td>0.0955014</td>\n",
       "<td>340.9140833</td>\n",
       "<td>377.3428822</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.5252263</td>\n",
       "<td>4.0437525</td>\n",
       "<td>4.5303272</td>\n",
       "<td>0.7372040</td>\n",
       "<td>0.8259099</td>\n",
       "<td>0.0404205</td>\n",
       "<td>0.1359219</td>\n",
       "<td>304.3752460</td>\n",
       "<td>353.0327178</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4929238</td>\n",
       "<td>3.4921400</td>\n",
       "<td>4.2707309</td>\n",
       "<td>0.6366412</td>\n",
       "<td>0.7785837</td>\n",
       "<td>0.0349334</td>\n",
       "<td>0.1708553</td>\n",
       "<td>249.2140033</td>\n",
       "<td>327.0730851</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4650058</td>\n",
       "<td>3.3774761</td>\n",
       "<td>4.0921618</td>\n",
       "<td>0.6157372</td>\n",
       "<td>0.7460293</td>\n",
       "<td>0.0337606</td>\n",
       "<td>0.2046159</td>\n",
       "<td>237.7476148</td>\n",
       "<td>309.2161760</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3687602</td>\n",
       "<td>2.8247225</td>\n",
       "<td>3.4584421</td>\n",
       "<td>0.5149664</td>\n",
       "<td>0.6304979</td>\n",
       "<td>0.1412415</td>\n",
       "<td>0.3458574</td>\n",
       "<td>182.4722509</td>\n",
       "<td>245.8442134</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.3073983</td>\n",
       "<td>2.2492230</td>\n",
       "<td>3.0553691</td>\n",
       "<td>0.4100489</td>\n",
       "<td>0.5570149</td>\n",
       "<td>0.1124654</td>\n",
       "<td>0.4583229</td>\n",
       "<td>124.9222994</td>\n",
       "<td>205.5369087</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2647046</td>\n",
       "<td>1.8122206</td>\n",
       "<td>2.7446176</td>\n",
       "<td>0.3303803</td>\n",
       "<td>0.5003627</td>\n",
       "<td>0.0906007</td>\n",
       "<td>0.5489235</td>\n",
       "<td>81.2220646</td>\n",
       "<td>174.4617576</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2073810</td>\n",
       "<td>1.4353980</td>\n",
       "<td>2.3081999</td>\n",
       "<td>0.2616830</td>\n",
       "<td>0.4208008</td>\n",
       "<td>0.1435453</td>\n",
       "<td>0.6924688</td>\n",
       "<td>43.5397989</td>\n",
       "<td>130.8199939</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1707956</td>\n",
       "<td>1.0878358</td>\n",
       "<td>2.0031205</td>\n",
       "<td>0.1983200</td>\n",
       "<td>0.3651827</td>\n",
       "<td>0.1087794</td>\n",
       "<td>0.8012482</td>\n",
       "<td>8.7835788</td>\n",
       "<td>100.3120550</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1453638</td>\n",
       "<td>0.7723589</td>\n",
       "<td>1.7569607</td>\n",
       "<td>0.1408064</td>\n",
       "<td>0.3203061</td>\n",
       "<td>0.0772388</td>\n",
       "<td>0.8784871</td>\n",
       "<td>-22.7641117</td>\n",
       "<td>75.6960698</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1262417</td>\n",
       "<td>0.5579504</td>\n",
       "<td>1.5571333</td>\n",
       "<td>0.1017182</td>\n",
       "<td>0.2838761</td>\n",
       "<td>0.0557929</td>\n",
       "<td>0.9342800</td>\n",
       "<td>-44.2049569</td>\n",
       "<td>55.7133283</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1101940</td>\n",
       "<td>0.3535362</td>\n",
       "<td>1.3851965</td>\n",
       "<td>0.0644521</td>\n",
       "<td>0.2525309</td>\n",
       "<td>0.0353523</td>\n",
       "<td>0.9696322</td>\n",
       "<td>-64.6463841</td>\n",
       "<td>38.5196464</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0961032</td>\n",
       "<td>0.2173830</td>\n",
       "<td>1.2392142</td>\n",
       "<td>0.0396304</td>\n",
       "<td>0.2259173</td>\n",
       "<td>0.0217391</td>\n",
       "<td>0.9913714</td>\n",
       "<td>-78.2616996</td>\n",
       "<td>23.9214208</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0822907</td>\n",
       "<td>0.0783309</td>\n",
       "<td>1.1102315</td>\n",
       "<td>0.0142803</td>\n",
       "<td>0.2024029</td>\n",
       "<td>0.0078328</td>\n",
       "<td>0.9992042</td>\n",
       "<td>-92.1669121</td>\n",
       "<td>11.0231549</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0614696</td>\n",
       "<td>0.0079581</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0014508</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0007958</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.2041855</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ----------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.62715            5.13772     5.13772            0.936641         0.936641                    0.0513948       0.0513948                  413.772   413.772\n",
       "    2        0.0200069                   0.566282           4.40914     4.77343            0.803817         0.870229                    0.0441066       0.0955014                  340.914   377.343\n",
       "    3        0.0300027                   0.525226           4.04375     4.53033            0.737204         0.82591                     0.0404205       0.135922                   304.375   353.033\n",
       "    4        0.0400061                   0.492924           3.49214     4.27073            0.636641         0.778584                    0.0349334       0.170855                   249.214   327.073\n",
       "    5        0.0500019                   0.465006           3.37748     4.09216            0.615737         0.746029                    0.0337606       0.204616                   237.748   309.216\n",
       "    6        0.100004                    0.36876            2.82472     3.45844            0.514966         0.630498                    0.141242        0.345857                   182.472   245.844\n",
       "    7        0.150006                    0.307398           2.24922     3.05537            0.410049         0.557015                    0.112465        0.458323                   124.922   205.537\n",
       "    8        0.2                         0.264705           1.81222     2.74462            0.33038          0.500363                    0.0906007       0.548924                   81.2221   174.462\n",
       "    9        0.300004                    0.207381           1.4354      2.3082             0.261683         0.420801                    0.143545        0.692469                   43.5398   130.82\n",
       "    10       0.4                         0.170796           1.08784     2.00312            0.19832          0.365183                    0.108779        0.801248                   8.78358   100.312\n",
       "    11       0.500004                    0.145364           0.772359    1.75696            0.140806         0.320306                    0.0772388       0.878487                   -22.7641  75.6961\n",
       "    12       0.6                         0.126242           0.55795     1.55713            0.101718         0.283876                    0.0557929       0.93428                    -44.205   55.7133\n",
       "    13       0.699996                    0.110194           0.353536    1.3852             0.0644521        0.252531                    0.0353523       0.969632                   -64.6464  38.5196\n",
       "    14       0.8                         0.0961032          0.217383    1.23921            0.0396304        0.225917                    0.0217391       0.991371                   -78.2617  23.9214\n",
       "    15       0.899996                    0.0822907          0.0783309   1.11023            0.0142803        0.202403                    0.00783279      0.999204                   -92.1669  11.0232\n",
       "    16       1                           0.0614696          0.00795814  1                  0.00145082       0.182307                    0.000795845     1                          -99.2042  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.1386513474415332\n",
      "RMSE: 0.37235916457304125\n",
      "LogLoss: 0.4394559813847542\n",
      "Null degrees of freedom: 33031\n",
      "Residual degrees of freedom: 33025\n",
      "Null deviance: 31732.354674481932\n",
      "Residual deviance: 29032.219954202403\n",
      "AIC: 29046.219954202403\n",
      "AUC: 0.7102666513197816\n",
      "Gini: 0.42053330263956323\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.18707726312815404: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>19160.0</td>\n",
       "<td>7730.0</td>\n",
       "<td>0.2875</td>\n",
       "<td> (7730.0/26890.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2529.0</td>\n",
       "<td>3613.0</td>\n",
       "<td>0.4118</td>\n",
       "<td> (2529.0/6142.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>21689.0</td>\n",
       "<td>11343.0</td>\n",
       "<td>0.3106</td>\n",
       "<td> (10259.0/33032.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      19160  7730   0.2875   (7730.0/26890.0)\n",
       "1      2529   3613   0.4118   (2529.0/6142.0)\n",
       "Total  21689  11343  0.3106   (10259.0/33032.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1870773</td>\n",
       "<td>0.4132685</td>\n",
       "<td>255.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1147101</td>\n",
       "<td>0.5726111</td>\n",
       "<td>334.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2910025</td>\n",
       "<td>0.3779960</td>\n",
       "<td>173.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5696954</td>\n",
       "<td>0.8161177</td>\n",
       "<td>37.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8006882</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0668733</td>\n",
       "<td>1.0</td>\n",
       "<td>396.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8006882</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1870773</td>\n",
       "<td>0.2464403</td>\n",
       "<td>255.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1677169</td>\n",
       "<td>0.6499442</td>\n",
       "<td>273.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1499723</td>\n",
       "<td>0.6538983</td>\n",
       "<td>291.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.187077     0.413269  255\n",
       "max f2                       0.11471      0.572611  334\n",
       "max f0point5                 0.291002     0.377996  173\n",
       "max accuracy                 0.569695     0.816118  37\n",
       "max precision                0.800688     1         0\n",
       "max recall                   0.0668733    1         396\n",
       "max specificity              0.800688     1         0\n",
       "max absolute_mcc             0.187077     0.24644   255\n",
       "max min_per_class_accuracy   0.167717     0.649944  273\n",
       "max mean_per_class_accuracy  0.149972     0.653898  291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.59 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100206</td>\n",
       "<td>0.5870321</td>\n",
       "<td>3.2008350</td>\n",
       "<td>3.2008350</td>\n",
       "<td>0.5951662</td>\n",
       "<td>0.5951662</td>\n",
       "<td>0.0320742</td>\n",
       "<td>0.0320742</td>\n",
       "<td>220.0835021</td>\n",
       "<td>220.0835021</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200109</td>\n",
       "<td>0.5271460</td>\n",
       "<td>2.5260551</td>\n",
       "<td>2.8639555</td>\n",
       "<td>0.4696970</td>\n",
       "<td>0.5325265</td>\n",
       "<td>0.0252361</td>\n",
       "<td>0.0573103</td>\n",
       "<td>152.6055080</td>\n",
       "<td>186.3955474</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300012</td>\n",
       "<td>0.4907770</td>\n",
       "<td>2.3467867</td>\n",
       "<td>2.6917398</td>\n",
       "<td>0.4363636</td>\n",
       "<td>0.5005045</td>\n",
       "<td>0.0234451</td>\n",
       "<td>0.0807555</td>\n",
       "<td>134.6786655</td>\n",
       "<td>169.1739823</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400218</td>\n",
       "<td>0.4607515</td>\n",
       "<td>2.4209361</td>\n",
       "<td>2.6239365</td>\n",
       "<td>0.4501511</td>\n",
       "<td>0.4878971</td>\n",
       "<td>0.0242592</td>\n",
       "<td>0.1050147</td>\n",
       "<td>142.0936133</td>\n",
       "<td>162.3936479</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500121</td>\n",
       "<td>0.4368630</td>\n",
       "<td>2.2490039</td>\n",
       "<td>2.5490407</td>\n",
       "<td>0.4181818</td>\n",
       "<td>0.4739709</td>\n",
       "<td>0.0224683</td>\n",
       "<td>0.1274829</td>\n",
       "<td>124.9003878</td>\n",
       "<td>154.9040741</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000242</td>\n",
       "<td>0.3510749</td>\n",
       "<td>2.0086311</td>\n",
       "<td>2.2788359</td>\n",
       "<td>0.3734867</td>\n",
       "<td>0.4237288</td>\n",
       "<td>0.1004559</td>\n",
       "<td>0.2279388</td>\n",
       "<td>100.8631082</td>\n",
       "<td>127.8835912</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500061</td>\n",
       "<td>0.2954589</td>\n",
       "<td>1.8111431</td>\n",
       "<td>2.1230012</td>\n",
       "<td>0.3367656</td>\n",
       "<td>0.3947528</td>\n",
       "<td>0.0905243</td>\n",
       "<td>0.3184630</td>\n",
       "<td>81.1143143</td>\n",
       "<td>112.3001248</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000182</td>\n",
       "<td>0.2562466</td>\n",
       "<td>1.5723968</td>\n",
       "<td>1.9853293</td>\n",
       "<td>0.2923729</td>\n",
       "<td>0.3691539</td>\n",
       "<td>0.0786389</td>\n",
       "<td>0.3971019</td>\n",
       "<td>57.2396779</td>\n",
       "<td>98.5329296</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000121</td>\n",
       "<td>0.2033653</td>\n",
       "<td>1.3644590</td>\n",
       "<td>1.7783934</td>\n",
       "<td>0.2537087</td>\n",
       "<td>0.3306761</td>\n",
       "<td>0.1364376</td>\n",
       "<td>0.5335396</td>\n",
       "<td>36.4459039</td>\n",
       "<td>77.8393428</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000061</td>\n",
       "<td>0.1689245</td>\n",
       "<td>1.1153394</td>\n",
       "<td>1.6126425</td>\n",
       "<td>0.2073872</td>\n",
       "<td>0.2998562</td>\n",
       "<td>0.1115272</td>\n",
       "<td>0.6450668</td>\n",
       "<td>11.5339429</td>\n",
       "<td>61.2642473</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1444336</td>\n",
       "<td>1.0290431</td>\n",
       "<td>1.4959297</td>\n",
       "<td>0.1913412</td>\n",
       "<td>0.2781545</td>\n",
       "<td>0.1028981</td>\n",
       "<td>0.7479648</td>\n",
       "<td>2.9043094</td>\n",
       "<td>49.5929665</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999939</td>\n",
       "<td>0.1259430</td>\n",
       "<td>0.7652694</td>\n",
       "<td>1.3741591</td>\n",
       "<td>0.1422949</td>\n",
       "<td>0.2555124</td>\n",
       "<td>0.0765223</td>\n",
       "<td>0.8244871</td>\n",
       "<td>-23.4730611</td>\n",
       "<td>37.4159097</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999879</td>\n",
       "<td>0.1109070</td>\n",
       "<td>0.6415237</td>\n",
       "<td>1.2695014</td>\n",
       "<td>0.1192855</td>\n",
       "<td>0.2360522</td>\n",
       "<td>0.0641485</td>\n",
       "<td>0.8886356</td>\n",
       "<td>-35.8476299</td>\n",
       "<td>26.9501424</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999818</td>\n",
       "<td>0.0968897</td>\n",
       "<td>0.5259192</td>\n",
       "<td>1.1765572</td>\n",
       "<td>0.0977899</td>\n",
       "<td>0.2187701</td>\n",
       "<td>0.0525887</td>\n",
       "<td>0.9412244</td>\n",
       "<td>-47.4080824</td>\n",
       "<td>17.6557160</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999758</td>\n",
       "<td>0.0829745</td>\n",
       "<td>0.4054300</td>\n",
       "<td>1.0908792</td>\n",
       "<td>0.0753860</td>\n",
       "<td>0.2028391</td>\n",
       "<td>0.0405405</td>\n",
       "<td>0.9817649</td>\n",
       "<td>-59.4570047</td>\n",
       "<td>9.0879242</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0624311</td>\n",
       "<td>0.1823069</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0338983</td>\n",
       "<td>0.1859409</td>\n",
       "<td>0.0182351</td>\n",
       "<td>1.0</td>\n",
       "<td>-81.7693127</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100206                   0.587032           3.20084   3.20084            0.595166         0.595166                    0.0320742       0.0320742                  220.084   220.084\n",
       "    2        0.0200109                   0.527146           2.52606   2.86396            0.469697         0.532526                    0.0252361       0.0573103                  152.606   186.396\n",
       "    3        0.0300012                   0.490777           2.34679   2.69174            0.436364         0.500505                    0.0234451       0.0807555                  134.679   169.174\n",
       "    4        0.0400218                   0.460751           2.42094   2.62394            0.450151         0.487897                    0.0242592       0.105015                   142.094   162.394\n",
       "    5        0.0500121                   0.436863           2.249     2.54904            0.418182         0.473971                    0.0224683       0.127483                   124.9     154.904\n",
       "    6        0.100024                    0.351075           2.00863   2.27884            0.373487         0.423729                    0.100456        0.227939                   100.863   127.884\n",
       "    7        0.150006                    0.295459           1.81114   2.123              0.336766         0.394753                    0.0905243       0.318463                   81.1143   112.3\n",
       "    8        0.200018                    0.256247           1.5724    1.98533            0.292373         0.369154                    0.0786389       0.397102                   57.2397   98.5329\n",
       "    9        0.300012                    0.203365           1.36446   1.77839            0.253709         0.330676                    0.136438        0.53354                    36.4459   77.8393\n",
       "    10       0.400006                    0.168924           1.11534   1.61264            0.207387         0.299856                    0.111527        0.645067                   11.5339   61.2642\n",
       "    11       0.5                         0.144434           1.02904   1.49593            0.191341         0.278155                    0.102898        0.747965                   2.90431   49.593\n",
       "    12       0.599994                    0.125943           0.765269  1.37416            0.142295         0.255512                    0.0765223       0.824487                   -23.4731  37.4159\n",
       "    13       0.699988                    0.110907           0.641524  1.2695             0.119285         0.236052                    0.0641485       0.888636                   -35.8476  26.9501\n",
       "    14       0.799982                    0.0968897          0.525919  1.17656            0.0977899        0.21877                     0.0525887       0.941224                   -47.4081  17.6557\n",
       "    15       0.899976                    0.0829745          0.40543   1.09088            0.075386         0.202839                    0.0405405       0.981765                   -59.457   9.08792\n",
       "    16       1                           0.0624311          0.182307  1                  0.0338983        0.185941                    0.0182351       1                          -81.7693  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.13731436524493457\n",
      "RMSE: 0.37055952996102337\n",
      "LogLoss: 0.4365729016322642\n",
      "Null degrees of freedom: 130954\n",
      "Residual degrees of freedom: 130947\n",
      "Null deviance: 124375.46514588114\n",
      "Residual deviance: 114342.80866650632\n",
      "AIC: 114358.80866650632\n",
      "AUC: 0.7052102719993631\n",
      "Gini: 0.4104205439987263\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.1834653742477123: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>75544.0</td>\n",
       "<td>31537.0</td>\n",
       "<td>0.2945</td>\n",
       "<td> (31537.0/107081.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>9818.0</td>\n",
       "<td>14056.0</td>\n",
       "<td>0.4112</td>\n",
       "<td> (9818.0/23874.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>85362.0</td>\n",
       "<td>45593.0</td>\n",
       "<td>0.3158</td>\n",
       "<td> (41355.0/130955.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      75544  31537  0.2945   (31537.0/107081.0)\n",
       "1      9818   14056  0.4112   (9818.0/23874.0)\n",
       "Total  85362  45593  0.3158   (41355.0/130955.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1834654</td>\n",
       "<td>0.4046814</td>\n",
       "<td>261.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1142473</td>\n",
       "<td>0.5642710</td>\n",
       "<td>335.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2981927</td>\n",
       "<td>0.3695123</td>\n",
       "<td>172.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6033991</td>\n",
       "<td>0.8184109</td>\n",
       "<td>29.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8158356</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0635534</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8158356</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2024945</td>\n",
       "<td>0.2396925</td>\n",
       "<td>244.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1661343</td>\n",
       "<td>0.6488453</td>\n",
       "<td>278.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1577254</td>\n",
       "<td>0.6492678</td>\n",
       "<td>287.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.183465     0.404681  261\n",
       "max f2                       0.114247     0.564271  335\n",
       "max f0point5                 0.298193     0.369512  172\n",
       "max accuracy                 0.603399     0.818411  29\n",
       "max precision                0.815836     1         0\n",
       "max recall                   0.0635534    1         399\n",
       "max specificity              0.815836     1         0\n",
       "max absolute_mcc             0.202495     0.239692  244\n",
       "max min_per_class_accuracy   0.166134     0.648845  278\n",
       "max mean_per_class_accuracy  0.157725     0.649268  287"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.23 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100034</td>\n",
       "<td>0.5751406</td>\n",
       "<td>2.9143039</td>\n",
       "<td>2.9143039</td>\n",
       "<td>0.5312977</td>\n",
       "<td>0.5312977</td>\n",
       "<td>0.0291531</td>\n",
       "<td>0.0291531</td>\n",
       "<td>191.4303912</td>\n",
       "<td>191.4303912</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200069</td>\n",
       "<td>0.5209152</td>\n",
       "<td>2.6044498</td>\n",
       "<td>2.7593768</td>\n",
       "<td>0.4748092</td>\n",
       "<td>0.5030534</td>\n",
       "<td>0.0260534</td>\n",
       "<td>0.0552065</td>\n",
       "<td>160.4449761</td>\n",
       "<td>175.9376837</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300027</td>\n",
       "<td>0.4825780</td>\n",
       "<td>2.3173006</td>\n",
       "<td>2.6120931</td>\n",
       "<td>0.4224599</td>\n",
       "<td>0.4762026</td>\n",
       "<td>0.0231633</td>\n",
       "<td>0.0783698</td>\n",
       "<td>131.7300632</td>\n",
       "<td>161.2093113</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400061</td>\n",
       "<td>0.4532771</td>\n",
       "<td>2.3867144</td>\n",
       "<td>2.5557377</td>\n",
       "<td>0.4351145</td>\n",
       "<td>0.4659286</td>\n",
       "<td>0.0238753</td>\n",
       "<td>0.1022451</td>\n",
       "<td>138.6714411</td>\n",
       "<td>155.5737682</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500019</td>\n",
       "<td>0.4296896</td>\n",
       "<td>2.2460635</td>\n",
       "<td>2.4938312</td>\n",
       "<td>0.4094729</td>\n",
       "<td>0.4546426</td>\n",
       "<td>0.0224512</td>\n",
       "<td>0.1246963</td>\n",
       "<td>124.6063542</td>\n",
       "<td>149.3831230</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000038</td>\n",
       "<td>0.3444799</td>\n",
       "<td>2.0850339</td>\n",
       "<td>2.2894326</td>\n",
       "<td>0.3801161</td>\n",
       "<td>0.4173794</td>\n",
       "<td>0.1042557</td>\n",
       "<td>0.2289520</td>\n",
       "<td>108.5033904</td>\n",
       "<td>128.9432567</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500057</td>\n",
       "<td>0.2913521</td>\n",
       "<td>1.7440902</td>\n",
       "<td>2.1076518</td>\n",
       "<td>0.3179597</td>\n",
       "<td>0.3842395</td>\n",
       "<td>0.0872078</td>\n",
       "<td>0.3161598</td>\n",
       "<td>74.4090232</td>\n",
       "<td>110.7651789</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2530770</td>\n",
       "<td>1.5650616</td>\n",
       "<td>1.9720198</td>\n",
       "<td>0.2853215</td>\n",
       "<td>0.3595128</td>\n",
       "<td>0.0782441</td>\n",
       "<td>0.3944040</td>\n",
       "<td>56.5061566</td>\n",
       "<td>97.2019770</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000038</td>\n",
       "<td>0.2009810</td>\n",
       "<td>1.3796910</td>\n",
       "<td>1.7745718</td>\n",
       "<td>0.2515272</td>\n",
       "<td>0.3235167</td>\n",
       "<td>0.1379744</td>\n",
       "<td>0.5323783</td>\n",
       "<td>37.9690976</td>\n",
       "<td>77.4571813</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1671209</td>\n",
       "<td>1.1100365</td>\n",
       "<td>1.6084443</td>\n",
       "<td>0.2023673</td>\n",
       "<td>0.2932305</td>\n",
       "<td>0.1109994</td>\n",
       "<td>0.6433777</td>\n",
       "<td>11.0036518</td>\n",
       "<td>60.8444333</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000038</td>\n",
       "<td>0.1431993</td>\n",
       "<td>0.9369668</td>\n",
       "<td>1.4741447</td>\n",
       "<td>0.1708155</td>\n",
       "<td>0.2687468</td>\n",
       "<td>0.0937003</td>\n",
       "<td>0.7370780</td>\n",
       "<td>-6.3033177</td>\n",
       "<td>47.4144729</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1250298</td>\n",
       "<td>0.8076039</td>\n",
       "<td>1.3630588</td>\n",
       "<td>0.1472318</td>\n",
       "<td>0.2484950</td>\n",
       "<td>0.0807573</td>\n",
       "<td>0.8178353</td>\n",
       "<td>-19.2396073</td>\n",
       "<td>36.3058837</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999962</td>\n",
       "<td>0.1098459</td>\n",
       "<td>0.6635086</td>\n",
       "<td>1.2631264</td>\n",
       "<td>0.1209622</td>\n",
       "<td>0.2302767</td>\n",
       "<td>0.0663483</td>\n",
       "<td>0.8841836</td>\n",
       "<td>-33.6491379</td>\n",
       "<td>26.3126362</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0960804</td>\n",
       "<td>0.5298449</td>\n",
       "<td>1.1714627</td>\n",
       "<td>0.0965944</td>\n",
       "<td>0.2135657</td>\n",
       "<td>0.0529865</td>\n",
       "<td>0.9371701</td>\n",
       "<td>-47.0155105</td>\n",
       "<td>17.1462679</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999962</td>\n",
       "<td>0.0823591</td>\n",
       "<td>0.4163684</td>\n",
       "<td>1.0875662</td>\n",
       "<td>0.0759068</td>\n",
       "<td>0.1982708</td>\n",
       "<td>0.0416353</td>\n",
       "<td>0.9788054</td>\n",
       "<td>-58.3631585</td>\n",
       "<td>8.7566164</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0614275</td>\n",
       "<td>0.2119380</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0386378</td>\n",
       "<td>0.1823069</td>\n",
       "<td>0.0211946</td>\n",
       "<td>1.0</td>\n",
       "<td>-78.8062042</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100034                   0.575141           2.9143    2.9143             0.531298         0.531298                    0.0291531       0.0291531                  191.43    191.43\n",
       "    2        0.0200069                   0.520915           2.60445   2.75938            0.474809         0.503053                    0.0260534       0.0552065                  160.445   175.938\n",
       "    3        0.0300027                   0.482578           2.3173    2.61209            0.42246          0.476203                    0.0231633       0.0783698                  131.73    161.209\n",
       "    4        0.0400061                   0.453277           2.38671   2.55574            0.435115         0.465929                    0.0238753       0.102245                   138.671   155.574\n",
       "    5        0.0500019                   0.42969            2.24606   2.49383            0.409473         0.454643                    0.0224512       0.124696                   124.606   149.383\n",
       "    6        0.100004                    0.34448            2.08503   2.28943            0.380116         0.417379                    0.104256        0.228952                   108.503   128.943\n",
       "    7        0.150006                    0.291352           1.74409   2.10765            0.31796          0.384239                    0.0872078       0.31616                    74.409    110.765\n",
       "    8        0.2                         0.253077           1.56506   1.97202            0.285322         0.359513                    0.0782441       0.394404                   56.5062   97.202\n",
       "    9        0.300004                    0.200981           1.37969   1.77457            0.251527         0.323517                    0.137974        0.532378                   37.9691   77.4572\n",
       "    10       0.4                         0.167121           1.11004   1.60844            0.202367         0.29323                     0.110999        0.643378                   11.0037   60.8444\n",
       "    11       0.500004                    0.143199           0.936967  1.47414            0.170816         0.268747                    0.0937003       0.737078                   -6.30332  47.4145\n",
       "    12       0.6                         0.12503            0.807604  1.36306            0.147232         0.248495                    0.0807573       0.817835                   -19.2396  36.3059\n",
       "    13       0.699996                    0.109846           0.663509  1.26313            0.120962         0.230277                    0.0663483       0.884184                   -33.6491  26.3126\n",
       "    14       0.8                         0.0960804          0.529845  1.17146            0.0965944        0.213566                    0.0529865       0.93717                    -47.0155  17.1463\n",
       "    15       0.899996                    0.0823591          0.416368  1.08757            0.0759068        0.198271                    0.0416353       0.978805                   -58.3632  8.75662\n",
       "    16       1                           0.0614275          0.211938  1                  0.0386378        0.182307                    0.0211946       1                          -78.8062  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method ModelBase.weights of >"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Sets for Binary Classifier \n",
    "\n",
    "#### Some Kaggle Binary classification competitions  \n",
    "\n",
    "The idea here is to get a range of datasets to test our H2O binary classification models as well as to understand which approaches work best for binary classification.   The hope is to get a single model or set of models that perform well in these competitions as well as logic and tests to dynamically choose the best models and their parameters.  \n",
    "\n",
    "[Santander Customer Satisfaction](https://www.kaggle.com/c/santander-customer-satisfaction)    \n",
    "\n",
    "[Facebook Recruiting IV: Human or Robot?](https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot)    \n",
    "\n",
    "[DonorsChoose.org Application Screening Predict whether teachers' project proposals are accepted](https://www.kaggle.com/c/donorschoose-application-screening)    \n",
    "\n",
    "[Statoil/C-CORE Iceberg Classifier Challenge Ship or iceberg, can you decide from space?](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge)    \n",
    "\n",
    "[WSDM - KKBox's Churn Prediction Challenge Can you predict when subscribers will churn?](https://www.kaggle.com/c/kkbox-churn-prediction-challenge)    \n",
    "\n",
    "[Porto Seguroâ€™s Safe Driver Prediction Predict if a driver will file an insurance claim next year.](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction)    \n",
    "\n",
    "[Porto Seguroâ€™s Safe Driver Prediction Predict if a driver will file an insurance claim next year.](https://www.kaggle.com/c/dato-native)    \n",
    "\n",
    "[Data Science Bowl 2017 Can you improve lung cancer detection?](https://www.kaggle.com/c/data-science-bowl-2017)    \n",
    "\n",
    "[Random Acts of Pizza Predicting altruism through free pizza](https://www.kaggle.com/c/random-acts-of-pizza)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update:  June 24, 2018"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
